meqsqitivmfissbsqkvemaymlebeneckmqkkdmai length 6 357701 page 357701 <!DOCTYPE html>
<html class="client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>Hallucination (artificial intelligence) - Wikipedia</title>
<script>(function(){var className="client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\w+$|[^\w-]+/g,'')+'-clientpref-\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"8409ff2e-11fa-42b5-9742-43b9bcfd7dee","wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Hallucination_(artificial_intelligence)","wgTitle":"Hallucination (artificial intelligence)","wgCurRevisionId":1325695960,"wgRevisionId":1325695960,"wgArticleId":72607666,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles with short description","Short description is different from Wikidata","Use dmy dates from December 2022","Pages using multiple image with auto scaled images","Articles containing video clips","Anthropomorphism","Communication of falsehoods","Computational linguistics","Computational neuroscience","Deep learning","Disinformation","Language modeling","Machine learning","Misinformation","Software bugs","Generative artificial intelligence","Philosophy of artificial intelligence","Unsupervised learning","Pejorative terms related to technology"],"wgPageViewLanguage":"en","wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Hallucination_(artificial_intelligence)","wgRelevantArticleId":72607666,"wgTempUserName":null,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgNoticeProject":"wikipedia","wgFlaggedRevsParams":{"tags":{"status":{"levels":1}}},"wgConfirmEditCaptchaNeededForGenericEdit":"hcaptcha","wgConfirmEditHCaptchaVisualEditorOnLoadIntegrationEnabled":false,"wgConfirmEditHCaptchaSiteKey":"ccd26dec-6c86-4034-a704-e402435cd53c","wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsFlags":0,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"watchlist":true,"tagline":false,"nearby":true},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":80000,"wgEditSubmitButtonLabelPublish":true,"wgULSPosition":"interlanguage","wgULSisCompactLinksEnabled":false,"wgVector2022LanguageInHeader":true,"wgULSisLanguageSelectorEmpty":false,"wgWikibaseItemId":"Q116197048","wgCheckUserClientHintsHeadersJsApi":["brands","architecture","bitness","fullVersionList","mobile","model","platform","platformVersion"],"GEHomepageSuggestedEditsEnableTopics":true,"wgGESuggestedEditsTaskTypes":{"taskTypes":["copyedit","link-recommendation"],"unavailableTaskTypes":[]},"wgGETopicsMatchModeEnabled":false,"wgGELevelingUpEnabledForUser":false,"wgGEUseMetricsPlatformExtension":true,"wgMetricsPlatformUserExperiments":{"active_experiments":[],"overrides":[],"enrolled":[],"assigned":[],"subject_ids":[],"sampling_units":[],"coordinator":[]}};
RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.tmh.player.styles":"ready","ext.wikimediamessages.styles":"ready","skins.vector.search.codex.styles":"ready","skins.vector.styles":"ready","skins.vector.icons":"ready","jquery.makeCollapsible.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","ext.tmh.player","mediawiki.page.media","site","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","skins.vector.js","ext.centralNotice.geoIP","ext.centralNotice.startUp","ext.gadget.ReferenceTooltips","ext.gadget.switcher","ext.urlShortener.toolbar","ext.centralauth.centralautologin","mmv.bootstrap","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.echo.centralauth","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.cx.uls.quick.actions","wikibase.client.vector-2022","wikibase.databox.fromWikidata","ext.checkUser.clientHints","ext.growthExperiments.SuggestedEditSession","ext.xLab"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return["user.options@12s5i",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
}];});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.tmh.player.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediamessages.styles%7Cjquery.makeCollapsible.styles%7Cskins.vector.icons%2Cstyles%7Cskins.vector.search.codex.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector-2022">
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector-2022"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector-2022">
<meta name="generator" content="MediaWiki 1.46.0-wmf.5">
<meta name="referrer" content="origin">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="robots" content="max-image-preview:standard">
<meta name="format-detection" content="telephone=no">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Photoreal-train.webm/1200px--Photoreal-train.webm.jpg">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="675">
<meta name="viewport" content="width=1120">
<meta property="og:title" content="Hallucination (artificial intelligence) - Wikipedia">
<meta property="og:type" content="website">
<link rel="preconnect" href="//upload.wikimedia.org">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit">
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png">
<link rel="icon" href="/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="/w/rest.php/v1/search" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd">
<link rel="canonical" href="https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)">
<link rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom">
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<link rel="dns-prefetch" href="auth.wikimedia.org">
</head>
<body class="skin--responsive skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Hallucination_artificial_intelligence rootpage-Hallucination_artificial_intelligence skin-vector-2022 action-view">
<div id="mw-aria-live-region" class="mw-aria-live-region" aria-live="polite"></div><a class="mw-jump-link" href="#bodyContent">Jump to content</a>
<div class="vector-header-container">
	<header class="vector-header mw-header no-font-mode-scale">
		<div class="vector-header-start">
			<nav class="vector-main-menu-landmark" aria-label="Site">
				
<div id="vector-main-menu-dropdown" class="vector-dropdown vector-main-menu-dropdown vector-button-flush-left vector-button-flush-right"  title="Main menu" >
	<input type="checkbox" id="vector-main-menu-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-main-menu-dropdown" class="vector-dropdown-checkbox "  aria-label="Main menu"  >
	<label id="vector-main-menu-dropdown-label" for="vector-main-menu-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-menu mw-ui-icon-wikimedia-menu"></span>

<span class="vector-dropdown-label-text">Main menu</span>
	</label>
	<div class="vector-dropdown-content">


				<div id="vector-main-menu-unpinned-container" class="vector-unpinned-container">
		
<div id="vector-main-menu" class="vector-main-menu vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-main-menu-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="main-menu-pinned"
	data-pinnable-element-id="vector-main-menu"
	data-pinned-container-id="vector-main-menu-pinned-container"
	data-unpinned-container-id="vector-main-menu-unpinned-container"
>
	<div class="vector-pinnable-header-label">Main menu</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-main-menu.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-main-menu.unpin">hide</button>
</div>

	
<div id="p-navigation" class="vector-menu mw-portlet mw-portlet-navigation"  >
	<div class="vector-menu-heading">
		Navigation
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-contents" class="mw-list-item"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia"><span>Contents</span></a></li><li id="n-currentevents" class="mw-list-item"><a href="/wiki/Portal:Current_events" title="Articles related to current events"><span>Current events</span></a></li><li id="n-randompage" class="mw-list-item"><a href="/wiki/Special:Random" title="Visit a randomly selected article [x]" accesskey="x"><span>Random article</span></a></li><li id="n-aboutsite" class="mw-list-item"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works"><span>About Wikipedia</span></a></li><li id="n-contactpage" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia"><span>Contact us</span></a></li>
		</ul>
		
	</div>
</div>

	
<div id="p-interaction" class="vector-menu mw-portlet mw-portlet-interaction"  >
	<div class="vector-menu-heading">
		Contribute
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-help" class="mw-list-item"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia"><span>Help</span></a></li><li id="n-introduction" class="mw-list-item"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia"><span>Learn to edit</span></a></li><li id="n-portal" class="mw-list-item"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors"><span>Community portal</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_upload_wizard" title="Add images or other media for use on Wikipedia"><span>Upload file</span></a></li><li id="n-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages"><span>Special pages</span></a></li>
		</ul>
		
	</div>
</div>

</div>

				</div>

	</div>
</div>

		</nav>
			
<a href="/wiki/Main_Page" class="mw-logo">
	<img class="mw-logo-icon" src="/static/images/icons/wikipedia.png" alt="" aria-hidden="true" height="50" width="50">
	<span class="mw-logo-container skin-invert">
		<img class="mw-logo-wordmark" alt="Wikipedia" src="/static/images/mobile/copyright/wikipedia-wordmark-en.svg" style="width: 7.5em; height: 1.125em;">
		<img class="mw-logo-tagline" alt="The Free Encyclopedia" src="/static/images/mobile/copyright/wikipedia-tagline-en.svg" width="117" height="13" style="width: 7.3125em; height: 0.8125em;">
	</span>
</a>

		</div>
		<div class="vector-header-end">
			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-collapses vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<a href="/wiki/Special:Search" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only search-toggle" title="Search Wikipedia [f]" accesskey="f"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
	</a>
	<div class="vector-typeahead-search-container">
		<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail cdx-typeahead-search--auto-expand-width">
			<form action="/w/index.php" id="searchform" class="cdx-search-input cdx-search-input--has-end-button">
				<div id="simpleSearch" class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
					<div class="cdx-text-input cdx-text-input--has-start-icon">
						<input
							class="cdx-text-input__input mw-searchInput" autocomplete="off"
							 type="search" name="search" placeholder="Search Wikipedia" aria-label="Search Wikipedia" autocapitalize="sentences" spellcheck="false" title="Search Wikipedia [f]" accesskey="f" id="searchInput"
							>
						<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
					</div>
					<input type="hidden" name="title" value="Special:Search">
				</div>
				<button class="cdx-button cdx-search-input__end-button">Search</button>
			</form>
		</div>
	</div>
</div>

			<nav class="vector-user-links vector-user-links-wide" aria-label="Personal tools">
	<div class="vector-user-links-main">
	
<div id="p-vector-user-menu-preferences" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-userpage" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	<nav class="vector-appearance-landmark" aria-label="Appearance">
		
<div id="vector-appearance-dropdown" class="vector-dropdown "  title="Change the appearance of the page&#039;s font size, width, and color" >
	<input type="checkbox" id="vector-appearance-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-appearance-dropdown" class="vector-dropdown-checkbox "  aria-label="Appearance"  >
	<label id="vector-appearance-dropdown-label" for="vector-appearance-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-appearance mw-ui-icon-wikimedia-appearance"></span>

<span class="vector-dropdown-label-text">Appearance</span>
	</label>
	<div class="vector-dropdown-content">


			<div id="vector-appearance-unpinned-container" class="vector-unpinned-container">
				
			</div>
		
	</div>
</div>

	</nav>
	
<div id="p-vector-user-menu-notifications" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-overflow" class="vector-menu mw-portlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			<li id="pt-sitesupport-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw-interface  href="https://donate.wikimedia.org/?wmf_source=donate&amp;wmf_medium=sidebar&amp;wmf_campaign=en.wikipedia.org&amp;uselang=en" class=""><span>Donate</span></a>
</li>
<li id="pt-createaccount-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw-interface  href="/w/index.php?title=Special:CreateAccount&amp;returnto=Hallucination+%28artificial+intelligence%29" title="You are encouraged to create an account and log in; however, it is not mandatory" class=""><span>Create account</span></a>
</li>
<li id="pt-login-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw-interface  href="/w/index.php?title=Special:UserLogin&amp;returnto=Hallucination+%28artificial+intelligence%29" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o" class=""><span>Log in</span></a>
</li>

			
		</ul>
		
	</div>
</div>

	</div>
	
<div id="vector-user-links-dropdown" class="vector-dropdown vector-user-menu vector-button-flush-right vector-user-menu-logged-out user-links-collapsible-item"  title="Log in and more options" >
	<input type="checkbox" id="vector-user-links-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-user-links-dropdown" class="vector-dropdown-checkbox "  aria-label="Personal tools"  >
	<label id="vector-user-links-dropdown-label" for="vector-user-links-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-ellipsis mw-ui-icon-wikimedia-ellipsis"></span>

<span class="vector-dropdown-label-text">Personal tools</span>
	</label>
	<div class="vector-dropdown-content">


		
<div id="p-personal" class="vector-menu mw-portlet mw-portlet-personal user-links-collapsible-item"  title="User menu" >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-sitesupport" class="user-links-collapsible-item mw-list-item"><a href="https://donate.wikimedia.org/?wmf_source=donate&amp;wmf_medium=sidebar&amp;wmf_campaign=en.wikipedia.org&amp;uselang=en"><span>Donate</span></a></li><li id="pt-createaccount" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Hallucination+%28artificial+intelligence%29" title="You are encouraged to create an account and log in; however, it is not mandatory"><span class="vector-icon mw-ui-icon-userAdd mw-ui-icon-wikimedia-userAdd"></span> <span>Create account</span></a></li><li id="pt-login" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Hallucination+%28artificial+intelligence%29" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o"><span class="vector-icon mw-ui-icon-logIn mw-ui-icon-wikimedia-logIn"></span> <span>Log in</span></a></li>
		</ul>
		
	</div>
</div>

	
	</div>
</div>

</nav>

		</div>
	</header>
</div>
<div class="mw-page-container">
	<div class="mw-page-container-inner">
		<div class="vector-sitenotice-container">
			<div id="siteNotice"><!-- CentralNotice --></div>
		</div>
		<div class="vector-column-start">
			<div class="vector-main-menu-container">
		<div id="mw-navigation">
			<nav id="mw-panel" class="vector-main-menu-landmark" aria-label="Site">
				<div id="vector-main-menu-pinned-container" class="vector-pinned-container">
				
				</div>
		</nav>
		</div>
	</div>
	<div class="vector-sticky-pinned-container">
				<nav id="mw-panel-toc" aria-label="Contents" data-event-name="ui.sidebar-toc" class="mw-table-of-contents-container vector-toc-landmark">
					<div id="vector-toc-pinned-container" class="vector-pinned-container">
					<div id="vector-toc" class="vector-toc vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-toc-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="toc-pinned"
	data-pinnable-element-id="vector-toc"
	
	
>
	<h2 class="vector-pinnable-header-label">Contents</h2>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-toc.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-toc.unpin">hide</button>
</div>


	<ul class="vector-toc-contents" id="mw-panel-toc-list">
		<li id="toc-mw-content-text"
			class="vector-toc-list-item vector-toc-level-1">
			<a href="#" class="vector-toc-link">
				<div class="vector-toc-text">(Top)</div>
			</a>
		</li>
		<li id="toc-Term"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Term">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">1</span>
				<span>Term</span>
			</div>
		</a>
		
			<button aria-controls="toc-Term-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Term subsection</span>
			</button>
		
		<ul id="toc-Term-sublist" class="vector-toc-list">
			<li id="toc-Origin"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Origin">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">1.1</span>
					<span>Origin</span>
				</div>
			</a>
			
			<ul id="toc-Origin-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Definitions_and_alternatives"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Definitions_and_alternatives">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">1.2</span>
					<span>Definitions and alternatives</span>
				</div>
			</a>
			
			<ul id="toc-Definitions_and_alternatives-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Criticism"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Criticism">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">1.3</span>
					<span>Criticism</span>
				</div>
			</a>
			
			<ul id="toc-Criticism-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-In_natural_language_generation"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#In_natural_language_generation">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">2</span>
				<span>In natural language generation</span>
			</div>
		</a>
		
			<button aria-controls="toc-In_natural_language_generation-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle In natural language generation subsection</span>
			</button>
		
		<ul id="toc-In_natural_language_generation-sublist" class="vector-toc-list">
			<li id="toc-Causes"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Causes">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">2.1</span>
					<span>Causes</span>
				</div>
			</a>
			
			<ul id="toc-Causes-sublist" class="vector-toc-list">
				<li id="toc-Hallucination_from_data"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Hallucination_from_data">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">2.1.1</span>
					<span>Hallucination from data</span>
				</div>
			</a>
			
			<ul id="toc-Hallucination_from_data-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Modeling-related_causes"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Modeling-related_causes">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">2.1.2</span>
					<span>Modeling-related causes</span>
				</div>
			</a>
			
			<ul id="toc-Modeling-related_causes-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Interpretability_research"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Interpretability_research">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">2.1.3</span>
					<span>Interpretability research</span>
				</div>
			</a>
			
			<ul id="toc-Interpretability_research-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
		</li>
		<li id="toc-Examples"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Examples">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">2.2</span>
					<span>Examples</span>
				</div>
			</a>
			
			<ul id="toc-Examples-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-In_other_modalities"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#In_other_modalities">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">3</span>
				<span>In other modalities</span>
			</div>
		</a>
		
			<button aria-controls="toc-In_other_modalities-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle In other modalities subsection</span>
			</button>
		
		<ul id="toc-In_other_modalities-sublist" class="vector-toc-list">
			<li id="toc-Object_detection"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Object_detection">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.1</span>
					<span>Object detection</span>
				</div>
			</a>
			
			<ul id="toc-Object_detection-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Text-to-audio_generative_AI"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Text-to-audio_generative_AI">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.2</span>
					<span>Text-to-audio generative AI</span>
				</div>
			</a>
			
			<ul id="toc-Text-to-audio_generative_AI-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Text-to-image_generative_AI"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Text-to-image_generative_AI">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.3</span>
					<span>Text-to-image generative AI</span>
				</div>
			</a>
			
			<ul id="toc-Text-to-image_generative_AI-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-In_scientific_research"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#In_scientific_research">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">4</span>
				<span>In scientific research</span>
			</div>
		</a>
		
			<button aria-controls="toc-In_scientific_research-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle In scientific research subsection</span>
			</button>
		
		<ul id="toc-In_scientific_research-sublist" class="vector-toc-list">
			<li id="toc-Problems"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Problems">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">4.1</span>
					<span>Problems</span>
				</div>
			</a>
			
			<ul id="toc-Problems-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Benefits"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Benefits">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">4.2</span>
					<span>Benefits</span>
				</div>
			</a>
			
			<ul id="toc-Benefits-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Consequences_of_hallucinations_in_education"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Consequences_of_hallucinations_in_education">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">5</span>
				<span>Consequences of hallucinations in education</span>
			</div>
		</a>
		
		<ul id="toc-Consequences_of_hallucinations_in_education-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Mitigation_methods"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Mitigation_methods">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">6</span>
				<span>Mitigation methods</span>
			</div>
		</a>
		
		<ul id="toc-Mitigation_methods-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-See_also"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#See_also">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">7</span>
				<span>See also</span>
			</div>
		</a>
		
		<ul id="toc-See_also-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-References"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#References">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">8</span>
				<span>References</span>
			</div>
		</a>
		
		<ul id="toc-References-sublist" class="vector-toc-list">
		</ul>
	</li>
</ul>
</div>

					</div>
		</nav>
			</div>
		</div>
		<div class="mw-content-container">
			<main id="content" class="mw-body">
				<header class="mw-body-header vector-page-titlebar no-font-mode-scale">
					<nav aria-label="Contents" class="vector-toc-landmark">
						
<div id="vector-page-titlebar-toc" class="vector-dropdown vector-page-titlebar-toc vector-button-flush-left"  title="Table of Contents" >
	<input type="checkbox" id="vector-page-titlebar-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-titlebar-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
	<label id="vector-page-titlebar-toc-label" for="vector-page-titlebar-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
	</label>
	<div class="vector-dropdown-content">


							<div id="vector-page-titlebar-toc-unpinned-container" class="vector-unpinned-container">
			</div>
		
	</div>
</div>

					</nav>
					<h1 id="firstHeading" class="firstHeading mw-first-heading"><span class="mw-page-title-main">Hallucination (artificial intelligence)</span></h1>
							
<div id="p-lang-btn" class="vector-dropdown mw-portlet mw-portlet-lang"  >
	<input type="checkbox" id="p-lang-btn-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-p-lang-btn" class="vector-dropdown-checkbox mw-interlanguage-selector" aria-label="Go to an article in another language. Available in 31 languages"   >
	<label id="p-lang-btn-label" for="p-lang-btn-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive mw-portlet-lang-heading-31" aria-hidden="true"  ><span class="vector-icon mw-ui-icon-language-progressive mw-ui-icon-wikimedia-language-progressive"></span>

<span class="vector-dropdown-label-text">31 languages</span>
	</label>
	<div class="vector-dropdown-content">

		<div class="vector-menu-content">
			
			<ul class="vector-menu-content-list">
				
				<li class="interlanguage-link interwiki-af mw-list-item"><a href="https://af.wikipedia.org/wiki/Hallusinasie_(kunsmatige_intelligensie)" title="Hallusinasie (kunsmatige intelligensie) – Afrikaans" lang="af" hreflang="af" data-title="Hallusinasie (kunsmatige intelligensie)" data-language-autonym="Afrikaans" data-language-local-name="Afrikaans" class="interlanguage-link-target"><span>Afrikaans</span></a></li><li class="interlanguage-link interwiki-ar mw-list-item"><a href="https://ar.wikipedia.org/wiki/%D9%87%D9%84%D9%88%D8%B3%D8%A9_(%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A)" title="هلوسة (ذكاء اصطناعي) – Arabic" lang="ar" hreflang="ar" data-title="هلوسة (ذكاء اصطناعي)" data-language-autonym="العربية" data-language-local-name="Arabic" class="interlanguage-link-target"><span>العربية</span></a></li><li class="interlanguage-link interwiki-az mw-list-item"><a href="https://az.wikipedia.org/wiki/S%C3%BCni_intellektin_hall%C3%BCsinasiyas%C4%B1" title="Süni intellektin hallüsinasiyası – Azerbaijani" lang="az" hreflang="az" data-title="Süni intellektin hallüsinasiyası" data-language-autonym="Azərbaycanca" data-language-local-name="Azerbaijani" class="interlanguage-link-target"><span>Azərbaycanca</span></a></li><li class="interlanguage-link interwiki-bg mw-list-item"><a href="https://bg.wikipedia.org/wiki/%D0%A5%D0%B0%D0%BB%D1%8E%D1%86%D0%B8%D0%BD%D0%B0%D1%86%D0%B8%D1%8F_(%D0%B8%D0%B7%D0%BA%D1%83%D1%81%D1%82%D0%B2%D0%B5%D0%BD_%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%B5%D0%BA%D1%82)" title="Халюцинация (изкуствен интелект) – Bulgarian" lang="bg" hreflang="bg" data-title="Халюцинация (изкуствен интелект)" data-language-autonym="Български" data-language-local-name="Bulgarian" class="interlanguage-link-target"><span>Български</span></a></li><li class="interlanguage-link interwiki-ca mw-list-item"><a href="https://ca.wikipedia.org/wiki/Al%C2%B7lucinaci%C3%B3_(intel%C2%B7lig%C3%A8ncia_artificial)" title="Al·lucinació (intel·ligència artificial) – Catalan" lang="ca" hreflang="ca" data-title="Al·lucinació (intel·ligència artificial)" data-language-autonym="Català" data-language-local-name="Catalan" class="interlanguage-link-target"><span>Català</span></a></li><li class="interlanguage-link interwiki-cs mw-list-item"><a href="https://cs.wikipedia.org/wiki/Halucinace_(um%C4%9Bl%C3%A1_inteligence)" title="Halucinace (umělá inteligence) – Czech" lang="cs" hreflang="cs" data-title="Halucinace (umělá inteligence)" data-language-autonym="Čeština" data-language-local-name="Czech" class="interlanguage-link-target"><span>Čeština</span></a></li><li class="interlanguage-link interwiki-de mw-list-item"><a href="https://de.wikipedia.org/wiki/Halluzination_(K%C3%BCnstliche_Intelligenz)" title="Halluzination (Künstliche Intelligenz) – German" lang="de" hreflang="de" data-title="Halluzination (Künstliche Intelligenz)" data-language-autonym="Deutsch" data-language-local-name="German" class="interlanguage-link-target"><span>Deutsch</span></a></li><li class="interlanguage-link interwiki-et mw-list-item"><a href="https://et.wikipedia.org/wiki/Hallutsineerimine" title="Hallutsineerimine – Estonian" lang="et" hreflang="et" data-title="Hallutsineerimine" data-language-autonym="Eesti" data-language-local-name="Estonian" class="interlanguage-link-target"><span>Eesti</span></a></li><li class="interlanguage-link interwiki-es mw-list-item"><a href="https://es.wikipedia.org/wiki/Alucinaci%C3%B3n_(inteligencia_artificial)" title="Alucinación (inteligencia artificial) – Spanish" lang="es" hreflang="es" data-title="Alucinación (inteligencia artificial)" data-language-autonym="Español" data-language-local-name="Spanish" class="interlanguage-link-target"><span>Español</span></a></li><li class="interlanguage-link interwiki-fa mw-list-item"><a href="https://fa.wikipedia.org/wiki/%D8%AA%D9%88%D9%87%D9%85_(%D9%87%D9%88%D8%B4_%D9%85%D8%B5%D9%86%D9%88%D8%B9%DB%8C)" title="توهم (هوش مصنوعی) – Persian" lang="fa" hreflang="fa" data-title="توهم (هوش مصنوعی)" data-language-autonym="فارسی" data-language-local-name="Persian" class="interlanguage-link-target"><span>فارسی</span></a></li><li class="interlanguage-link interwiki-fr mw-list-item"><a href="https://fr.wikipedia.org/wiki/Hallucination_(intelligence_artificielle)" title="Hallucination (intelligence artificielle) – French" lang="fr" hreflang="fr" data-title="Hallucination (intelligence artificielle)" data-language-autonym="Français" data-language-local-name="French" class="interlanguage-link-target"><span>Français</span></a></li><li class="interlanguage-link interwiki-gl mw-list-item"><a href="https://gl.wikipedia.org/wiki/Alucinaci%C3%B3n_(IA)" title="Alucinación (IA) – Galician" lang="gl" hreflang="gl" data-title="Alucinación (IA)" data-language-autonym="Galego" data-language-local-name="Galician" class="interlanguage-link-target"><span>Galego</span></a></li><li class="interlanguage-link interwiki-ko mw-list-item"><a href="https://ko.wikipedia.org/wiki/%ED%99%98%EA%B0%81_(%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5)" title="환각 (인공지능) – Korean" lang="ko" hreflang="ko" data-title="환각 (인공지능)" data-language-autonym="한국어" data-language-local-name="Korean" class="interlanguage-link-target"><span>한국어</span></a></li><li class="interlanguage-link interwiki-id mw-list-item"><a href="https://id.wikipedia.org/wiki/Halusinasi_(kecerdasan_buatan)" title="Halusinasi (kecerdasan buatan) – Indonesian" lang="id" hreflang="id" data-title="Halusinasi (kecerdasan buatan)" data-language-autonym="Bahasa Indonesia" data-language-local-name="Indonesian" class="interlanguage-link-target"><span>Bahasa Indonesia</span></a></li><li class="interlanguage-link interwiki-it mw-list-item"><a href="https://it.wikipedia.org/wiki/Allucinazione_(intelligenza_artificiale)" title="Allucinazione (intelligenza artificiale) – Italian" lang="it" hreflang="it" data-title="Allucinazione (intelligenza artificiale)" data-language-autonym="Italiano" data-language-local-name="Italian" class="interlanguage-link-target"><span>Italiano</span></a></li><li class="interlanguage-link interwiki-he mw-list-item"><a href="https://he.wikipedia.org/wiki/%D7%94%D7%96%D7%99%D7%94_(%D7%91%D7%99%D7%A0%D7%94_%D7%9E%D7%9C%D7%90%D7%9B%D7%95%D7%AA%D7%99%D7%AA)" title="הזיה (בינה מלאכותית) – Hebrew" lang="he" hreflang="he" data-title="הזיה (בינה מלאכותית)" data-language-autonym="עברית" data-language-local-name="Hebrew" class="interlanguage-link-target"><span>עברית</span></a></li><li class="interlanguage-link interwiki-kk mw-list-item"><a href="https://kk.wikipedia.org/wiki/%D0%93%D0%B0%D0%BB%D0%BB%D1%8E%D1%86%D0%B8%D0%BD%D0%B0%D1%86%D0%B8%D1%8F_(%D0%B6%D0%B0%D1%81%D0%B0%D0%BD%D0%B4%D1%8B_%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82)" title="Галлюцинация (жасанды интеллект) – Kazakh" lang="kk" hreflang="kk" data-title="Галлюцинация (жасанды интеллект)" data-language-autonym="Қазақша" data-language-local-name="Kazakh" class="interlanguage-link-target"><span>Қазақша</span></a></li><li class="interlanguage-link interwiki-lmo mw-list-item"><a href="https://lmo.wikipedia.org/wiki/Allucinazzion_(intelligenza_artificial)" title="Allucinazzion (intelligenza artificial) – Lombard" lang="lmo" hreflang="lmo" data-title="Allucinazzion (intelligenza artificial)" data-language-autonym="Lombard" data-language-local-name="Lombard" class="interlanguage-link-target"><span>Lombard</span></a></li><li class="interlanguage-link interwiki-nl mw-list-item"><a href="https://nl.wikipedia.org/wiki/Hallucinatie_(kunstmatige_intelligentie)" title="Hallucinatie (kunstmatige intelligentie) – Dutch" lang="nl" hreflang="nl" data-title="Hallucinatie (kunstmatige intelligentie)" data-language-autonym="Nederlands" data-language-local-name="Dutch" class="interlanguage-link-target"><span>Nederlands</span></a></li><li class="interlanguage-link interwiki-ja mw-list-item"><a href="https://ja.wikipedia.org/wiki/%E3%83%8F%E3%83%AB%E3%82%B7%E3%83%8D%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3_(%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD)" title="ハルシネーション (人工知能) – Japanese" lang="ja" hreflang="ja" data-title="ハルシネーション (人工知能)" data-language-autonym="日本語" data-language-local-name="Japanese" class="interlanguage-link-target"><span>日本語</span></a></li><li class="interlanguage-link interwiki-pl mw-list-item"><a href="https://pl.wikipedia.org/wiki/Halucynacja_(sztuczna_inteligencja)" title="Halucynacja (sztuczna inteligencja) – Polish" lang="pl" hreflang="pl" data-title="Halucynacja (sztuczna inteligencja)" data-language-autonym="Polski" data-language-local-name="Polish" class="interlanguage-link-target"><span>Polski</span></a></li><li class="interlanguage-link interwiki-pt mw-list-item"><a href="https://pt.wikipedia.org/wiki/Alucina%C3%A7%C3%A3o_(intelig%C3%AAncia_artificial)" title="Alucinação (inteligência artificial) – Portuguese" lang="pt" hreflang="pt" data-title="Alucinação (inteligência artificial)" data-language-autonym="Português" data-language-local-name="Portuguese" class="interlanguage-link-target"><span>Português</span></a></li><li class="interlanguage-link interwiki-ru mw-list-item"><a href="https://ru.wikipedia.org/wiki/%D0%93%D0%B0%D0%BB%D0%BB%D1%8E%D1%86%D0%B8%D0%BD%D0%B0%D1%86%D0%B8%D1%8F_(%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82)" title="Галлюцинация (искусственный интеллект) – Russian" lang="ru" hreflang="ru" data-title="Галлюцинация (искусственный интеллект)" data-language-autonym="Русский" data-language-local-name="Russian" class="interlanguage-link-target"><span>Русский</span></a></li><li class="interlanguage-link interwiki-sr mw-list-item"><a href="https://sr.wikipedia.org/wiki/Halucinacija_(ve%C5%A1ta%C4%8Dka_inteligencija)" title="Halucinacija (veštačka inteligencija) – Serbian" lang="sr" hreflang="sr" data-title="Halucinacija (veštačka inteligencija)" data-language-autonym="Српски / srpski" data-language-local-name="Serbian" class="interlanguage-link-target"><span>Српски / srpski</span></a></li><li class="interlanguage-link interwiki-fi mw-list-item"><a href="https://fi.wikipedia.org/wiki/Hallusinointi_(teko%C3%A4ly)" title="Hallusinointi (tekoäly) – Finnish" lang="fi" hreflang="fi" data-title="Hallusinointi (tekoäly)" data-language-autonym="Suomi" data-language-local-name="Finnish" class="interlanguage-link-target"><span>Suomi</span></a></li><li class="interlanguage-link interwiki-sv mw-list-item"><a href="https://sv.wikipedia.org/wiki/Hallucination_(artificiell_intelligens)" title="Hallucination (artificiell intelligens) – Swedish" lang="sv" hreflang="sv" data-title="Hallucination (artificiell intelligens)" data-language-autonym="Svenska" data-language-local-name="Swedish" class="interlanguage-link-target"><span>Svenska</span></a></li><li class="interlanguage-link interwiki-th mw-list-item"><a href="https://th.wikipedia.org/wiki/%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B8%AA%E0%B8%B2%E0%B8%97%E0%B8%AB%E0%B8%A5%E0%B8%AD%E0%B8%99_(%E0%B8%9B%E0%B8%B1%E0%B8%8D%E0%B8%8D%E0%B8%B2%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B8%94%E0%B8%B4%E0%B8%A9%E0%B8%90%E0%B9%8C)" title="ประสาทหลอน (ปัญญาประดิษฐ์) – Thai" lang="th" hreflang="th" data-title="ประสาทหลอน (ปัญญาประดิษฐ์)" data-language-autonym="ไทย" data-language-local-name="Thai" class="interlanguage-link-target"><span>ไทย</span></a></li><li class="interlanguage-link interwiki-tr mw-list-item"><a href="https://tr.wikipedia.org/wiki/Hal%C3%BCsinasyon_(yapay_zek%C3%A2)" title="Halüsinasyon (yapay zekâ) – Turkish" lang="tr" hreflang="tr" data-title="Halüsinasyon (yapay zekâ)" data-language-autonym="Türkçe" data-language-local-name="Turkish" class="interlanguage-link-target"><span>Türkçe</span></a></li><li class="interlanguage-link interwiki-uk mw-list-item"><a href="https://uk.wikipedia.org/wiki/%D0%93%D0%B0%D0%BB%D1%8E%D1%86%D0%B8%D0%BD%D0%B0%D1%86%D1%96%D1%8F_(%D1%88%D1%82%D1%83%D1%87%D0%BD%D0%B8%D0%B9_%D1%96%D0%BD%D1%82%D0%B5%D0%BB%D0%B5%D0%BA%D1%82)" title="Галюцинація (штучний інтелект) – Ukrainian" lang="uk" hreflang="uk" data-title="Галюцинація (штучний інтелект)" data-language-autonym="Українська" data-language-local-name="Ukrainian" class="interlanguage-link-target"><span>Українська</span></a></li><li class="interlanguage-link interwiki-zh-yue mw-list-item"><a href="https://zh-yue.wikipedia.org/wiki/%E5%B9%BB%E8%A6%BA_(%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD)" title="幻覺 (人工智能) – Cantonese" lang="yue" hreflang="yue" data-title="幻覺 (人工智能)" data-language-autonym="粵語" data-language-local-name="Cantonese" class="interlanguage-link-target"><span>粵語</span></a></li><li class="interlanguage-link interwiki-zh mw-list-item"><a href="https://zh.wikipedia.org/wiki/%E5%B9%BB%E8%A7%89_(%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD)" title="幻觉 (人工智能) – Chinese" lang="zh" hreflang="zh" data-title="幻觉 (人工智能)" data-language-autonym="中文" data-language-local-name="Chinese" class="interlanguage-link-target"><span>中文</span></a></li>
			</ul>
			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q116197048#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
		</div>

	</div>
</div>
</header>
				<div class="vector-page-toolbar vector-feature-custom-font-size-clientpref--excluded">
					<div class="vector-page-toolbar-container">
						<div id="left-navigation">
							<nav aria-label="Namespaces">
								
<div id="p-associated-pages" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-associated-pages"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-nstab-main" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/Hallucination_(artificial_intelligence)" title="View the content page [c]" accesskey="c"><span>Article</span></a></li><li id="ca-talk" class="vector-tab-noicon mw-list-item"><a href="/wiki/Talk:Hallucination_(artificial_intelligence)" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t"><span>Talk</span></a></li>
		</ul>
		
	</div>
</div>

								
<div id="vector-variants-dropdown" class="vector-dropdown emptyPortlet"  >
	<input type="checkbox" id="vector-variants-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-variants-dropdown" class="vector-dropdown-checkbox " aria-label="Change language variant"   >
	<label id="vector-variants-dropdown-label" for="vector-variants-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">English</span>
	</label>
	<div class="vector-dropdown-content">


					
<div id="p-variants" class="vector-menu mw-portlet mw-portlet-variants emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

				
	</div>
</div>

							</nav>
						</div>
						<div id="right-navigation" class="vector-collapsible">
							<nav aria-label="Views">
								
<div id="p-views" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-views"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-view" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/Hallucination_(artificial_intelligence)"><span>Read</span></a></li><li id="ca-edit" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-history" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

							</nav>
				
							<nav class="vector-page-tools-landmark" aria-label="Page tools">
								
<div id="vector-page-tools-dropdown" class="vector-dropdown vector-page-tools-dropdown"  >
	<input type="checkbox" id="vector-page-tools-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-tools-dropdown" class="vector-dropdown-checkbox "  aria-label="Tools"  >
	<label id="vector-page-tools-dropdown-label" for="vector-page-tools-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">Tools</span>
	</label>
	<div class="vector-dropdown-content">


									<div id="vector-page-tools-unpinned-container" class="vector-unpinned-container">
						
<div id="vector-page-tools" class="vector-page-tools vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-page-tools-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="page-tools-pinned"
	data-pinnable-element-id="vector-page-tools"
	data-pinned-container-id="vector-page-tools-pinned-container"
	data-unpinned-container-id="vector-page-tools-unpinned-container"
>
	<div class="vector-pinnable-header-label">Tools</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-page-tools.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-page-tools.unpin">hide</button>
</div>

	
<div id="p-cactions" class="vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-has-collapsible-items"  title="More options" >
	<div class="vector-menu-heading">
		Actions
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-more-view" class="selected vector-more-collapsible-item mw-list-item"><a href="/wiki/Hallucination_(artificial_intelligence)"><span>Read</span></a></li><li id="ca-more-edit" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-more-history" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=history"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-tb" class="vector-menu mw-portlet mw-portlet-tb"  >
	<div class="vector-menu-heading">
		General
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/Hallucination_(artificial_intelligence)" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/Hallucination_(artificial_intelligence)" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-upload" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u"><span>Upload file</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;oldid=1325695960" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=info" title="More information about this page"><span>Page information</span></a></li><li id="t-cite" class="mw-list-item"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Hallucination_%28artificial_intelligence%29&amp;id=1325695960&amp;wpFormIdentifier=titleform" title="Information on how to cite this page"><span>Cite this page</span></a></li><li id="t-urlshortener" class="mw-list-item"><a href="/w/index.php?title=Special:UrlShortener&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FHallucination_%28artificial_intelligence%29"><span>Get shortened URL</span></a></li><li id="t-urlshortener-qrcode" class="mw-list-item"><a href="/w/index.php?title=Special:QrCode&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FHallucination_%28artificial_intelligence%29"><span>Download QR code</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-coll-print_export" class="vector-menu mw-portlet mw-portlet-coll-print_export"  >
	<div class="vector-menu-heading">
		Print/export
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Hallucination_%28artificial_intelligence%29&amp;action=show-download-screen" title="Download this page as a PDF file"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-wikibase-otherprojects" class="vector-menu mw-portlet mw-portlet-wikibase-otherprojects"  >
	<div class="vector-menu-heading">
		In other projects
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li class="wb-otherproject-link wb-otherproject-commons mw-list-item"><a href="https://commons.wikimedia.org/wiki/Category:Hallucination_(artificial_intelligence)" hreflang="en"><span>Wikimedia Commons</span></a></li><li id="t-wikibase" class="wb-otherproject-link wb-otherproject-wikibase-dataitem mw-list-item"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q116197048" title="Structured data on this page hosted by Wikidata [g]" accesskey="g"><span>Wikidata item</span></a></li>
		</ul>
		
	</div>
</div>

</div>

									</div>
				
	</div>
</div>

							</nav>
						</div>
					</div>
				</div>
				<div class="vector-column-end no-font-mode-scale">
					<div class="vector-sticky-pinned-container">
						<nav class="vector-page-tools-landmark" aria-label="Page tools">
							<div id="vector-page-tools-pinned-container" class="vector-pinned-container">
				
							</div>
		</nav>
						<nav class="vector-appearance-landmark" aria-label="Appearance">
							<div id="vector-appearance-pinned-container" class="vector-pinned-container">
				<div id="vector-appearance" class="vector-appearance vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-appearance-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="appearance-pinned"
	data-pinnable-element-id="vector-appearance"
	data-pinned-container-id="vector-appearance-pinned-container"
	data-unpinned-container-id="vector-appearance-unpinned-container"
>
	<div class="vector-pinnable-header-label">Appearance</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-appearance.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-appearance.unpin">hide</button>
</div>


</div>

							</div>
		</nav>
					</div>
				</div>
				<div id="bodyContent" class="vector-body" aria-labelledby="firstHeading" data-mw-ve-target-container>
					<div class="vector-body-before-content">
							<div class="mw-indicators">
		</div>

						<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
					</div>
					<div id="contentSub"><div id="mw-content-subtitle"></div></div>
					
					
					<div id="mw-content-text" class="mw-body-content"><div class="mw-content-ltr mw-parser-output" lang="en" dir="ltr"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Erroneous AI-generated content</div>
<style data-mw-deduplicate="TemplateStyles:r1320445320">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}}</style><div role="note" class="hatnote navigation-not-searchable">This article is about the phenomenon of AI presenting fabricated information as fact. For the appearance of AI-induced psychosis in humans, see <a href="/wiki/Chatbot_psychosis" title="Chatbot psychosis">Chatbot psychosis</a>.</div>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1320445320" /><div role="note" class="hatnote navigation-not-searchable">Not to be confused with <a href="/wiki/Artificial_imagination" title="Artificial imagination">Artificial imagination</a>.</div>
<p class="mw-empty-elt">
</p>
<style data-mw-deduplicate="TemplateStyles:r1273380762/mw-parser-output/.tmulti">.mw-parser-output .tmulti .multiimageinner{display:flex;flex-direction:column}.mw-parser-output .tmulti .trow{display:flex;flex-direction:row;clear:left;flex-wrap:wrap;width:100%;box-sizing:border-box}.mw-parser-output .tmulti .tsingle{margin:1px;float:left}.mw-parser-output .tmulti .theader{clear:both;font-weight:bold;text-align:center;align-self:center;background-color:transparent;width:100%}.mw-parser-output .tmulti .thumbcaption{background-color:transparent}.mw-parser-output .tmulti .text-align-left{text-align:left}.mw-parser-output .tmulti .text-align-right{text-align:right}.mw-parser-output .tmulti .text-align-center{text-align:center}@media all and (max-width:720px){.mw-parser-output .tmulti .thumbinner{width:100%!important;box-sizing:border-box;max-width:none!important;align-items:center}.mw-parser-output .tmulti .trow{justify-content:center}.mw-parser-output .tmulti .tsingle{float:none!important;max-width:100%!important;box-sizing:border-box;text-align:center}.mw-parser-output .tmulti .tsingle .thumbcaption{text-align:left}.mw-parser-output .tmulti .trow>.thumbcaption{text-align:center}}@media screen{html.skin-theme-clientpref-night .mw-parser-output .tmulti .multiimageinner span:not(.skin-invert-image):not(.skin-invert):not(.bg-transparent) img{background-color:white}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .tmulti .multiimageinner span:not(.skin-invert-image):not(.skin-invert):not(.bg-transparent) img{background-color:white}}</style><div class="thumb tmulti tright"><div class="thumbinner multiimageinner" style="width:242px;max-width:242px"><div class="trow"><div class="tsingle" style="width:240px;max-width:240px"><div class="thumbimage" style="height:133px;overflow:hidden"><span typeof="mw:File"><span><video id="mwe_player_0" poster="//upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Photoreal-train.webm/238px--Photoreal-train.webm.jpg" controls="" preload="none" data-mw-tmh="" class="mw-file-element" width="238" height="134" data-durationhint="9" data-mwtitle="Photoreal-train.webm" data-mwprovider="wikimediacommons"><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/9/9c/Photoreal-train.webm/Photoreal-train.webm.480p.vp9.webm" type="video/webm; codecs=&quot;vp9, opus&quot;" data-transcodekey="480p.vp9.webm" data-width="854" data-height="480" /><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/9/9c/Photoreal-train.webm/Photoreal-train.webm.720p.vp9.webm" type="video/webm; codecs=&quot;vp9, opus&quot;" data-transcodekey="720p.vp9.webm" data-width="1280" data-height="720" /><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/9/9c/Photoreal-train.webm/Photoreal-train.webm.1080p.vp9.webm" type="video/webm; codecs=&quot;vp9, opus&quot;" data-transcodekey="1080p.vp9.webm" data-width="1920" data-height="1080" /><source src="//upload.wikimedia.org/wikipedia/commons/9/9c/Photoreal-train.webm" type="video/webm; codecs=&quot;vp9&quot;" data-width="1920" data-height="1080" /><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/9/9c/Photoreal-train.webm/Photoreal-train.webm.144p.mjpeg.mov" type="video/quicktime" data-transcodekey="144p.mjpeg.mov" data-width="256" data-height="144" /><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/9/9c/Photoreal-train.webm/Photoreal-train.webm.240p.vp9.webm" type="video/webm; codecs=&quot;vp9, opus&quot;" data-transcodekey="240p.vp9.webm" data-width="426" data-height="240" /><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/9/9c/Photoreal-train.webm/Photoreal-train.webm.360p.vp9.webm" type="video/webm; codecs=&quot;vp9, opus&quot;" data-transcodekey="360p.vp9.webm" data-width="640" data-height="360" /><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/9/9c/Photoreal-train.webm/Photoreal-train.webm.360p.webm" type="video/webm; codecs=&quot;vp8, vorbis&quot;" data-transcodekey="360p.webm" data-width="640" data-height="360" /></video></span></span></div><div class="thumbcaption">First-generation <a href="/wiki/Sora_(text-to-video_model)" title="Sora (text-to-video model)">Sora</a> video of the <a href="/wiki/Glenfinnan_Viaduct" title="Glenfinnan Viaduct">Glenfinnan Viaduct</a> in Scotland, incorrectly showing: a <a href="/wiki/Double-track_railway" title="Double-track railway">second track</a>, trains <a href="/wiki/Left-_and_right-hand_traffic#Rail_traffic" title="Left- and right-hand traffic">traveling on the right instead of the left</a>, a second chimney on its interpretation of the train <a href="/wiki/The_Jacobite_(steam_train)" title="The Jacobite (steam train)"><i>The Jacobite</i></a>, and some carriages much longer than others</div></div></div><div class="trow"><div class="tsingle" style="width:240px;max-width:240px"><div class="thumbimage" style="height:145px;overflow:hidden"><span typeof="mw:File"><a href="/wiki/File:Glenfinnan_Viaduct_-_2022_(cropped).jpg" class="mw-file-description"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Glenfinnan_Viaduct_-_2022_%28cropped%29.jpg/250px-Glenfinnan_Viaduct_-_2022_%28cropped%29.jpg" decoding="async" width="238" height="145" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Glenfinnan_Viaduct_-_2022_%28cropped%29.jpg/500px-Glenfinnan_Viaduct_-_2022_%28cropped%29.jpg 1.5x" data-file-width="1414" data-file-height="862" /></a></span></div><div class="thumbcaption">The real Glenfinnan Viaduct with <i>The Jacobite</i> on it</div></div></div></div></div>
<p>In the field of <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> (AI), a <b>hallucination</b> or <b>artificial hallucination</b> (also called <b>bullshitting</b>,<sup id="cite&#95;ref-Hicks&#95;Humphries&#95;Slater&#95;2024&#95;1-0" class="reference"><a href="#cite_note-Hicks_Humphries_Slater_2024-1"><span class="cite-bracket">&#91;</span>1<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-2" class="reference"><a href="#cite_note-2"><span class="cite-bracket">&#91;</span>2<span class="cite-bracket">&#93;</span></a></sup> <b>confabulation</b>,<sup id="cite&#95;ref-ars&#95;making&#95;things&#95;up&#95;3-0" class="reference"><a href="#cite_note-ars_making_things_up-3"><span class="cite-bracket">&#91;</span>3<span class="cite-bracket">&#93;</span></a></sup> or <b>delusion</b><sup id="cite&#95;ref-4" class="reference"><a href="#cite_note-4"><span class="cite-bracket">&#91;</span>4<span class="cite-bracket">&#93;</span></a></sup>) is a response generated by AI that contains false or <a href="/wiki/Misleading_information" class="mw-redirect" title="Misleading information">misleading information</a> presented as <a href="/wiki/Fact" title="Fact">fact</a>.<sup id="cite&#95;ref-5" class="reference"><a href="#cite_note-5"><span class="cite-bracket">&#91;</span>5<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-Ji&#95;Lee&#95;Frieske&#95;Survey&#95;of&#95;Hallucination&#95;6-0" class="reference"><a href="#cite_note-Ji_Lee_Frieske_Survey_of_Hallucination-6"><span class="cite-bracket">&#91;</span>6<span class="cite-bracket">&#93;</span></a></sup> This term draws a loose analogy with human psychology, where a <a href="/wiki/Hallucination" title="Hallucination">hallucination</a> typically involves false <i><a href="/wiki/Percept#Process_and_terminology" class="mw-redirect" title="Percept">percepts</a></i>. However, there is a key difference: AI hallucination is associated with erroneously constructed responses (confabulation), rather than perceptual experiences.<sup id="cite&#95;ref-Ji&#95;Lee&#95;Frieske&#95;Survey&#95;of&#95;Hallucination&#95;6-1" class="reference"><a href="#cite_note-Ji_Lee_Frieske_Survey_of_Hallucination-6"><span class="cite-bracket">&#91;</span>6<span class="cite-bracket">&#93;</span></a></sup> 
</p><p>For example, a <a href="/wiki/Chatbot" title="Chatbot">chatbot</a> powered by <a href="/wiki/Large_language_model" title="Large language model">large language models</a> (LLMs), like <a href="/wiki/ChatGPT" title="ChatGPT">ChatGPT</a>, may embed plausible-sounding random falsehoods within its generated content. Detecting and mitigating errors and hallucinations pose significant challenges for practical deployment and reliability of LLMs in high-stakes scenarios, such as chip design, supply chain logistics, and medical diagnostics.<sup id="cite&#95;ref-nyt&#95;7-0" class="reference"><a href="#cite_note-nyt-7"><span class="cite-bracket">&#91;</span>7<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-de&#95;Wynter-2023&#95;8-0" class="reference"><a href="#cite_note-de_Wynter-2023-8"><span class="cite-bracket">&#91;</span>8<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-cnbc&#95;several&#95;errors&#95;9-0" class="reference"><a href="#cite_note-cnbc_several_errors-9"><span class="cite-bracket">&#91;</span>9<span class="cite-bracket">&#93;</span></a></sup> Some software engineers and statisticians have criticized the specific term "AI hallucination" for unreasonably <a href="/wiki/Anthropomorphism#In_computing" title="Anthropomorphism">anthropomorphizing computers</a>.<sup id="cite&#95;ref-sigplan&#95;10-0" class="reference"><a href="#cite_note-sigplan-10"><span class="cite-bracket">&#91;</span>10<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-salon&#95;11-0" class="reference"><a href="#cite_note-salon-11"><span class="cite-bracket">&#91;</span>11<span class="cite-bracket">&#93;</span></a></sup>
</p>
<meta property="mw:PageProp/toc" />
<div class="mw-heading mw-heading2"><h2 id="Term">Term</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=1" title="Edit section: Term"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<div class="mw-heading mw-heading3"><h3 id="Origin">Origin</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=2" title="Edit section: Origin"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Since the 1980s, the term "hallucination" has been used in <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> with a positive connotation to describe the process of adding detail to an image. For example, the task of generating high-resolution face images from low-resolution inputs is called <a href="/wiki/Face_hallucination" title="Face hallucination">face hallucination</a>.<sup id="cite&#95;ref-Maleki&#95;Padmanabhan&#95;AI&#95;Hallucinations&#95;12-0" class="reference"><a href="#cite_note-Maleki_Padmanabhan_AI_Hallucinations-12"><span class="cite-bracket">&#91;</span>12<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-13" class="reference"><a href="#cite_note-13"><span class="cite-bracket">&#91;</span>13<span class="cite-bracket">&#93;</span></a></sup> The first documented use of the term "hallucination" in this sense is in the PhD thesis of Eric Mjolsness in 1986.<sup id="cite&#95;ref-14" class="reference"><a href="#cite_note-14"><span class="cite-bracket">&#91;</span>14<span class="cite-bracket">&#93;</span></a></sup> A notable work is the face hallucination algorithm by Simon Baker and <a href="/wiki/Takeo_Kanade" title="Takeo Kanade">Takeo Kanade</a> published in 1999.<sup id="cite&#95;ref-15" class="reference"><a href="#cite_note-15"><span class="cite-bracket">&#91;</span>15<span class="cite-bracket">&#93;</span></a></sup>
</p><p>In 1995, Stephen Thaler demonstrated how hallucinations and phantom experiences emerge from <a href="/wiki/Artificial_neural_networks" class="mw-redirect" title="Artificial neural networks">artificial neural networks</a> through random perturbation of their connection weights.<sup id="cite&#95;ref-16" class="reference"><a href="#cite_note-16"><span class="cite-bracket">&#91;</span>16<span class="cite-bracket">&#93;</span></a></sup>
</p><p>In the 2000s, hallucinations were described in <a href="/wiki/Statistical_machine_translation" title="Statistical machine translation">statistical machine translation</a> as a failure mode.<sup id="cite&#95;ref-17" class="reference"><a href="#cite_note-17"><span class="cite-bracket">&#91;</span>17<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Since the 2010s, the term underwent a <a href="/wiki/Semantic_shift" class="mw-redirect" title="Semantic shift">semantic shift</a> to signify the generation of factually incorrect or misleading outputs by AI systems in tasks like <a href="/wiki/Machine_translation" title="Machine translation">machine translation</a> and <a href="/wiki/Object_detection" title="Object detection">object detection</a>.<sup id="cite&#95;ref-Maleki&#95;Padmanabhan&#95;AI&#95;Hallucinations&#95;12-1" class="reference"><a href="#cite_note-Maleki_Padmanabhan_AI_Hallucinations-12"><span class="cite-bracket">&#91;</span>12<span class="cite-bracket">&#93;</span></a></sup> In 2015, hallucinations were identified in visual <a href="/wiki/Semantic_role_labeling" title="Semantic role labeling">semantic role labeling</a> tasks by Saurabh Gupta and <a href="/wiki/Jitendra_Malik" title="Jitendra Malik">Jitendra Malik</a>.<sup id="cite&#95;ref-18" class="reference"><a href="#cite_note-18"><span class="cite-bracket">&#91;</span>18<span class="cite-bracket">&#93;</span></a></sup> In 2017, Google researchers used the term to describe the responses generated by <a href="/wiki/Neural_machine_translation" title="Neural machine translation">neural machine translation</a> (NMT) models when they are not related to the source text,<sup id="cite&#95;ref-19" class="reference"><a href="#cite_note-19"><span class="cite-bracket">&#91;</span>19<span class="cite-bracket">&#93;</span></a></sup> and in 2018, the term was used in computer vision to describe instances where non-existent objects are erroneously detected because of adversarial attacks.<sup id="cite&#95;ref-Simonite2018&#95;20-0" class="reference"><a href="#cite_note-Simonite2018-20"><span class="cite-bracket">&#91;</span>20<span class="cite-bracket">&#93;</span></a></sup>
</p><p>The term "hallucinations" in AI gained wider recognition during the <a href="/wiki/AI_boom" title="AI boom">AI boom</a>, alongside the rollout of widely used chatbots based on large language models (LLMs).<sup id="cite&#95;ref-21" class="reference"><a href="#cite_note-21"><span class="cite-bracket">&#91;</span>21<span class="cite-bracket">&#93;</span></a></sup> In July 2021, <a href="/wiki/Meta_Platforms" title="Meta Platforms">Meta</a> warned during its release of BlenderBot 2 that the system is prone to "hallucinations", which Meta defined as "confident statements that are not true".<sup id="cite&#95;ref-22" class="reference"><a href="#cite_note-22"><span class="cite-bracket">&#91;</span>22<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-23" class="reference"><a href="#cite_note-23"><span class="cite-bracket">&#91;</span>23<span class="cite-bracket">&#93;</span></a></sup> Following <a href="/wiki/OpenAI" title="OpenAI">OpenAI</a>'s <a href="/wiki/ChatGPT" title="ChatGPT">ChatGPT</a> release in beta version in November 2022, some users complained that such chatbots often seem to pointlessly embed plausible-sounding random falsehoods within their generated content.<sup id="cite&#95;ref-24" class="reference"><a href="#cite_note-24"><span class="cite-bracket">&#91;</span>24<span class="cite-bracket">&#93;</span></a></sup> Many news outlets, including <i><a href="/wiki/The_New_York_Times" title="The New York Times">The New York Times</a></i>, started to use the term "hallucinations" to describe these models' occasionally incorrect or inconsistent responses.<sup id="cite&#95;ref-25" class="reference"><a href="#cite_note-25"><span class="cite-bracket">&#91;</span>25<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Some researchers have highlighted a lack of consistency in how the term is used, but also identified several alternative terms in the literature, such as confabulations, fabrications, and factual errors.<sup id="cite&#95;ref-Maleki&#95;Padmanabhan&#95;AI&#95;Hallucinations&#95;12-2" class="reference"><a href="#cite_note-Maleki_Padmanabhan_AI_Hallucinations-12"><span class="cite-bracket">&#91;</span>12<span class="cite-bracket">&#93;</span></a></sup>
</p><p>In 2023, the Cambridge dictionary updated its definition of hallucination to include this new sense specific to the field of AI.<sup id="cite&#95;ref-26" class="reference"><a href="#cite_note-26"><span class="cite-bracket">&#91;</span>26<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Definitions_and_alternatives">Definitions and alternatives</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=3" title="Edit section: Definitions and alternatives"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:AI_Overviews_result_for_Joaquin_Correa_brother,_10_August_2025.jpg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/3/3f/AI_Overviews_result_for_Joaquin_Correa_brother%2C_10_August_2025.jpg/250px-AI_Overviews_result_for_Joaquin_Correa_brother%2C_10_August_2025.jpg" decoding="async" width="250" height="378" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3f/AI_Overviews_result_for_Joaquin_Correa_brother%2C_10_August_2025.jpg/500px-AI_Overviews_result_for_Joaquin_Correa_brother%2C_10_August_2025.jpg 1.5x" data-file-width="1080" data-file-height="1635" /></a><figcaption><a href="/wiki/AI_Overviews" class="mw-redirect" title="AI Overviews">AI Overviews</a> result (10 August 2025) incorrectly stating that <a href="/wiki/Joaqu%C3%ADn_Correa" title="Joaquín Correa">Joaquín Correa</a> is the brother of <a href="/wiki/%C3%81ngel_Correa" title="Ángel Correa">Ángel Correa</a>; the two are unrelated.<sup id="cite&#95;ref-27" class="reference"><a href="#cite_note-27"><span class="cite-bracket">&#91;</span>27<span class="cite-bracket">&#93;</span></a></sup></figcaption></figure>
<p>Uses, definitions and characterizations of the term "hallucination" in the context of LLMs include:
</p>
<ul><li>"a tendency to invent facts in moments of uncertainty" (OpenAI, May 2023)<sup id="cite&#95;ref-cnbc&#95;new&#95;way&#95;28-0" class="reference"><a href="#cite_note-cnbc_new_way-28"><span class="cite-bracket">&#91;</span>28<span class="cite-bracket">&#93;</span></a></sup></li>
<li>"a model's logical mistakes" (OpenAI, May 2023)<sup id="cite&#95;ref-cnbc&#95;new&#95;way&#95;28-1" class="reference"><a href="#cite_note-cnbc_new_way-28"><span class="cite-bracket">&#91;</span>28<span class="cite-bracket">&#93;</span></a></sup></li>
<li>"fabricating information entirely, but behaving as if spouting facts" (<a href="/wiki/CNBC" title="CNBC">CNBC</a>, May 2023)<sup id="cite&#95;ref-cnbc&#95;new&#95;way&#95;28-2" class="reference"><a href="#cite_note-cnbc_new_way-28"><span class="cite-bracket">&#91;</span>28<span class="cite-bracket">&#93;</span></a></sup></li>
<li>"making up information" (<i><a href="/wiki/The_Verge" title="The Verge">The Verge</a></i>, February 2023)<sup id="cite&#95;ref-29" class="reference"><a href="#cite_note-29"><span class="cite-bracket">&#91;</span>29<span class="cite-bracket">&#93;</span></a></sup></li>
<li>"probability distributions" (in scientific contexts)<sup id="cite&#95;ref-nyt-science&#95;30-0" class="reference"><a href="#cite_note-nyt-science-30"><span class="cite-bracket">&#91;</span>30<span class="cite-bracket">&#93;</span></a></sup></li></ul>
<p>Journalist Benj Edwards, in <i><a href="/wiki/Ars_Technica" title="Ars Technica">Ars Technica</a></i>, writes that the term "hallucination" is controversial, but that some form of metaphor remains necessary; Edwards suggests "<a href="/wiki/Confabulation" title="Confabulation">confabulation</a>" as an analogy for processes that involve "creative gap-filling".<sup id="cite&#95;ref-ars&#95;making&#95;things&#95;up&#95;3-1" class="reference"><a href="#cite_note-ars_making_things_up-3"><span class="cite-bracket">&#91;</span>3<span class="cite-bracket">&#93;</span></a></sup> In July 2024, a White House report on fostering public trust in AI research mentioned hallucinations only in the context of reducing them. Notably, when acknowledging <a href="/wiki/David_Baker_(biochemist)" title="David Baker (biochemist)">David Baker</a>'s Nobel Prize-winning work with AI-generated proteins, the Nobel committee avoided the term entirely, instead referring to "imaginative protein creation".<sup id="cite&#95;ref-nyt-science&#95;30-1" class="reference"><a href="#cite_note-nyt-science-30"><span class="cite-bracket">&#91;</span>30<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Hicks, Humphries, and Slater, in their article in <i><a href="/wiki/Ethics_and_Information_Technology" title="Ethics and Information Technology">Ethics and Information Technology</a></i>, argue that the output of LLMs is "<a href="/wiki/Bullshit" title="Bullshit">bullshit</a>" under <a href="/wiki/On_Bullshit" title="On Bullshit">Harry Frankfurt's definition of the term</a>, and that the models are "in an important way indifferent to the truth of their outputs", with true statements only accidentally true, and false ones accidentally false.<sup id="cite&#95;ref-Hicks&#95;Humphries&#95;Slater&#95;2024&#95;1-1" class="reference"><a href="#cite_note-Hicks_Humphries_Slater_2024-1"><span class="cite-bracket">&#91;</span>1<span class="cite-bracket">&#93;</span></a></sup><sup class="reference nowrap"><span title="Page: 9">&#58;&#8202;9&#8202;</span></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Criticism">Criticism</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=4" title="Edit section: Criticism"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>In the scientific community, some researchers avoid the term "hallucination", seeing it as potentially misleading. It has been criticized by <a href="/wiki/Usama_Fayyad" title="Usama Fayyad">Usama Fayyad</a>, executive director of the Institute for Experimental Artificial Intelligence at <a href="/wiki/Northeastern_University" title="Northeastern University">Northeastern University</a>, on the grounds that it misleadingly personifies large language models and is vague.<sup id="cite&#95;ref-31" class="reference"><a href="#cite_note-31"><span class="cite-bracket">&#91;</span>31<span class="cite-bracket">&#93;</span></a></sup> <a href="/wiki/Mary_Shaw_(computer_scientist)" title="Mary Shaw (computer scientist)">Mary Shaw</a> said, "The current fashion for calling generative AI's errors 'hallucinations' is appalling. It anthropomorphizes the software, and it spins actual errors as somehow being idiosyncratic quirks of the system even when they're objectively incorrect."<sup id="cite&#95;ref-sigplan&#95;10-1" class="reference"><a href="#cite_note-sigplan-10"><span class="cite-bracket">&#91;</span>10<span class="cite-bracket">&#93;</span></a></sup> In <i><a href="/wiki/Salon_(magazine)" class="mw-redirect" title="Salon (magazine)">Salon</a></i>, statistician Gary N. Smith argues that LLMs "do not understand what words mean" and consequently that the term "hallucination" unreasonably anthropomorphizes the machine.<sup id="cite&#95;ref-salon&#95;11-1" class="reference"><a href="#cite_note-salon-11"><span class="cite-bracket">&#91;</span>11<span class="cite-bracket">&#93;</span></a></sup> Some see the AI outputs not as illusory but as prospective—that is, having some chance of being true, similar to early-stage scientific conjectures. The term has also been criticized for its association with psychedelic drug experiences.<sup id="cite&#95;ref-nyt-science&#95;30-2" class="reference"><a href="#cite_note-nyt-science-30"><span class="cite-bracket">&#91;</span>30<span class="cite-bracket">&#93;</span></a></sup> 
</p>
<div class="mw-heading mw-heading2"><h2 id="In_natural_language_generation">In natural language generation</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=5" title="Edit section: In natural language generation"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:ChatPGTLojbanLion123.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/1/1f/ChatPGTLojbanLion123.png/330px-ChatPGTLojbanLion123.png" decoding="async" width="330" height="234" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/1f/ChatPGTLojbanLion123.png/500px-ChatPGTLojbanLion123.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/1f/ChatPGTLojbanLion123.png/960px-ChatPGTLojbanLion123.png 2x" data-file-width="1071" data-file-height="761" /></a><figcaption>A translation on the <a href="/wiki/Vicuna_LLM" title="Vicuna LLM">Vicuna LLM</a> <a href="/wiki/Test_bed" class="mw-redirect" title="Test bed">test bed</a> of English into the <a href="/wiki/Constructed_language" title="Constructed language">constructed language</a> <a href="/wiki/Lojban" title="Lojban">Lojban</a>, and then back into English in a new round, generates a <a href="/wiki/Surrealism" title="Surrealism">surreal</a> artifact from Genesis 1:6 (<a href="/wiki/Revised_Standard_Version" title="Revised Standard Version">RSV</a>).</figcaption></figure>
<p>In <a href="/wiki/Natural_language_generation" title="Natural language generation">natural language generation</a>, a hallucination is often defined as "generated content that appears factual but is ungrounded".<sup id="cite&#95;ref-32" class="reference"><a href="#cite_note-32"><span class="cite-bracket">&#91;</span>32<span class="cite-bracket">&#93;</span></a></sup> There are different ways to categorize hallucinations. Depending on whether the output contradicts the source or cannot be verified from the source, they are divided into intrinsic and extrinsic, respectively.<sup id="cite&#95;ref-Ji&#95;Lee&#95;Frieske&#95;Survey&#95;of&#95;Hallucination&#95;6-2" class="reference"><a href="#cite_note-Ji_Lee_Frieske_Survey_of_Hallucination-6"><span class="cite-bracket">&#91;</span>6<span class="cite-bracket">&#93;</span></a></sup> Depending on whether the output contradicts the prompt or not, they could be divided into closed-domain and open-domain, respectively.<sup id="cite&#95;ref-33" class="reference"><a href="#cite_note-33"><span class="cite-bracket">&#91;</span>33<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Causes">Causes</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=6" title="Edit section: Causes"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>There are several reasons why natural language models hallucinate:<sup id="cite&#95;ref-Ji&#95;Lee&#95;Frieske&#95;Survey&#95;of&#95;Hallucination&#95;6-3" class="reference"><a href="#cite_note-Ji_Lee_Frieske_Survey_of_Hallucination-6"><span class="cite-bracket">&#91;</span>6<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-34" class="reference"><a href="#cite_note-34"><span class="cite-bracket">&#91;</span>34<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading4"><h4 id="Hallucination_from_data">Hallucination from data</h4><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=7" title="Edit section: Hallucination from data"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Hallucinations can stem from incomplete, inaccurate or unrepresentative data sets.<sup id="cite&#95;ref-35" class="reference"><a href="#cite_note-35"><span class="cite-bracket">&#91;</span>35<span class="cite-bracket">&#93;</span></a></sup> One possible cause is source-reference divergence. This divergence may occur as an artifact of heuristic data collection or due to the nature of some natural language generation tasks that inevitably contain such divergence. When a model is trained on data with source-reference (target) divergence, the model can be encouraged to generate text that is not necessarily grounded and not faithful to the provided source.<sup id="cite&#95;ref-Ji&#95;Lee&#95;Frieske&#95;Survey&#95;of&#95;Hallucination&#95;6-4" class="reference"><a href="#cite_note-Ji_Lee_Frieske_Survey_of_Hallucination-6"><span class="cite-bracket">&#91;</span>6<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading4"><h4 id="Modeling-related_causes">Modeling-related causes</h4><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=8" title="Edit section: Modeling-related causes"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>The pre-training of <a href="/wiki/Generative_pre-trained_transformer" title="Generative pre-trained transformer">generative pretrained transformers</a> (GPT) involves predicting the next word. It incentivizes GPT models to "give a guess" about what the next word is, even when they lack information.<sup id="cite&#95;ref-:1&#95;36-0" class="reference"><a href="#cite_note-:1-36"><span class="cite-bracket">&#91;</span>36<span class="cite-bracket">&#93;</span></a></sup> After pre-training, though, hallucinations can be mitigated through anti-hallucination <a href="/wiki/Fine-tuning_(deep_learning)" title="Fine-tuning (deep learning)">fine-tuning</a><sup id="cite&#95;ref-37" class="reference"><a href="#cite_note-37"><span class="cite-bracket">&#91;</span>37<span class="cite-bracket">&#93;</span></a></sup> (such as with <a href="/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">reinforcement learning from human feedback</a>). Some researchers take an anthropomorphic perspective and posit that hallucinations arise from a tension between <a href="/wiki/Novelty" title="Novelty">novelty</a> and usefulness. For instance, <a href="/wiki/Teresa_Amabile" title="Teresa Amabile">Teresa Amabile</a> and Pratt define human creativity as the production of novel and useful ideas.<sup id="cite&#95;ref-38" class="reference"><a href="#cite_note-38"><span class="cite-bracket">&#91;</span>38<span class="cite-bracket">&#93;</span></a></sup> By extension, a focus on novelty in machine creativity can lead to the production of original but inaccurate responses—that is, falsehoods—whereas a focus on usefulness may result in memorized content lacking originality.<sup id="cite&#95;ref-39" class="reference"><a href="#cite_note-39"><span class="cite-bracket">&#91;</span>39<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Pre-training of models on a large corpus is known to result in the model memorizing knowledge in its parameters, creating hallucinations if the system is overconfident in its knowledge. In systems such as GPT-3, an AI generates each next word based on a sequence of previous words (including the words it has itself previously generated during the same conversation), causing a cascade of possible hallucinations as the response grows longer.<sup id="cite&#95;ref-Ji&#95;Lee&#95;Frieske&#95;Survey&#95;of&#95;Hallucination&#95;6-5" class="reference"><a href="#cite_note-Ji_Lee_Frieske_Survey_of_Hallucination-6"><span class="cite-bracket">&#91;</span>6<span class="cite-bracket">&#93;</span></a></sup> By 2022, newspapers such as <i><a href="/wiki/The_New_York_Times" title="The New York Times">The New York Times</a></i> expressed concern that, as the adoption of bots based on large language models continued to grow, unwarranted user confidence in bot output could lead to problems.<sup id="cite&#95;ref-40" class="reference"><a href="#cite_note-40"><span class="cite-bracket">&#91;</span>40<span class="cite-bracket">&#93;</span></a></sup>
</p><p>For models that also have an encoder (unlike GPTs), errors in encoding and decoding between text and representations can cause hallucinations. When encoders learn the wrong correlations between different parts of the training data, it can result in an erroneous generation that diverges from the input. The decoder takes the encoded input from the encoder and generates the final target sequence. Two aspects of decoding contribute to hallucinations. First, decoders can attend to the wrong part of the encoded input source, leading to erroneous generation. Second, the design of the decoding strategy itself can contribute to hallucinations. A decoding strategy that improves generation diversity, such as top-k sampling, is positively correlated with increased hallucination.<sup id="cite&#95;ref-41" class="reference"><a href="#cite_note-41"><span class="cite-bracket">&#91;</span>41<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading4"><h4 id="Interpretability_research">Interpretability research</h4><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=9" title="Edit section: Interpretability research"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>In 2025, <a href="/wiki/Interpretability_(machine_learning)" class="mw-redirect" title="Interpretability (machine learning)">interpretability</a> research by <a href="/wiki/Anthropic" title="Anthropic">Anthropic</a> on the LLM <a href="/wiki/Claude_(language_model)" title="Claude (language model)">Claude</a> identified internal circuits that cause it to decline to answer questions unless it knows the answer. By default, the circuit is active and the LLM doesn't answer. When the LLM has sufficient information, these circuits are inhibited and the LLM answers the question. Hallucinations were found to occur when this inhibition happens incorrectly, such as when Claude recognizes a name but lacks sufficient information about that person, causing it to generate plausible but untrue responses.<sup id="cite&#95;ref-42" class="reference"><a href="#cite_note-42"><span class="cite-bracket">&#91;</span>42<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Examples">Examples</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=10" title="Edit section: Examples"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>On 15 November 2022, researchers from <a href="/wiki/Meta_AI" title="Meta AI">Meta AI</a> published Galactica,<sup id="cite&#95;ref-43" class="reference"><a href="#cite_note-43"><span class="cite-bracket">&#91;</span>43<span class="cite-bracket">&#93;</span></a></sup> designed to "store, combine and reason about scientific knowledge". Content generated by Galactica came with the warning: "Outputs may be unreliable! Language Models are prone to hallucinate text." In one case, when asked to draft a paper on creating avatars, Galactica cited a fictitious paper from a real author who works in the relevant area. Meta withdrew Galactica on 17 November due to offensiveness and inaccuracy.<sup id="cite&#95;ref-44" class="reference"><a href="#cite_note-44"><span class="cite-bracket">&#91;</span>44<span class="cite-bracket">&#93;</span></a></sup> Before the cancellation, researchers were working on Galactica Instruct, which would use <a href="/wiki/Instruction_tuning" class="mw-redirect" title="Instruction tuning">instruction tuning</a> to allow the model to follow instructions to manipulate <a href="/wiki/LaTeX" title="LaTeX">LaTeX</a> documents on <a href="/wiki/Overleaf" title="Overleaf">Overleaf</a>.<sup id="cite&#95;ref-45" class="reference"><a href="#cite_note-45"><span class="cite-bracket">&#91;</span>45<span class="cite-bracket">&#93;</span></a></sup>
</p><p><a href="/wiki/OpenAI" title="OpenAI">OpenAI</a>'s <a href="/wiki/ChatGPT" title="ChatGPT">ChatGPT</a>, released in beta version to the public on November 30, 2022, was based on the <a href="/wiki/Foundation_model" title="Foundation model">foundation model</a> GPT-3.5 (a revision of GPT-3). Professor Ethan Mollick of <a href="/wiki/Wharton_School_of_the_University_of_Pennsylvania" class="mw-redirect" title="Wharton School of the University of Pennsylvania">Wharton</a> called it an "omniscient, eager-to-please intern who sometimes lies to you". Data scientist Teresa Kubacka has recounted deliberately making up the phrase "cycloidal inverted electromagnon" and testing ChatGPT by asking it about the (nonexistent) phenomenon. ChatGPT invented a plausible-sounding answer backed with plausible-looking citations that compelled her to double-check whether she had accidentally typed in the name of a real phenomenon. Other scholars such as <a href="/wiki/Oren_Etzioni" title="Oren Etzioni">Oren Etzioni</a> have joined Kubacka in assessing that such software can often give "a very impressive-sounding answer that's just dead wrong".<sup id="cite&#95;ref-46" class="reference"><a href="#cite_note-46"><span class="cite-bracket">&#91;</span>46<span class="cite-bracket">&#93;</span></a></sup>
</p><p>When <a href="/wiki/CNBC" title="CNBC">CNBC</a> asked ChatGPT for the lyrics to "<a href="/wiki/Ballad_of_Dwight_Fry" class="mw-redirect" title="Ballad of Dwight Fry">Ballad of Dwight Fry</a>", ChatGPT supplied invented lyrics rather than the actual lyrics.<sup id="cite&#95;ref-47" class="reference"><a href="#cite_note-47"><span class="cite-bracket">&#91;</span>47<span class="cite-bracket">&#93;</span></a></sup> Asked questions about the Canadian province of <a href="/wiki/New_Brunswick" title="New Brunswick">New Brunswick</a>, ChatGPT got many answers right but incorrectly classified Toronto-born <a href="/wiki/Samantha_Bee" title="Samantha Bee">Samantha Bee</a> as a "person from New Brunswick".<sup id="cite&#95;ref-48" class="reference"><a href="#cite_note-48"><span class="cite-bracket">&#91;</span>48<span class="cite-bracket">&#93;</span></a></sup> Asked about astrophysical magnetic fields, ChatGPT incorrectly volunteered that "(strong) magnetic fields of <a href="/wiki/Black_holes" class="mw-redirect" title="Black holes">black holes</a> are generated by the extremely strong gravitational forces in their vicinity". (In reality, as a consequence of the <a href="/wiki/No-hair_theorem" title="No-hair theorem">no-hair theorem</a>, a black hole without an accretion disk is believed to have no magnetic field.)<sup id="cite&#95;ref-49" class="reference"><a href="#cite_note-49"><span class="cite-bracket">&#91;</span>49<span class="cite-bracket">&#93;</span></a></sup> <i><a href="/wiki/Fast_Company" title="Fast Company">Fast Company</a></i> asked ChatGPT to generate a news article on Tesla's last financial quarter; ChatGPT created a coherent article, but made up the financial numbers contained within.<sup id="cite&#95;ref-fast&#95;company&#95;2022&#95;50-0" class="reference"><a href="#cite_note-fast_company_2022-50"><span class="cite-bracket">&#91;</span>50<span class="cite-bracket">&#93;</span></a></sup>
</p>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:ChatGPT_hallucination.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/3/3a/ChatGPT_hallucination.png/250px-ChatGPT_hallucination.png" decoding="async" width="250" height="128" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3a/ChatGPT_hallucination.png/500px-ChatGPT_hallucination.png 1.5x" data-file-width="901" data-file-height="463" /></a><figcaption>When prompted to "summarize an article" with a fake URL that contains meaningful keywords, even with no Internet connection, the chatbot generates a response that seems valid at first glance.</figcaption></figure>
<p>Other examples involve baiting ChatGPT with a false premise to see if it embellishes upon the premise. When asked about "<a href="/wiki/Harold_Coward" title="Harold Coward">Harold Coward</a>'s idea of dynamic canonicity", ChatGPT fabricated that Coward wrote a book titled <i>Dynamic Canonicity: A Model for Biblical and Theological Interpretation</i>, arguing that religious principles are actually in a constant state of change. When pressed, ChatGPT continued to insist that the book was real.<sup id="cite&#95;ref-51" class="reference"><a href="#cite_note-51"><span class="cite-bracket">&#91;</span>51<span class="cite-bracket">&#93;</span></a></sup> Asked for proof that dinosaurs built a civilization, ChatGPT claimed there were fossil remains of dinosaur tools and stated, "Some species of dinosaurs even developed primitive forms of art, such as engravings on stones".<sup id="cite&#95;ref-52" class="reference"><a href="#cite_note-52"><span class="cite-bracket">&#91;</span>52<span class="cite-bracket">&#93;</span></a></sup> When prompted that "Scientists have recently discovered <a href="/wiki/Churros" class="mw-redirect" title="Churros">churros</a>, the delicious fried-dough pastries&#160;... (are) ideal tools for home surgery", ChatGPT claimed that a "study published in the journal <i><a href="/wiki/Science_(journal)" title="Science (journal)">Science</a></i><span style="padding-left:.15em;">"</span> found that the dough is pliable enough to form into surgical instruments that can get into hard-to-reach places, and that the flavor has a calming effect on patients.<sup id="cite&#95;ref-53" class="reference"><a href="#cite_note-53"><span class="cite-bracket">&#91;</span>53<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-54" class="reference"><a href="#cite_note-54"><span class="cite-bracket">&#91;</span>54<span class="cite-bracket">&#93;</span></a></sup>
</p><p>By 2023, analysts considered frequent hallucination to be a major problem in LLM technology, with a Google executive identifying hallucination reduction as a "fundamental" task for ChatGPT competitor <a href="/wiki/Google_Gemini" title="Google Gemini">Google Gemini</a>.<sup id="cite&#95;ref-cnbc&#95;several&#95;errors&#95;9-1" class="reference"><a href="#cite_note-cnbc_several_errors-9"><span class="cite-bracket">&#91;</span>9<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-55" class="reference"><a href="#cite_note-55"><span class="cite-bracket">&#91;</span>55<span class="cite-bracket">&#93;</span></a></sup> A 2023 demo for Microsoft's GPT-based <a href="/wiki/Bing_AI" class="mw-redirect" title="Bing AI">Bing AI</a> appeared to contain several hallucinations that went uncaught by the presenter.<sup id="cite&#95;ref-cnbc&#95;several&#95;errors&#95;9-2" class="reference"><a href="#cite_note-cnbc_several_errors-9"><span class="cite-bracket">&#91;</span>9<span class="cite-bracket">&#93;</span></a></sup>
</p><p>In May 2023, it was discovered that Stephen Schwartz had submitted six fake case precedents generated by ChatGPT in his <a href="/wiki/Brief_(law)" title="Brief (law)">brief</a> to the <a href="/wiki/Southern_District_of_New_York" title="Southern District of New York">Southern District of New York</a> on <i><a href="/wiki/Mata_v._Avianca,_Inc." title="Mata v. Avianca, Inc.">Mata v. Avianca, Inc.</a></i>, a <a href="/wiki/Personal_injury" title="Personal injury">personal injury</a> case against the airline <a href="/wiki/Avianca" title="Avianca">Avianca</a>. Schwartz said that he had never previously used ChatGPT, that he did not recognize the possibility that ChatGPT's output could have been fabricated, and that ChatGPT continued to assert the authenticity of the precedents after their nonexistence was discovered.<sup id="cite&#95;ref-56" class="reference"><a href="#cite_note-56"><span class="cite-bracket">&#91;</span>56<span class="cite-bracket">&#93;</span></a></sup> In response, <a href="/wiki/Brantley_Starr" title="Brantley Starr">Brantley Starr</a> of the <a href="/wiki/Northern_District_of_Texas" class="mw-redirect" title="Northern District of Texas">Northern District of Texas</a> banned the submission of AI-generated case filings that have not been reviewed by a human, noting that:<sup id="cite&#95;ref-57" class="reference"><a href="#cite_note-57"><span class="cite-bracket">&#91;</span>57<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-58" class="reference"><a href="#cite_note-58"><span class="cite-bracket">&#91;</span>58<span class="cite-bracket">&#93;</span></a></sup>
</p>
<style data-mw-deduplicate="TemplateStyles:r1244412712">.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}}</style><blockquote class="templatequote"><p><a href="/wiki/Generative_artificial_intelligence" title="Generative artificial intelligence">Generative artificial intelligence</a> platforms in their current states are prone to hallucinations and <a href="/wiki/Algorithmic_bias" title="Algorithmic bias">bias</a>. On hallucinations, they make stuff up—even quotes and citations. Another issue is reliability or bias. While attorneys swear an oath to set aside their personal prejudices, biases, and beliefs to faithfully uphold the law and represent their clients, generative artificial intelligence is the product of programming devised by humans who did not have to swear such an oath. As such, these systems hold no allegiance to any client, the rule of law, or the laws and Constitution of the United States (or, as addressed above, the truth). Unbound by any sense of duty, honor, or justice, such programs act according to computer code rather than conviction, based on programming rather than principle.
</p></blockquote>
<p>On June 23, judge <a href="/wiki/P._Kevin_Castel" title="P. Kevin Castel">P. Kevin Castel</a> dismissed the <i>Mata</i> case and issued a $5,000 fine to Schwartz and another lawyer—who had both continued to stand by the fictitious precedents despite Schwartz's previous claims—for <a href="/wiki/Bad_faith" title="Bad faith">bad faith</a> conduct. Castel characterized numerous errors and inconsistencies in the opinion summaries, describing one of the cited opinions as "gibberish" and "[bordering] on nonsensical".<sup id="cite&#95;ref-59" class="reference"><a href="#cite_note-59"><span class="cite-bracket">&#91;</span>59<span class="cite-bracket">&#93;</span></a></sup>
</p><p>In June 2023, Mark Walters, a <a href="/wiki/Gun_rights" class="mw-redirect" title="Gun rights">gun rights</a> activist and radio personality, sued OpenAI in a <a href="/wiki/Georgia_(U.S._state)" title="Georgia (U.S. state)">Georgia</a> state court after ChatGPT mischaracterized a legal <a href="/wiki/Complaint" title="Complaint">complaint</a> in a manner alleged to be <a href="/wiki/Defamatory" class="mw-redirect" title="Defamatory">defamatory</a> against Walters. The complaint in question was brought in May 2023 by the <a href="/wiki/Second_Amendment_Foundation" title="Second Amendment Foundation">Second Amendment Foundation</a> against Washington attorney general <a href="/wiki/Robert_W._Ferguson" class="mw-redirect" title="Robert W. Ferguson">Robert W. Ferguson</a> for allegedly violating their freedom of speech, whereas the ChatGPT-generated summary bore no resemblance and claimed that Walters was accused of <a href="/wiki/Embezzlement" title="Embezzlement">embezzlement</a> and <a href="/wiki/Fraud" title="Fraud">fraud</a> while holding a Second Amendment Foundation office post that he never held in real life. According to AI legal expert <a href="/wiki/Eugene_Volokh" title="Eugene Volokh">Eugene Volokh</a>, OpenAI is likely not shielded against this claim by <a href="/wiki/Section_230" title="Section 230">Section 230</a>, because OpenAI likely "materially contributed" to the creation of the defamatory content.<sup id="cite&#95;ref-60" class="reference"><a href="#cite_note-60"><span class="cite-bracket">&#91;</span>60<span class="cite-bracket">&#93;</span></a></sup> In May 2025, Judge Tracie Cason of Gwinnett County Superior Court ruled in favor of OpenAI. Stating that the plaintiff had not shown he was defamed, as Walters failed to show that OpenAI's statements about him were negligent or made with "actual malice".<sup id="cite&#95;ref-61" class="reference"><a href="#cite_note-61"><span class="cite-bracket">&#91;</span>61<span class="cite-bracket">&#93;</span></a></sup>
</p><p>In February 2024, Canadian airline <a href="/wiki/Air_Canada" title="Air Canada">Air Canada</a> was ordered by the <a href="/wiki/Civil_Resolution_Tribunal" title="Civil Resolution Tribunal">Civil Resolution Tribunal</a> to pay damages to a customer and honor a <a href="/wiki/Bereavement_flight" title="Bereavement flight">bereavement fare</a> policy that was hallucinated by a support chatbot, which incorrectly stated that customers could retroactively request a bereavement discount within 90 days of the date the ticket was issued (the actual policy does not allow the fare to be requested after the flight is booked). The Tribunal rejected Air Canada's defense that the chatbot was a "separate legal entity that is responsible for its own actions".<sup id="cite&#95;ref-62" class="reference"><a href="#cite_note-62"><span class="cite-bracket">&#91;</span>62<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-63" class="reference"><a href="#cite_note-63"><span class="cite-bracket">&#91;</span>63<span class="cite-bracket">&#93;</span></a></sup>
</p><p>In October 2025, several hallucinations, including non-existent academic sources and a fake quote from a federal court judgement were discovered in an <a href="/wiki/A$" class="mw-redirect" title="A$">A$</a>440,000 report written by <a href="/wiki/Deloitte" title="Deloitte">Deloitte</a> and submitted to the <a href="/wiki/Australian_government" class="mw-redirect" title="Australian government">Australian government</a> in July. The company later submitted a revised report with these errors removed, and will issue a partial refund to the government.<sup id="cite&#95;ref-64" class="reference"><a href="#cite_note-64"><span class="cite-bracket">&#91;</span>64<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-65" class="reference"><a href="#cite_note-65"><span class="cite-bracket">&#91;</span>65<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="In_other_modalities">In other modalities</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=11" title="Edit section: In other modalities"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1273380762/mw-parser-output/.tmulti" /><div class="thumb tmulti tright"><div class="thumbinner multiimageinner" style="width:267px;max-width:267px"><div class="trow"><div class="tsingle" style="width:265px;max-width:265px"><div class="thumbimage" style="height:300px;overflow:hidden"><span typeof="mw:File"><a href="/wiki/File:Simplified_neural_network_training_example.svg" class="mw-file-description"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Simplified_neural_network_training_example.svg/330px-Simplified_neural_network_training_example.svg.png" decoding="async" width="263" height="301" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Simplified_neural_network_training_example.svg/500px-Simplified_neural_network_training_example.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Simplified_neural_network_training_example.svg/960px-Simplified_neural_network_training_example.svg.png 2x" data-file-width="773" data-file-height="884" /></a></span></div><div class="thumbcaption">The images above demonstrate an example of how an <a href="/wiki/Artificial_neural_network" class="mw-redirect" title="Artificial neural network">artificial neural network</a> might make a <a href="/wiki/False_positive" class="mw-redirect" title="False positive">false positive</a> result in <a href="/wiki/Object_detection" title="Object detection">object detection</a>. The input image is a simplified example of the training phase, using multiple images that are known to depict <a href="/wiki/Starfish" title="Starfish">starfish</a> and <a href="/wiki/Sea_urchin" title="Sea urchin">sea urchins</a>, respectively. The starfish match with a ringed texture and a star outline, whereas most sea urchins match with a striped texture and oval shape. However, the instance of a ring textured sea urchin creates a weakly weighted association between them.</div></div></div><div class="trow"><div class="tsingle" style="width:265px;max-width:265px"><div class="thumbimage" style="height:225px;overflow:hidden"><span typeof="mw:File"><a href="/wiki/File:Simplified_neural_network_example.svg" class="mw-file-description"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Simplified_neural_network_example.svg/330px-Simplified_neural_network_example.svg.png" decoding="async" width="263" height="226" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Simplified_neural_network_example.svg/500px-Simplified_neural_network_example.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Simplified_neural_network_example.svg/960px-Simplified_neural_network_example.svg.png 2x" data-file-width="1028" data-file-height="882" /></a></span></div><div class="thumbcaption">Subsequent run of the network on an input image (left):<sup id="cite&#95;ref-66" class="reference"><a href="#cite_note-66"><span class="cite-bracket">&#91;</span>66<span class="cite-bracket">&#93;</span></a></sup> The network correctly detects the starfish. However, the weakly weighted association between ringed texture and sea urchin also confers a weak signal to the latter from one of two intermediate nodes. In addition, a shell that was not included in the training gives a weak signal for the oval shape, also resulting in a weak signal for the sea urchin output. These weak signals may result in a false positive result for the presence of a sea urchin although there was none in the input image.
   In reality, textures and outlines would not be represented by single nodes, but rather by associated weight patterns of multiple nodes.</div></div></div></div></div>
<p>The concept of "hallucination" is not limited to text generation, and can occur with other <a href="/wiki/Modality_(human%E2%80%93computer_interaction)" title="Modality (human–computer interaction)">modalities</a>. A confident response from any AI that seems erroneous by the training data can be labeled a hallucination.<sup id="cite&#95;ref-Ji&#95;Lee&#95;Frieske&#95;Survey&#95;of&#95;Hallucination&#95;6-6" class="reference"><a href="#cite_note-Ji_Lee_Frieske_Survey_of_Hallucination-6"><span class="cite-bracket">&#91;</span>6<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Object_detection">Object detection</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=12" title="Edit section: Object detection"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Various researchers cited by <a href="/wiki/Wired_(magazine)" title="Wired (magazine)"><i>Wired</i></a> have classified adversarial hallucinations as a high-dimensional statistical phenomenon, or have attributed hallucinations to insufficient training data. Some researchers believe that some "incorrect" AI responses classified by humans as "hallucinations" in the case of <a href="/wiki/Object_detection" title="Object detection">object detection</a> may in fact be justified by the training data, or even that an AI may be giving the "correct" answer that the human reviewers are failing to see. For example, an adversarial image that looks, to a human, like an ordinary image of a dog, may in fact be seen by the AI to contain tiny patterns that (in authentic images) would only appear when viewing a cat. The AI is detecting real-world visual patterns that humans are insensitive to.<sup id="cite&#95;ref-67" class="reference"><a href="#cite_note-67"><span class="cite-bracket">&#91;</span>67<span class="cite-bracket">&#93;</span></a></sup>
</p><p><i><a href="/wiki/Wired_(magazine)" title="Wired (magazine)">Wired</a></i> noted in 2018 that, despite no recorded attacks "in the wild" (that is, outside of <a href="/wiki/Proof-of-concept" class="mw-redirect" title="Proof-of-concept">proof-of-concept</a> attacks by researchers), there was "little dispute" that consumer gadgets, and systems such as <a href="/wiki/Automated_driving" class="mw-redirect" title="Automated driving">automated driving</a>, were susceptible to <a href="/wiki/Adversarial_machine_learning" title="Adversarial machine learning">adversarial attacks</a> that could cause AI to hallucinate. Examples included a stop sign rendered invisible to computer vision; an audio clip engineered to sound innocuous to humans, but that software transcribed as "evil dot com"; and an image of two men on skis, that <a href="/wiki/Google_Cloud_Platform#Cloud_AI" title="Google Cloud Platform">Google Cloud Vision</a> identified as 91% likely to be "a dog".<sup id="cite&#95;ref-Simonite2018&#95;20-1" class="reference"><a href="#cite_note-Simonite2018-20"><span class="cite-bracket">&#91;</span>20<span class="cite-bracket">&#93;</span></a></sup> However, these findings have been challenged by other researchers.<sup id="cite&#95;ref-bugs&#95;68-0" class="reference"><a href="#cite_note-bugs-68"><span class="cite-bracket">&#91;</span>68<span class="cite-bracket">&#93;</span></a></sup> For example, it was objected that the models can be biased towards superficial statistics, leading <a href="/wiki/Adversarial_machine_learning" title="Adversarial machine learning">adversarial training</a> to not be robust in real-world scenarios.<sup id="cite&#95;ref-bugs&#95;68-1" class="reference"><a href="#cite_note-bugs-68"><span class="cite-bracket">&#91;</span>68<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Text-to-audio_generative_AI">Text-to-audio generative AI</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=13" title="Edit section: Text-to-audio generative AI"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Text-to-audio generative AI – more narrowly known as <a href="/wiki/Speech_synthesis" title="Speech synthesis">text-to-speech</a> (TTS) synthesis, depending on the modality – are known to produce inaccurate and unexpected results.<sup id="cite&#95;ref-69" class="reference"><a href="#cite_note-69"><span class="cite-bracket">&#91;</span>69<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Text-to-image_generative_AI">Text-to-image generative AI</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=14" title="Edit section: Text-to-image generative AI"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Text-to-image models, such as <a href="/wiki/Stable_Diffusion" title="Stable Diffusion">Stable Diffusion</a>, <a href="/wiki/Midjourney" title="Midjourney">Midjourney</a> and others, often produce inaccurate or unexpected results. For instance, <a href="/wiki/Gemini_(chatbot)" class="mw-redirect" title="Gemini (chatbot)">Gemini</a> depicted <a href="/wiki/Nazi_Germany" title="Nazi Germany">Nazi German</a> soldiers as <a href="/wiki/People_of_color" class="mw-redirect" title="People of color">people of color</a>,<sup id="cite&#95;ref-70" class="reference"><a href="#cite_note-70"><span class="cite-bracket">&#91;</span>70<span class="cite-bracket">&#93;</span></a></sup> causing controversy and leading Google to pause image generation involving people in Gemini.<sup id="cite&#95;ref-71" class="reference"><a href="#cite_note-71"><span class="cite-bracket">&#91;</span>71<span class="cite-bracket">&#93;</span></a></sup> Generative AI is also used in photo sleuthing, occasionally causing problems. Luther (2025) describes instances in which generative AI tools used in photo-sleuthing incorrectly identify individuals or fabricate historical matches when analyzing archival military images. These image-based hallucinations can lead to the spread of <a href="/wiki/Misinformation" title="Misinformation">misinformation</a> about historical figures, military records, and genealogical research. <sup id="cite&#95;ref-72" class="reference"><a href="#cite_note-72"><span class="cite-bracket">&#91;</span>72<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="In_scientific_research">In scientific research</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=15" title="Edit section: In scientific research"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<div class="mw-heading mw-heading3"><h3 id="Problems">Problems</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=16" title="Edit section: Problems"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>AI models can cause problems in the world of academic and scientific research due to their hallucinations. Specifically, models like ChatGPT have been recorded in multiple cases to cite sources for information that are either not correct or do not exist. A 2023 study conducted in the <i><a href="/wiki/Cureus" title="Cureus">Cureus Journal of Medical Science</a></i> showed that out of 178 total references cited by GPT-3, 69 returned an incorrect or nonexistent <a href="/wiki/Digital_object_identifier" title="Digital object identifier">digital object identifier</a> (DOI). An additional 28 had no known DOI nor could be located in a <a href="/wiki/Google_Search" title="Google Search">Google search</a>.<sup id="cite&#95;ref-Athaluri2023&#95;73-0" class="reference"><a href="#cite_note-Athaluri2023-73"><span class="cite-bracket">&#91;</span>73<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Some nonexistent phrases such as "vegetative electron microscopy" have appeared in many research papers as a result of having become embedded in AI training data.<sup id="cite&#95;ref-veg&#95;74-0" class="reference"><a href="#cite_note-veg-74"><span class="cite-bracket">&#91;</span>74<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Another instance was documented by Jerome Goddard from <a href="/wiki/Mississippi_State_University" title="Mississippi State University">Mississippi State University</a>. In an experiment, ChatGPT had provided questionable information about <a href="/wiki/Tick" title="Tick">ticks</a>. Unsure about the validity of the response, they inquired about the source that the information had been gathered from. Upon looking at the source, it was apparent that the DOI and the names of the authors had been hallucinated. Some of the authors were contacted and confirmed that they had no knowledge of the paper's existence whatsoever.<sup id="cite&#95;ref-Goddard2023&#95;75-0" class="reference"><a href="#cite_note-Goddard2023-75"><span class="cite-bracket">&#91;</span>75<span class="cite-bracket">&#93;</span></a></sup> Goddard says that, "in [ChatGPT's] current state of development, physicians and biomedical researchers should NOT ask ChatGPT for sources, references, or citations on a particular topic. Or, if they do, all such references should be carefully vetted for accuracy."<sup id="cite&#95;ref-Goddard2023&#95;75-1" class="reference"><a href="#cite_note-Goddard2023-75"><span class="cite-bracket">&#91;</span>75<span class="cite-bracket">&#93;</span></a></sup> The use of these language models is not ready for fields of academic research and that their use should be handled carefully.<sup id="cite&#95;ref-76" class="reference"><a href="#cite_note-76"><span class="cite-bracket">&#91;</span>76<span class="cite-bracket">&#93;</span></a></sup>
</p><p>In Addition, in <i>FAKING IT: Navigating the new era of generative AI may be the most critical challenge to democracy yet</i>, Nina Schick argues that the persuasive quality of hallucinated content generated by AI systems poses risks to democratic institutions by enabling the spread of convincing but false narratives. She says, "The core risk to democracy is a future in which AI is used as an engine to power all information and knowledge-consequently degrading trust in the medium of digital information itself." The article emphasizes that the rapid proliferation of generative AI may challenge public trust by blurring the boundaries between verified information and synthetic misinformation. <sup id="cite&#95;ref-77" class="reference"><a href="#cite_note-77"><span class="cite-bracket">&#91;</span>77<span class="cite-bracket">&#93;</span></a></sup>
</p><p>On top of providing incorrect or missing reference material, ChatGPT also has issues with hallucinating the contents of some reference material. A study that analyzed a total of 115 references provided by ChatGPT-3.5 documented that 47% of them were fabricated. Another 46% cited real references but extracted incorrect information from them. Only the remaining 7% of references were cited correctly and provided accurate information. ChatGPT has also been observed to "double-down" on a lot of the incorrect information. When asked about a mistake that may have been hallucinated, sometimes ChatGPT will try to correct itself but other times it will claim the response is correct and provide even more <a href="/wiki/Misinformation" title="Misinformation">misleading information</a>.<sup id="cite&#95;ref-78" class="reference"><a href="#cite_note-78"><span class="cite-bracket">&#91;</span>78<span class="cite-bracket">&#93;</span></a></sup>
</p><p>These hallucinated articles generated by <a href="/wiki/Language_model" title="Language model">language models</a> also pose an issue because it is difficult to tell whether an article was generated by an AI. To show this, a group of researchers at the <a href="/wiki/Northwestern_University" title="Northwestern University">Northwestern University of Chicago</a> generated 50 <a href="/wiki/Abstract_(summary)" title="Abstract (summary)">abstracts</a> based on existing reports and analyzed their originality. <a href="/wiki/Plagiarism_detector" class="mw-redirect" title="Plagiarism detector">Plagiarism detectors</a> gave the generated articles an originality score of 100%, meaning that the information presented appears to be completely original. Other software designed to detect AI generated text was only able to correctly identify these generated articles with an accuracy of 66%. Research scientists had a similar rate of human error, identifying these abstracts at a rate of 68%.<sup id="cite&#95;ref-79" class="reference"><a href="#cite_note-79"><span class="cite-bracket">&#91;</span>79<span class="cite-bracket">&#93;</span></a></sup> From this information, the authors of this study concluded, "[t]he ethical and acceptable boundaries of ChatGPT's use in scientific writing remain unclear, although some publishers are beginning to lay down policies."<sup id="cite&#95;ref-80" class="reference"><a href="#cite_note-80"><span class="cite-bracket">&#91;</span>80<span class="cite-bracket">&#93;</span></a></sup> Because of AI's ability to fabricate research undetected, the use of AI in the field of research will make determining the originality of research more difficult and require new policies regulating its use in the future.
</p><p>Given the ability of AI generated language to pass as real scientific research in some cases, AI hallucinations present problems for the application of language models in the academic and scientific fields of research due to their ability to be undetectable when presented to real researchers. The high likelihood of returning non-existent reference material and incorrect information may require limitations to be put in place regarding these language models. Some say that rather than hallucinations, these events are more akin to "fabrications" and "falsifications" and that the use of these language models presents a risk to the integrity of the field as a whole.<sup id="cite&#95;ref-81" class="reference"><a href="#cite_note-81"><span class="cite-bracket">&#91;</span>81<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Some academic professionals who support scholarly research, such as academic librarians, have observed a significant increase in workload related to verifying the accuracy of references.<sup id="cite&#95;ref-82" class="reference"><a href="#cite_note-82"><span class="cite-bracket">&#91;</span>82<span class="cite-bracket">&#93;</span></a></sup> Zoë Teel noted in a 2023 paper that universities may need to resort to implementing their own citation auditing in order to track the problem of fictitious references.<sup id="cite&#95;ref-83" class="reference"><a href="#cite_note-83"><span class="cite-bracket">&#91;</span>83<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Benefits">Benefits</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=17" title="Edit section: Benefits"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Scientists have also found that hallucinations can serve as a valuable tool for scientific discovery, particularly in fields requiring innovative approaches to complex problems. At the <a href="/wiki/University_of_Washington" title="University of Washington">University of Washington</a>, <a href="/wiki/David_Baker_(biochemist)" title="David Baker (biochemist)">David Baker</a>'s lab has used AI hallucinations to design "ten million brand-new" proteins that don't occur in nature, leading to roughly 100 patents and the founding of over 20 biotech companies. This work contributed to Baker receiving the 2024 <a href="/wiki/Nobel_Prize_in_Chemistry" title="Nobel Prize in Chemistry">Nobel Prize in Chemistry</a>, although the committee avoided using the "hallucinations" language.<sup id="cite&#95;ref-nyt-science&#95;30-3" class="reference"><a href="#cite_note-nyt-science-30"><span class="cite-bracket">&#91;</span>30<span class="cite-bracket">&#93;</span></a></sup>
</p><p>In medical research and device development, hallucinations have enabled practical innovations. At <a href="/wiki/California_Institute_of_Technology" title="California Institute of Technology">California Institute of Technology</a>, researchers used hallucinations to design a novel catheter geometry that significantly reduces bacterial contamination. The design features sawtooth-like spikes on the inner walls that prevent bacteria from gaining traction, potentially addressing a global health issue that causes millions of urinary tract infections annually. These scientific applications of hallucinations differ fundamentally from chatbot hallucinations, as they are grounded in physical reality and scientific facts rather than ambiguous language or internet data. <a href="/wiki/Anima_Anandkumar" title="Anima Anandkumar">Anima Anandkumar</a>, a professor at Caltech, emphasizes that these AI models are "taught physics" and their outputs must be validated through rigorous testing. In meteorology, scientists use AI to generate thousands of subtle forecast variations, helping identify unexpected factors that can influence extreme weather events.<sup id="cite&#95;ref-nyt-science&#95;30-4" class="reference"><a href="#cite_note-nyt-science-30"><span class="cite-bracket">&#91;</span>30<span class="cite-bracket">&#93;</span></a></sup>
</p><p>At <a href="/wiki/Memorial_Sloan_Kettering_Cancer_Center" title="Memorial Sloan Kettering Cancer Center">Memorial Sloan Kettering Cancer Center</a>, researchers have applied hallucinatory techniques to enhance blurry medical images, while the <a href="/wiki/University_of_Texas_at_Austin" title="University of Texas at Austin">University of Texas at Austin</a> has utilized them to improve robot navigation systems. These applications demonstrate how hallucinations, when properly constrained by scientific methodology, can accelerate the discovery process from years to days or even minutes.<sup id="cite&#95;ref-nyt-science&#95;30-5" class="reference"><a href="#cite_note-nyt-science-30"><span class="cite-bracket">&#91;</span>30<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="Consequences_of_hallucinations_in_education">Consequences of hallucinations in education</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=18" title="Edit section: Consequences of hallucinations in education"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Artificial intelligence hallucinations can impact the education industry. There has been a rise in students using AI tools for assistance in research or writing tools such as <a href="/wiki/Grammarly" title="Grammarly">Grammarly</a> or using generative AI programs such as ChatGPT. This has raised concern regarding <a href="/wiki/Academic_integrity" title="Academic integrity">academic integrity</a><sup id="cite&#95;ref-84" class="reference"><a href="#cite_note-84"><span class="cite-bracket">&#91;</span>84<span class="cite-bracket">&#93;</span></a></sup> in submitted projects in addition to hallucinations causing students to learn incorrect information.
</p><p>Part of this concern lies in the citations provided by LLMs and generative AI. A 2024 study at the <a href="/wiki/University_of_Mississippi" title="University of Mississippi">University of Mississippi</a> found that many of these citations that students submitted were partially or completely fabricated. 47% of these sources either had incorrect titles, dates, authors, or a combination of all.<sup id="cite&#95;ref-85" class="reference"><a href="#cite_note-85"><span class="cite-bracket">&#91;</span>85<span class="cite-bracket">&#93;</span></a></sup> The study notes that these inconsistencies in student-submitted citations cause educators and librarians to manually check the accuracy more frequently.
</p><p>The Journal of <a href="/wiki/Cranio-maxillofacial_surgery" class="mw-redirect" title="Cranio-maxillofacial surgery">Cranio-Maxillofacial Surgery</a> addresses this risk when it comes to the medical and surgical fields. They mention how academic publishers have acknowledged the issue, and that some journals such as <a href="/wiki/JAMA" title="JAMA">JAMA</a> have changed some of their policies to discourage the use of AI-generated citations.<sup id="cite&#95;ref-86" class="reference"><a href="#cite_note-86"><span class="cite-bracket">&#91;</span>86<span class="cite-bracket">&#93;</span></a></sup> Although the journal states that a policy will not be enough to diminish the use of AI-generated citations, automatic tools to check the citations and <a href="/wiki/AI_literacy" title="AI literacy">AI literacy</a> training should also be adopted.
</p><p>Instructors may use tools such as <a href="/wiki/Turnitin" title="Turnitin">Turnitin</a> for plagiarism checking and verification of academic integrity. These tools have been found to sometimes flag papers that have not used any AI assistance in their writing.<sup id="cite&#95;ref-87" class="reference"><a href="#cite_note-87"><span class="cite-bracket">&#91;</span>87<span class="cite-bracket">&#93;</span></a></sup> OpenAI has also found a lack of accuracy in their own AI detection software that the company shut it down entirely.<sup id="cite&#95;ref-88" class="reference"><a href="#cite_note-88"><span class="cite-bracket">&#91;</span>88<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="Mitigation_methods">Mitigation methods</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=19" title="Edit section: Mitigation methods"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>The hallucination phenomenon is still not completely understood. Researchers have proposed that hallucinations are inevitable and are an innate limitation of large language models.<sup id="cite&#95;ref-89" class="reference"><a href="#cite_note-89"><span class="cite-bracket">&#91;</span>89<span class="cite-bracket">&#93;</span></a></sup> Therefore, there is still ongoing research to try to mitigate its occurrence.<sup id="cite&#95;ref-90" class="reference"><a href="#cite_note-90"><span class="cite-bracket">&#91;</span>90<span class="cite-bracket">&#93;</span></a></sup> Particularly, it was shown that language models not only hallucinate but also amplify hallucinations, even for those which were designed to alleviate this issue.<sup id="cite&#95;ref-91" class="reference"><a href="#cite_note-91"><span class="cite-bracket">&#91;</span>91<span class="cite-bracket">&#93;</span></a></sup> Researchers from OpenAI wrote that hallucinations occur because the training and evaluation of LLMs reward guessing over acknowledging uncertainty, and proposed modifying the scoring of benchmarks.<sup id="cite&#95;ref-:1&#95;36-1" class="reference"><a href="#cite_note-:1-36"><span class="cite-bracket">&#91;</span>36<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Ji et al. divide common mitigation methods into two categories: <i>data-related methods</i> and <i>modeling and inference methods</i>.<sup id="cite&#95;ref-Ji&#95;Lee&#95;Frieske&#95;Survey&#95;of&#95;Hallucination&#95;6-7" class="reference"><a href="#cite_note-Ji_Lee_Frieske_Survey_of_Hallucination-6"><span class="cite-bracket">&#91;</span>6<span class="cite-bracket">&#93;</span></a></sup> Data-related methods include building a faithful dataset, cleaning data automatically, and information augmentation by augmenting the inputs with external information. Model and inference methods include changes in the architecture (either modifying the encoder, attention, or the decoder in various ways); changes in the training process, such as using <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">reinforcement learning</a>; and post-processing methods that can correct hallucinations in the output.
</p><p>Researchers have proposed a variety of mitigation measures, including getting different chatbots to debate one another until they reach consensus on an answer.<sup id="cite&#95;ref-92" class="reference"><a href="#cite_note-92"><span class="cite-bracket">&#91;</span>92<span class="cite-bracket">&#93;</span></a></sup> Another approach proposes to actively validate the correctness corresponding to the low-confidence generation of the model using web search results. They have shown that a generated sentence is hallucinated more often when the model has already hallucinated in its previously generated sentences for the input, and they are instructing the model to create a validation question checking the correctness of the information about the selected concept using <a href="/wiki/Microsoft_Bing" title="Microsoft Bing">Bing</a> search API.<sup id="cite&#95;ref-93" class="reference"><a href="#cite_note-93"><span class="cite-bracket">&#91;</span>93<span class="cite-bracket">&#93;</span></a></sup> An extra layer of <a href="/wiki/Logic" title="Logic">logic</a>-based rules was proposed for the web search mitigation method, by using different ranks of web pages as a knowledge base, which differ in hierarchy.<sup id="cite&#95;ref-94" class="reference"><a href="#cite_note-94"><span class="cite-bracket">&#91;</span>94<span class="cite-bracket">&#93;</span></a></sup> When there are no external data sources available to validate LLM-generated responses (or the responses are already based on external data as in RAG), model uncertainty estimation techniques from machine learning may be applied to detect hallucinations.<sup id="cite&#95;ref-Chen-2024&#95;95-0" class="reference"><a href="#cite_note-Chen-2024-95"><span class="cite-bracket">&#91;</span>95<span class="cite-bracket">&#93;</span></a></sup> Another proposal includes a two-phase framework that detects hallucinations in LLM-generated content via unsupervised screening and LLM validation.<sup id="cite&#95;ref-96" class="reference"><a href="#cite_note-96"><span class="cite-bracket">&#91;</span>96<span class="cite-bracket">&#93;</span></a></sup>
</p><p>According to Luo et al.,<sup id="cite&#95;ref-Luo-2024&#95;97-0" class="reference"><a href="#cite_note-Luo-2024-97"><span class="cite-bracket">&#91;</span>97<span class="cite-bracket">&#93;</span></a></sup> the previous methods fall into knowledge- and retrieval-based approaches, which ground LLM responses in factual data using external knowledge sources, such as path grounding.<sup id="cite&#95;ref-98" class="reference"><a href="#cite_note-98"><span class="cite-bracket">&#91;</span>98<span class="cite-bracket">&#93;</span></a></sup> Luo et al. also mention training or reference guiding for language models, involving strategies like employing control codes<sup id="cite&#95;ref-99" class="reference"><a href="#cite_note-99"><span class="cite-bracket">&#91;</span>99<span class="cite-bracket">&#93;</span></a></sup> or contrastive learning<sup id="cite&#95;ref-100" class="reference"><a href="#cite_note-100"><span class="cite-bracket">&#91;</span>100<span class="cite-bracket">&#93;</span></a></sup> to guide the generation process to differentiate between correct and hallucinated content. Another category is evaluation and mitigation focused on specific hallucination types,<sup id="cite&#95;ref-Luo-2024&#95;97-1" class="reference"><a href="#cite_note-Luo-2024-97"><span class="cite-bracket">&#91;</span>97<span class="cite-bracket">&#93;</span></a></sup> such as employing methods to evaluate quantity entity in summarization<sup id="cite&#95;ref-101" class="reference"><a href="#cite_note-101"><span class="cite-bracket">&#91;</span>101<span class="cite-bracket">&#93;</span></a></sup> and methods to detect and mitigate self-contradictory statements.<sup id="cite&#95;ref-102" class="reference"><a href="#cite_note-102"><span class="cite-bracket">&#91;</span>102<span class="cite-bracket">&#93;</span></a></sup>
</p><p><a href="/wiki/Nvidia" title="Nvidia">Nvidia</a> Guardrails, launched in 2023, can be configured to <a href="/wiki/Hard_coding" title="Hard coding">hard-code</a> certain responses via script instead of leaving them to the LLM.<sup id="cite&#95;ref-103" class="reference"><a href="#cite_note-103"><span class="cite-bracket">&#91;</span>103<span class="cite-bracket">&#93;</span></a></sup> Furthermore, numerous tools like SelfCheckGPT,<sup id="cite&#95;ref-104" class="reference"><a href="#cite_note-104"><span class="cite-bracket">&#91;</span>104<span class="cite-bracket">&#93;</span></a></sup> the Trustworthy Language Model,<sup id="cite&#95;ref-105" class="reference"><a href="#cite_note-105"><span class="cite-bracket">&#91;</span>105<span class="cite-bracket">&#93;</span></a></sup> and Aimon<sup id="cite&#95;ref-106" class="reference"><a href="#cite_note-106"><span class="cite-bracket">&#91;</span>106<span class="cite-bracket">&#93;</span></a></sup> have emerged to aid in the detection of hallucination in offline experimentation and real-time production scenarios.
</p><p>Evaluating multiple possible replies before answering a query by assigning confidence scores to each could mitigate the problem. However, this approach would multiply computational costs. <a href="/wiki/Active_learning_(machine_learning)" title="Active learning (machine learning)">Active learning</a> would further increase these costs. In high-stakes domains such as chip design, supply chain logistics, and medical diagnostics, the added costs are operationally necessary and therefore economically viable. In chatbots, however, customers tend to prefer rapid, overconfident answers over cautious, uncertainty-aware ones.<sup id="cite&#95;ref-:0&#95;107-0" class="reference"><a href="#cite_note-:0-107"><span class="cite-bracket">&#91;</span>107<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="See_also">See also</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=20" title="Edit section: See also"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1184024115">.mw-parser-output .div-col{margin-top:0.3em;column-width:30em}.mw-parser-output .div-col-small{font-size:90%}.mw-parser-output .div-col-rules{column-rule:1px solid #aaa}.mw-parser-output .div-col dl,.mw-parser-output .div-col ol,.mw-parser-output .div-col ul{margin-top:0}.mw-parser-output .div-col li,.mw-parser-output .div-col dd{page-break-inside:avoid;break-inside:avoid-column}</style><div class="div-col">
<ul><li><a href="/wiki/AI_alignment" title="AI alignment">AI alignment</a></li>
<li><a href="/wiki/AI_effect" title="AI effect">AI effect</a></li>
<li><a href="/wiki/AI_safety" title="AI safety">AI safety</a></li>
<li><a href="/wiki/AI_slop" title="AI slop">AI slop</a></li>
<li><a href="/wiki/Artifact_(error)" title="Artifact (error)">Artifact</a></li>
<li><a href="/wiki/Artificial_stupidity" title="Artificial stupidity">Artificial stupidity</a></li>
<li><a href="/wiki/Chatbot_psychosis" title="Chatbot psychosis">Chatbot psychosis</a></li>
<li><a href="/wiki/Memetic_algorithm" title="Memetic algorithm">Memetic algorithm</a></li>
<li><a href="/wiki/Turing_test" title="Turing test">Turing test</a></li>
<li><a href="/wiki/Uncanny_valley" title="Uncanny valley">Uncanny valley</a></li></ul>
</div>
<div class="mw-heading mw-heading2"><h2 id="References">References</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hallucination_(artificial_intelligence)&amp;action=edit&amp;section=21" title="Edit section: References"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1239543626">.mw-parser-output .reflist{margin-bottom:0.5em;list-style-type:decimal}@media screen{.mw-parser-output .reflist{font-size:90%}}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class="reflist">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite&#95;note-Hicks&#95;Humphries&#95;Slater&#95;2024-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-Hicks_Humphries_Slater_2024_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Hicks_Humphries_Slater_2024_1-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><style data-mw-deduplicate="TemplateStyles:r1238218222">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}</style><cite id="CITEREFHicksHumphriesSlater2024" class="citation journal cs1">Hicks, Michael Townsen; Humphries, James; Slater, Joe (June 2024). <a rel="nofollow" class="external text" href="https://eprints.gla.ac.uk/327588/1/327588.pdf">"ChatGPT is bullshit"</a> <span class="cs1-format">(PDF)</span>. <i>Ethics and Information Technology</i>. <b>26</b> (2) 38. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs10676-024-09775-5">10.1007/s10676-024-09775-5</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Ethics+and+Information+Technology&amp;rft.atitle=ChatGPT+is+bullshit&amp;rft.volume=26&amp;rft.issue=2&amp;rft.artnum=38&amp;rft.date=2024-06&amp;rft&#95;id=info%3Adoi%2F10.1007%2Fs10676-024-09775-5&amp;rft.aulast=Hicks&amp;rft.aufirst=Michael+Townsen&amp;rft.au=Humphries%2C+James&amp;rft.au=Slater%2C+Joe&amp;rft&#95;id=https%3A%2F%2Feprints.gla.ac.uk%2F327588%2F1%2F327588.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFLiangHuZhaoSong2025" class="citation arxiv cs1">Liang, Kaiqu; Hu, Haimin; Zhao, Xuandong; Song, Dawn; Griffiths, Thomas L.; Fernández Fisac, Jaime (2025). "Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2507.07484">2507.07484</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Machine+Bullshit%3A+Characterizing+the+Emergent+Disregard+for+Truth+in+Large+Language+Models&amp;rft.date=2025&amp;rft&#95;id=info%3Aarxiv%2F2507.07484&amp;rft.aulast=Liang&amp;rft.aufirst=Kaiqu&amp;rft.au=Hu%2C+Haimin&amp;rft.au=Zhao%2C+Xuandong&amp;rft.au=Song%2C+Dawn&amp;rft.au=Griffiths%2C+Thomas+L.&amp;rft.au=Fern%C3%A1ndez+Fisac%2C+Jaime&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-ars&#95;making&#95;things&#95;up-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-ars_making_things_up_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ars_making_things_up_3-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFEdwards2023" class="citation news cs1">Edwards, Benj (6 April 2023). <a rel="nofollow" class="external text" href="https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/">"Why ChatGPT and Bing Chat are so good at making things up"</a>. <i>Ars Technica</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230611024338/https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/">Archived</a> from the original on 11 June 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">11 June</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Ars+Technica&amp;rft.atitle=Why+ChatGPT+and+Bing+Chat+are+so+good+at+making+things+up&amp;rft.date=2023-04-06&amp;rft.aulast=Edwards&amp;rft.aufirst=Benj&amp;rft&#95;id=https%3A%2F%2Farstechnica.com%2Finformation-technology%2F2023%2F04%2Fwhy-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFOrtegaKuneschDelétangGenewein2021" class="citation report cs1">Ortega, Pedro A.; Kunesch, Markus; Delétang, Grégoire; Genewein, Tim; Grau-Moya, Jordi; Veness, Joel; Buchli, Jonas; Degrave, Jonas; Piot, Bilal; Perolat, Julien; Everitt, Tom; Tallec, Corentin; Parisotto, Emilio; Erez, Tom; Chen, Yutian; Reed, Scott; Hutter, Marcus; Nando de Freitas; Legg, Shane (2021). Shaking the foundations: Delusions in sequence models for interaction and control (Preprint). <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2110.10819">2110.10819</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=Shaking+the+foundations%3A+Delusions+in+sequence+models+for+interaction+and+control&amp;rft.date=2021&amp;rft&#95;id=info%3Aarxiv%2F2110.10819&amp;rft.aulast=Ortega&amp;rft.aufirst=Pedro+A.&amp;rft.au=Kunesch%2C+Markus&amp;rft.au=Del%C3%A9tang%2C+Gr%C3%A9goire&amp;rft.au=Genewein%2C+Tim&amp;rft.au=Grau-Moya%2C+Jordi&amp;rft.au=Veness%2C+Joel&amp;rft.au=Buchli%2C+Jonas&amp;rft.au=Degrave%2C+Jonas&amp;rft.au=Piot%2C+Bilal&amp;rft.au=Perolat%2C+Julien&amp;rft.au=Everitt%2C+Tom&amp;rft.au=Tallec%2C+Corentin&amp;rft.au=Parisotto%2C+Emilio&amp;rft.au=Erez%2C+Tom&amp;rft.au=Chen%2C+Yutian&amp;rft.au=Reed%2C+Scott&amp;rft.au=Hutter%2C+Marcus&amp;rft.au=Nando+de+Freitas&amp;rft.au=Legg%2C+Shane&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFMaynezNarayanBohnetMcDonald2020" class="citation book cs1">Maynez, Joshua; Narayan, Shashi; Bohnet, Bernd; McDonald, Ryan (2020). "On Faithfulness and Factuality in Abstractive Summarization". <i>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</i>. pp.&#160;<span class="nowrap">1906–</span>1919. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.18653%2Fv1%2F2020.acl-main.173">10.18653/v1/2020.acl-main.173</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=On+Faithfulness+and+Factuality+in+Abstractive+Summarization&amp;rft.btitle=Proceedings+of+the+58th+Annual+Meeting+of+the+Association+for+Computational+Linguistics&amp;rft.pages=1906-1919&amp;rft.date=2020&amp;rft&#95;id=info%3Adoi%2F10.18653%2Fv1%2F2020.acl-main.173&amp;rft.aulast=Maynez&amp;rft.aufirst=Joshua&amp;rft.au=Narayan%2C+Shashi&amp;rft.au=Bohnet%2C+Bernd&amp;rft.au=McDonald%2C+Ryan&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Ji&#95;Lee&#95;Frieske&#95;Survey&#95;of&#95;Hallucination-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-Ji_Lee_Frieske_Survey_of_Hallucination_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Ji_Lee_Frieske_Survey_of_Hallucination_6-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Ji_Lee_Frieske_Survey_of_Hallucination_6-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-Ji_Lee_Frieske_Survey_of_Hallucination_6-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-Ji_Lee_Frieske_Survey_of_Hallucination_6-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-Ji_Lee_Frieske_Survey_of_Hallucination_6-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-Ji_Lee_Frieske_Survey_of_Hallucination_6-6"><sup><i><b>g</b></i></sup></a> <a href="#cite_ref-Ji_Lee_Frieske_Survey_of_Hallucination_6-7"><sup><i><b>h</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFJiLeeFrieskeYu2023" class="citation journal cs1">Ji, Ziwei; Lee, Nayeon; Frieske, Rita; Yu, Tiezheng; Su, Dan; Xu, Yan; Ishii, Etsuko; Bang, Ye Jin; Madotto, Andrea; Fung, Pascale (31 December 2023). "Survey of Hallucination in Natural Language Generation". <i>ACM Computing Surveys</i>. <b>55</b> (12): <span class="nowrap">1–</span>38. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2202.03629">2202.03629</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3571730">10.1145/3571730</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ACM+Computing+Surveys&amp;rft.atitle=Survey+of+Hallucination+in+Natural+Language+Generation&amp;rft.volume=55&amp;rft.issue=12&amp;rft.pages=1-38&amp;rft.date=2023-12-31&amp;rft&#95;id=info%3Aarxiv%2F2202.03629&amp;rft&#95;id=info%3Adoi%2F10.1145%2F3571730&amp;rft.aulast=Ji&amp;rft.aufirst=Ziwei&amp;rft.au=Lee%2C+Nayeon&amp;rft.au=Frieske%2C+Rita&amp;rft.au=Yu%2C+Tiezheng&amp;rft.au=Su%2C+Dan&amp;rft.au=Xu%2C+Yan&amp;rft.au=Ishii%2C+Etsuko&amp;rft.au=Bang%2C+Ye+Jin&amp;rft.au=Madotto%2C+Andrea&amp;rft.au=Fung%2C+Pascale&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-nyt-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-nyt_7-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFMetz2023" class="citation news cs1">Metz, Cade (6 November 2023). <a rel="nofollow" class="external text" href="https://www.nytimes.com/2023/11/06/technology/chatbots-hallucination-rates.html">"Chatbots May 'Hallucinate' More Often Than Many Realize"</a>. <i>The New York Times</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20231207081252/https://www.nytimes.com/2023/11/06/technology/chatbots-hallucination-rates.html">Archived</a> from the original on 7 December 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">6 November</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=Chatbots+May+%27Hallucinate%27+More+Often+Than+Many+Realize&amp;rft.date=2023-11-06&amp;rft.aulast=Metz&amp;rft.aufirst=Cade&amp;rft&#95;id=https%3A%2F%2Fwww.nytimes.com%2F2023%2F11%2F06%2Ftechnology%2Fchatbots-hallucination-rates.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-de&#95;Wynter-2023-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-de_Wynter-2023_8-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFde&#95;WynterWangSokolovGu2023" class="citation journal cs1">de Wynter, Adrian; Wang, Xun; Sokolov, Alex; Gu, Qilong; Chen, Si-Qing (September 2023). <a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.nlp.2023.100024">"An evaluation on large language model outputs: Discourse and memorization"</a>. <i>Natural Language Processing Journal</i>. <b>4</b> 100024. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2304.08637">2304.08637</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.nlp.2023.100024">10.1016/j.nlp.2023.100024</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Natural+Language+Processing+Journal&amp;rft.atitle=An+evaluation+on+large+language+model+outputs%3A+Discourse+and+memorization&amp;rft.volume=4&amp;rft.artnum=100024&amp;rft.date=2023-09&amp;rft&#95;id=info%3Aarxiv%2F2304.08637&amp;rft&#95;id=info%3Adoi%2F10.1016%2Fj.nlp.2023.100024&amp;rft.aulast=de+Wynter&amp;rft.aufirst=Adrian&amp;rft.au=Wang%2C+Xun&amp;rft.au=Sokolov%2C+Alex&amp;rft.au=Gu%2C+Qilong&amp;rft.au=Chen%2C+Si-Qing&amp;rft&#95;id=https%3A%2F%2Fdoi.org%2F10.1016%252Fj.nlp.2023.100024&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-cnbc&#95;several&#95;errors-9"><span class="mw-cite-backlink">^ <a href="#cite_ref-cnbc_several_errors_9-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-cnbc_several_errors_9-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-cnbc_several_errors_9-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFLeswing2023" class="citation news cs1">Leswing, Kif (14 February 2023). <a rel="nofollow" class="external text" href="https://www.cnbc.com/2023/02/14/microsoft-bing-ai-made-several-errors-in-launch-demo-last-week-.html">"Microsoft's Bing A.I. made several factual errors in last week's launch demo"</a>. <i>CNBC</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230216072604/https://www.cnbc.com/2023/02/14/microsoft-bing-ai-made-several-errors-in-launch-demo-last-week-.html">Archived</a> from the original on 16 February 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">16 February</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=CNBC&amp;rft.atitle=Microsoft%27s+Bing+A.I.+made+several+factual+errors+in+last+week%27s+launch+demo&amp;rft.date=2023-02-14&amp;rft.aulast=Leswing&amp;rft.aufirst=Kif&amp;rft&#95;id=https%3A%2F%2Fwww.cnbc.com%2F2023%2F02%2F14%2Fmicrosoft-bing-ai-made-several-errors-in-launch-demo-last-week-.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-sigplan-10"><span class="mw-cite-backlink">^ <a href="#cite_ref-sigplan_10-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-sigplan_10-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFKangShaw2024" class="citation book cs1">Kang, Eunsuk; Shaw, Mary (2024). "tl;dr: Chill, y'all: AI Will Not Devour SE". <i>Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software</i>. pp.&#160;<span class="nowrap">303–</span>315. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2409.00764">2409.00764</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3689492.3689816">10.1145/3689492.3689816</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/979-8-4007-1215-9" title="Special:BookSources/979-8-4007-1215-9"><bdi>979-8-4007-1215-9</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=tl%3Bdr%3A+Chill%2C+y%27all%3A+AI+Will+Not+Devour+SE&amp;rft.btitle=Proceedings+of+the+2024+ACM+SIGPLAN+International+Symposium+on+New+Ideas%2C+New+Paradigms%2C+and+Reflections+on+Programming+and+Software&amp;rft.pages=303-315&amp;rft.date=2024&amp;rft&#95;id=info%3Aarxiv%2F2409.00764&amp;rft&#95;id=info%3Adoi%2F10.1145%2F3689492.3689816&amp;rft.isbn=979-8-4007-1215-9&amp;rft.aulast=Kang&amp;rft.aufirst=Eunsuk&amp;rft.au=Shaw%2C+Mary&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-salon-11"><span class="mw-cite-backlink">^ <a href="#cite_ref-salon_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-salon_11-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFDesai2023" class="citation web cs1">Desai, Rajiv (13 October 2023). <a rel="nofollow" class="external text" href="https://drrajivdesaimd.com/2023/10/13/is-artificial-intelligence-ai-an-existential-threat/">"Is artificial intelligence (AI) an existential threat? – Dr Rajiv Desai"</a><span class="reference-accessdate">. Retrieved <span class="nowrap">25 November</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Is+artificial+intelligence+%28AI%29+an+existential+threat%3F+%E2%80%93+Dr+Rajiv+Desai&amp;rft.date=2023-10-13&amp;rft.aulast=Desai&amp;rft.aufirst=Rajiv&amp;rft&#95;id=https%3A%2F%2Fdrrajivdesaimd.com%2F2023%2F10%2F13%2Fis-artificial-intelligence-ai-an-existential-threat%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Maleki&#95;Padmanabhan&#95;AI&#95;Hallucinations-12"><span class="mw-cite-backlink">^ <a href="#cite_ref-Maleki_Padmanabhan_AI_Hallucinations_12-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Maleki_Padmanabhan_AI_Hallucinations_12-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Maleki_Padmanabhan_AI_Hallucinations_12-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFMalekiPadmanabhanDutta2024" class="citation book cs1">Maleki, Negar; Padmanabhan, Balaji; Dutta, Kaushik (2024). "AI Hallucinations: A Misnomer Worth Clarifying". <i>2024 IEEE Conference on Artificial Intelligence (CAI)</i>. pp.&#160;<span class="nowrap">133–</span>138. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2401.06796">2401.06796</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FCAI59869.2024.00033">10.1109/CAI59869.2024.00033</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/979-8-3503-5409-6" title="Special:BookSources/979-8-3503-5409-6"><bdi>979-8-3503-5409-6</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=AI+Hallucinations%3A+A+Misnomer+Worth+Clarifying&amp;rft.btitle=2024+IEEE+Conference+on+Artificial+Intelligence+%28CAI%29&amp;rft.pages=133-138&amp;rft.date=2024&amp;rft&#95;id=info%3Aarxiv%2F2401.06796&amp;rft&#95;id=info%3Adoi%2F10.1109%2FCAI59869.2024.00033&amp;rft.isbn=979-8-3503-5409-6&amp;rft.aulast=Maleki&amp;rft.aufirst=Negar&amp;rft.au=Padmanabhan%2C+Balaji&amp;rft.au=Dutta%2C+Kaushik&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFLiuShumFreeman2007" class="citation journal cs1">Liu, Ce; Shum, Heung-Yeung; Freeman, William T. (18 July 2007). "Face Hallucination: Theory and Practice". <i>International Journal of Computer Vision</i>. <b>75</b> (1): <span class="nowrap">115–</span>134. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs11263-006-0029-5">10.1007/s11263-006-0029-5</a>. <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><a href="/wiki/ProQuest" title="ProQuest">ProQuest</a>&#160;<a rel="nofollow" class="external text" href="https://www.proquest.com/docview/1113669475">1113669475</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Computer+Vision&amp;rft.atitle=Face+Hallucination%3A+Theory+and+Practice&amp;rft.volume=75&amp;rft.issue=1&amp;rft.pages=115-134&amp;rft.date=2007-07-18&amp;rft&#95;id=info%3Adoi%2F10.1007%2Fs11263-006-0029-5&amp;rft.aulast=Liu&amp;rft.aufirst=Ce&amp;rft.au=Shum%2C+Heung-Yeung&amp;rft.au=Freeman%2C+William+T.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFNasrollahiMoeslund2014" class="citation journal cs1">Nasrollahi, Kamal; Moeslund, Thomas B. (2014). <a rel="nofollow" class="external text" href="http://link.springer.com/10.1007/s00138-014-0623-4">"Super-resolution: a comprehensive survey"</a>. <i>Machine Vision and Applications</i>. <b>25</b> (6): <span class="nowrap">1423–</span>1468. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs00138-014-0623-4">10.1007/s00138-014-0623-4</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/0932-8092">0932-8092</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Machine+Vision+and+Applications&amp;rft.atitle=Super-resolution%3A+a+comprehensive+survey&amp;rft.volume=25&amp;rft.issue=6&amp;rft.pages=1423-1468&amp;rft.date=2014&amp;rft&#95;id=info%3Adoi%2F10.1007%2Fs00138-014-0623-4&amp;rft.issn=0932-8092&amp;rft.aulast=Nasrollahi&amp;rft.aufirst=Kamal&amp;rft.au=Moeslund%2C+Thomas+B.&amp;rft&#95;id=http%3A%2F%2Flink.springer.com%2F10.1007%2Fs00138-014-0623-4&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.ri.cmu.edu/publications/hallucinating-faces-2/">"Hallucinating Faces"</a>. <i>Robotics Institute Carnegie Mellon University</i>. 1999.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Robotics+Institute+Carnegie+Mellon+University&amp;rft.atitle=Hallucinating+Faces&amp;rft.date=1999&amp;rft&#95;id=https%3A%2F%2Fwww.ri.cmu.edu%2Fpublications%2Fhallucinating-faces-2%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text">
<ul><li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFThaler1995" class="citation journal cs1">Thaler, S.L. (January 1995). "<span class="cs1-kern-left"></span>'Virtual input' phenomena within the death of a simple pattern associator". <i>Neural Networks</i>. <b>8</b> (1): <span class="nowrap">55–</span>65. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2F0893-6080%2894%2900065-T">10.1016/0893-6080(94)00065-T</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Networks&amp;rft.atitle=%27Virtual+input%27+phenomena+within+the+death+of+a+simple+pattern+associator&amp;rft.volume=8&amp;rft.issue=1&amp;rft.pages=55-65&amp;rft.date=1995-01&amp;rft&#95;id=info%3Adoi%2F10.1016%2F0893-6080%2894%2900065-T&amp;rft.aulast=Thaler&amp;rft.aufirst=S.L.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFRicciardielloFornaro2013" class="citation journal cs1">Ricciardiello, Luciana; Fornaro, Pantaleo (May 2013). "Beyond the cliff of creativity". <i>Medical Hypotheses</i>. <b>80</b> (5): <span class="nowrap">534–</span>543. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.mehy.2012.12.018">10.1016/j.mehy.2012.12.018</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/23452643">23452643</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Medical+Hypotheses&amp;rft.atitle=Beyond+the+cliff+of+creativity&amp;rft.volume=80&amp;rft.issue=5&amp;rft.pages=534-543&amp;rft.date=2013-05&amp;rft&#95;id=info%3Adoi%2F10.1016%2Fj.mehy.2012.12.018&amp;rft&#95;id=info%3Apmid%2F23452643&amp;rft.aulast=Ricciardiello&amp;rft.aufirst=Luciana&amp;rft.au=Fornaro%2C+Pantaleo&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFThaler2016" class="citation journal cs1">Thaler, S. L. (2016). "Cycles of insanity and creativity within contemplative neural systems". <i>Medical Hypotheses</i>. <b>96</b>: <span class="nowrap">34–</span>43. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.mehy.2016.07.010">10.1016/j.mehy.2016.07.010</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/27515220">27515220</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Medical+Hypotheses&amp;rft.atitle=Cycles+of+insanity+and+creativity+within+contemplative+neural+systems&amp;rft.volume=96&amp;rft.pages=34-43&amp;rft.date=2016&amp;rft&#95;id=info%3Adoi%2F10.1016%2Fj.mehy.2016.07.010&amp;rft&#95;id=info%3Apmid%2F27515220&amp;rft.aulast=Thaler&amp;rft.aufirst=S.+L.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFThaler2014" class="citation journal cs1">Thaler, Stephen L. (2014). "Synaptic Perturbation and Consciousness". <i>International Journal of Machine Consciousness</i>. <b>6</b> (2). World Scientific Publishing Company: <span class="nowrap">75–</span>107. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1142%2FS1793843014400137">10.1142/S1793843014400137</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Machine+Consciousness&amp;rft.atitle=Synaptic+Perturbation+and+Consciousness&amp;rft.volume=6&amp;rft.issue=2&amp;rft.pages=75-107&amp;rft.date=2014&amp;rft&#95;id=info%3Adoi%2F10.1142%2FS1793843014400137&amp;rft.aulast=Thaler&amp;rft.aufirst=Stephen+L.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFThaler1996" class="citation journal cs1">Thaler, S. L. (Fall 1996). "The Death Dream and Near-Death Darwinism". <i>Journal of Near-Death Studies</i>. <b>15</b> (1).</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Near-Death+Studies&amp;rft.atitle=The+Death+Dream+and+Near-Death+Darwinism&amp;rft.ssn=fall&amp;rft.volume=15&amp;rft.issue=1&amp;rft.date=1996&amp;rft.aulast=Thaler&amp;rft.aufirst=S.+L.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></li></ul>
</span></li>
<li id="cite&#95;note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFYonggang&#95;DengByrne2008" class="citation journal cs1">Yonggang Deng; Byrne, W. (2008). <a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/document/4443885/">"HMM Word and Phrase Alignment for Statistical Machine Translation"</a>. <i>IEEE Transactions on Audio, Speech, and Language Processing</i>. <b>16</b> (3): <span class="nowrap">494–</span>507. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FTASL.2008.916056">10.1109/TASL.2008.916056</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/1558-7916">1558-7916</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Audio%2C+Speech%2C+and+Language+Processing&amp;rft.atitle=HMM+Word+and+Phrase+Alignment+for+Statistical+Machine+Translation&amp;rft.volume=16&amp;rft.issue=3&amp;rft.pages=494-507&amp;rft.date=2008&amp;rft&#95;id=info%3Adoi%2F10.1109%2FTASL.2008.916056&amp;rft.issn=1558-7916&amp;rft.au=Yonggang+Deng&amp;rft.au=Byrne%2C+W.&amp;rft&#95;id=http%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F4443885%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFGuptaMalik2015" class="citation cs2">Gupta, Saurabh; Malik, Jitendra (17 May 2015), <a rel="nofollow" class="external text" href="http://arxiv.org/abs/1505.04474"><i>Visual Semantic Role Labeling</i></a>, arXiv, <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.48550%2FarXiv.1505.04474">10.48550/arXiv.1505.04474</a>, arXiv:1505.04474</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Visual+Semantic+Role+Labeling&amp;rft.pub=arXiv&amp;rft.date=2015-05-17&amp;rft&#95;id=info%3Adoi%2F10.48550%2FarXiv.1505.04474&amp;rft.aulast=Gupta&amp;rft.aufirst=Saurabh&amp;rft.au=Malik%2C+Jitendra&amp;rft&#95;id=http%3A%2F%2Farxiv.org%2Fabs%2F1505.04474&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://research.google/pubs/hallucinations-in-neural-machine-translation/">"Hallucinations in Neural Machine Translation"</a>. <i>research.google</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20240402084731/https://research.google/pubs/hallucinations-in-neural-machine-translation/">Archived</a> from the original on 2 April 2024<span class="reference-accessdate">. Retrieved <span class="nowrap">2 April</span> 2024</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=research.google&amp;rft.atitle=Hallucinations+in+Neural+Machine+Translation&amp;rft&#95;id=https%3A%2F%2Fresearch.google%2Fpubs%2Fhallucinations-in-neural-machine-translation%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Simonite2018-20"><span class="mw-cite-backlink">^ <a href="#cite_ref-Simonite2018_20-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Simonite2018_20-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFSimonite2018" class="citation magazine cs1">Simonite, Tom (9 March 2018). <a rel="nofollow" class="external text" href="https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/">"AI Has a Hallucination Problem That's Proving Tough to Fix"</a>. <i><a href="/wiki/Wired_(magazine)" title="Wired (magazine)">Wired</a></i>. <a href="/wiki/Cond%C3%A9_Nast" title="Condé Nast">Condé Nast</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230405082603/https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/">Archived</a> from the original on 5 April 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">29 December</span> 2022</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft.atitle=AI+Has+a+Hallucination+Problem+That%27s+Proving+Tough+to+Fix&amp;rft.date=2018-03-09&amp;rft.aulast=Simonite&amp;rft.aufirst=Tom&amp;rft&#95;id=https%3A%2F%2Fwww.wired.com%2Fstory%2Fai-has-a-hallucination-problem-thats-proving-tough-to-fix%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFZhuoHuangChenXing2023" class="citation arxiv cs1">Zhuo, Terry Yue; Huang, Yujin; Chen, Chunyang; Xing, Zhenchang (2023). "Exploring AI Ethics of ChatGPT: A Diagnostic Analysis". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2301.12867">2301.12867</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Exploring+AI+Ethics+of+ChatGPT%3A+A+Diagnostic+Analysis&amp;rft.date=2023&amp;rft&#95;id=info%3Aarxiv%2F2301.12867&amp;rft.aulast=Zhuo&amp;rft.aufirst=Terry+Yue&amp;rft.au=Huang%2C+Yujin&amp;rft.au=Chen%2C+Chunyang&amp;rft.au=Xing%2C+Zhenchang&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://ai.meta.com/blog/blender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet/">"Blender Bot 2.0: An open source chatbot that builds long-term memory and searches the internet"</a>. <i>ai.meta.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2 March</span> 2024</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=ai.meta.com&amp;rft.atitle=Blender+Bot+2.0%3A+An+open+source+chatbot+that+builds+long-term+memory+and+searches+the+internet&amp;rft&#95;id=https%3A%2F%2Fai.meta.com%2Fblog%2Fblender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFTung2022" class="citation news cs1">Tung, Liam (8 August 2022). <a rel="nofollow" class="external text" href="https://www.zdnet.com/article/meta-warns-its-new-chatbot-may-not-tell-you-the-truth/">"Meta warns its new chatbot may forget that it's a bot"</a>. <a href="/wiki/ZDNET" title="ZDNET">ZDNET</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230326022031/https://www.zdnet.com/article/meta-warns-its-new-chatbot-may-not-tell-you-the-truth/">Archived</a> from the original on 26 March 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">30 December</span> 2022</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Meta+warns+its+new+chatbot+may+forget+that+it%27s+a+bot&amp;rft.date=2022-08-08&amp;rft.aulast=Tung&amp;rft.aufirst=Liam&amp;rft&#95;id=https%3A%2F%2Fwww.zdnet.com%2Farticle%2Fmeta-warns-its-new-chatbot-may-not-tell-you-the-truth%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFSeife2022" class="citation news cs1">Seife, Charles (13 December 2022). <a rel="nofollow" class="external text" href="https://slate.com/technology/2022/12/davinci-003-chatbot-gpt-wrote-my-obituary.html">"The Alarming Deceptions at the Heart of an Astounding New Chatbot"</a>. <i>Slate</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230326021521/https://slate.com/technology/2022/12/davinci-003-chatbot-gpt-wrote-my-obituary.html">Archived</a> from the original on 26 March 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">16 February</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Slate&amp;rft.atitle=The+Alarming+Deceptions+at+the+Heart+of+an+Astounding+New+Chatbot&amp;rft.date=2022-12-13&amp;rft.aulast=Seife&amp;rft.aufirst=Charles&amp;rft&#95;id=https%3A%2F%2Fslate.com%2Ftechnology%2F2022%2F12%2Fdavinci-003-chatbot-gpt-wrote-my-obituary.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFWeiseMetz2023" class="citation news cs1">Weise, Karen; Metz, Cade (1 May 2023). <a rel="nofollow" class="external text" href="https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html">"When A.I. Chatbots Hallucinate"</a>. <i>The New York Times</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20240404080348/https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html">Archived</a> from the original on 4 April 2024<span class="reference-accessdate">. Retrieved <span class="nowrap">8 May</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=When+A.I.+Chatbots+Hallucinate&amp;rft.date=2023-05-01&amp;rft.aulast=Weise&amp;rft.aufirst=Karen&amp;rft.au=Metz%2C+Cade&amp;rft&#95;id=https%3A%2F%2Fwww.nytimes.com%2F2023%2F05%2F01%2Fbusiness%2Fai-chatbots-hallucination.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFCreamer2023" class="citation web cs1">Creamer, Ella (15 November 2023). <a rel="nofollow" class="external text" href="https://www.theguardian.com/books/2023/nov/15/hallucinate-cambridge-dictionary-word-of-the-year">"<span class="cs1-kern-left"></span>'Hallucinate' chosen as Cambridge dictionary's word of the year"</a>. <i>The Guardian</i><span class="reference-accessdate">. Retrieved <span class="nowrap">7 June</span> 2024</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Guardian&amp;rft.atitle=%27Hallucinate%27+chosen+as+Cambridge+dictionary%27s+word+of+the+year&amp;rft.date=2023-11-15&amp;rft.aulast=Creamer&amp;rft.aufirst=Ella&amp;rft&#95;id=https%3A%2F%2Fwww.theguardian.com%2Fbooks%2F2023%2Fnov%2F15%2Fhallucinate-cambridge-dictionary-word-of-the-year&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.clarin.com/deportes/joaquin-correa-presentado-botafogo-jugar-mundial-clubes-insolito-cruce-periodista-hermano_0_1HjL2zmnR0.html">"Joaquín Correa fue presentado en Botafogo para jugar el Mundial de Clubes y tuvo un insólito cruce con un periodista: 'No es mi hermano'<span class="cs1-kern-right"></span>"</a>. <i>Clarin</i>. 14 June 2025<span class="reference-accessdate">. Retrieved <span class="nowrap">10 August</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Clarin&amp;rft.atitle=Joaqu%C3%ADn+Correa+fue+presentado+en+Botafogo+para+jugar+el+Mundial+de+Clubes+y+tuvo+un+ins%C3%B3lito+cruce+con+un+periodista%3A+%27No+es+mi+hermano%27&amp;rft.date=2025-06-14&amp;rft&#95;id=https%3A%2F%2Fwww.clarin.com%2Fdeportes%2Fjoaquin-correa-presentado-botafogo-jugar-mundial-clubes-insolito-cruce-periodista-hermano&#95;0&#95;1HjL2zmnR0.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-cnbc&#95;new&#95;way-28"><span class="mw-cite-backlink">^ <a href="#cite_ref-cnbc_new_way_28-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-cnbc_new_way_28-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-cnbc_new_way_28-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFField2023" class="citation news cs1">Field, Hayden (31 May 2023). <a rel="nofollow" class="external text" href="https://www.cnbc.com/2023/05/31/openai-is-pursuing-a-new-way-to-fight-ai-hallucinations.html">"OpenAI is pursuing a new way to fight A.I. 'hallucinations'<span class="cs1-kern-right"></span>"</a>. <i><a href="/wiki/CNBC" title="CNBC">CNBC</a></i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230610231908/https://www.cnbc.com/2023/05/31/openai-is-pursuing-a-new-way-to-fight-ai-hallucinations.html">Archived</a> from the original on 10 June 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">11 June</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=CNBC&amp;rft.atitle=OpenAI+is+pursuing+a+new+way+to+fight+A.I.+%27hallucinations%27&amp;rft.date=2023-05-31&amp;rft.aulast=Field&amp;rft.aufirst=Hayden&amp;rft&#95;id=https%3A%2F%2Fwww.cnbc.com%2F2023%2F05%2F31%2Fopenai-is-pursuing-a-new-way-to-fight-ai-hallucinations.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-29">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFVincent2023" class="citation news cs1">Vincent, James (8 February 2023). <a rel="nofollow" class="external text" href="https://www.theverge.com/2023/2/8/23590864/google-ai-chatbot-bard-mistake-error-exoplanet-demo">"Google's AI chatbot Bard makes factual error in first demo"</a>. <i><a href="/wiki/The_Verge" title="The Verge">The Verge</a></i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230212094317/https://www.theverge.com/2023/2/8/23590864/google-ai-chatbot-bard-mistake-error-exoplanet-demo">Archived</a> from the original on 12 February 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">11 June</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Verge&amp;rft.atitle=Google%27s+AI+chatbot+Bard+makes+factual+error+in+first+demo&amp;rft.date=2023-02-08&amp;rft.aulast=Vincent&amp;rft.aufirst=James&amp;rft&#95;id=https%3A%2F%2Fwww.theverge.com%2F2023%2F2%2F8%2F23590864%2Fgoogle-ai-chatbot-bard-mistake-error-exoplanet-demo&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-nyt-science-30"><span class="mw-cite-backlink">^ <a href="#cite_ref-nyt-science_30-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-nyt-science_30-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-nyt-science_30-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-nyt-science_30-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-nyt-science_30-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-nyt-science_30-5"><sup><i><b>f</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFBroad2024" class="citation news cs1">Broad, William J. (23 December 2024). <a rel="nofollow" class="external text" href="https://www.nytimes.com/2024/12/23/science/ai-hallucinations-science.html">"How Hallucinatory A.I. Helps Science Dream Up Big Breakthroughs"</a>. <i>The New York Times</i>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=How+Hallucinatory+A.I.+Helps+Science+Dream+Up+Big+Breakthroughs&amp;rft.date=2024-12-23&amp;rft.aulast=Broad&amp;rft.aufirst=William+J.&amp;rft&#95;id=https%3A%2F%2Fwww.nytimes.com%2F2024%2F12%2F23%2Fscience%2Fai-hallucinations-science.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFStening2023" class="citation web cs1">Stening, Tanner (10 November 2023). <a rel="nofollow" class="external text" href="https://news.northeastern.edu/2023/11/10/ai-chatbot-hallucinations/">"What are AI chatbots actually doing when they 'hallucinate'? Here's why experts don't like the term"</a>. <i>Northeastern Global News</i><span class="reference-accessdate">. Retrieved <span class="nowrap">14 June</span> 2024</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Northeastern+Global+News&amp;rft.atitle=What+are+AI+chatbots+actually+doing+when+they+%27hallucinate%27%3F+Here%27s+why+experts+don%27t+like+the+term&amp;rft.date=2023-11-10&amp;rft.aulast=Stening&amp;rft.aufirst=Tanner&amp;rft&#95;id=https%3A%2F%2Fnews.northeastern.edu%2F2023%2F11%2F10%2Fai-chatbot-hallucinations%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFTonmoyZamanJainRani2024" class="citation arxiv cs1">Tonmoy, S. M. Towhidul Islam; Zaman, S. M. Mehedi; Jain, Vinija; Rani, Anku; Rawte, Vipula; Chadha, Aman; Das, Amitava (8 January 2024). "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2401.01313">2401.01313</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=A+Comprehensive+Survey+of+Hallucination+Mitigation+Techniques+in+Large+Language+Models&amp;rft.date=2024-01-08&amp;rft&#95;id=info%3Aarxiv%2F2401.01313&amp;rft.aulast=Tonmoy&amp;rft.aufirst=S.+M.+Towhidul+Islam&amp;rft.au=Zaman%2C+S.+M.+Mehedi&amp;rft.au=Jain%2C+Vinija&amp;rft.au=Rani%2C+Anku&amp;rft.au=Rawte%2C+Vipula&amp;rft.au=Chadha%2C+Aman&amp;rft.au=Das%2C+Amitava&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFOpenAI2023" class="citation arxiv cs1">OpenAI (2023). "GPT-4 Technical Report". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2303.08774">2303.08774</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=GPT-4+Technical+Report&amp;rft.date=2023&amp;rft&#95;id=info%3Aarxiv%2F2303.08774&amp;rft.au=OpenAI&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-34">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFBarassi2024" class="citation journal cs1">Barassi, Veronica (2024). <a rel="nofollow" class="external text" href="https://hdsr.mitpress.mit.edu/pub/1yo82mqa/release/2">"Toward a Theory of AI Errors: Making Sense of Hallucinations, Catastrophic Failures, and the Fallacy of Generative AI"</a>. <i>Harvard Data Science Review</i>. <b>5</b>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2024HDSRv...5ebbd4B">2024HDSRv...5ebbd4B</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1162%2F99608f92.ad8ebbd4">10.1162/99608f92.ad8ebbd4</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Harvard+Data+Science+Review&amp;rft.atitle=Toward+a+Theory+of+AI+Errors%3A+Making+Sense+of+Hallucinations%2C+Catastrophic+Failures%2C+and+the+Fallacy+of+Generative+AI&amp;rft.volume=5&amp;rft.date=2024&amp;rft&#95;id=info%3Adoi%2F10.1162%2F99608f92.ad8ebbd4&amp;rft&#95;id=info%3Abibcode%2F2024HDSRv...5ebbd4B&amp;rft.aulast=Barassi&amp;rft.aufirst=Veronica&amp;rft&#95;id=https%3A%2F%2Fhdsr.mitpress.mit.edu%2Fpub%2F1yo82mqa%2Frelease%2F2&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-35">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFAaronson2024" class="citation report cs1">Aaronson, Susan Ariel (2024). <a rel="nofollow" class="external text" href="https://www.jstor.org/stable/resrep58361.6">Introduction: What Hath Generative Artificial Intelligence Wrought?</a> (Report). Centre for International Governance Innovation. pp.&#160;<span class="nowrap">1–</span>4.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=Introduction%3A+What+Hath+Generative+Artificial+Intelligence+Wrought%3F&amp;rft.pages=1-4&amp;rft.pub=Centre+for+International+Governance+Innovation&amp;rft.date=2024&amp;rft.aulast=Aaronson&amp;rft.aufirst=Susan+Ariel&amp;rft&#95;id=https%3A%2F%2Fwww.jstor.org%2Fstable%2Fresrep58361.6&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-:1-36"><span class="mw-cite-backlink">^ <a href="#cite_ref-:1_36-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_36-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFVaranasi" class="citation web cs1">Varanasi, Lakshmi. <a rel="nofollow" class="external text" href="https://www.businessinsider.com/why-ai-chatbots-hallucinate-openai-chatgpt-anthropic-claude-2025-9">"Why AI chatbots hallucinate, according to OpenAI researchers"</a>. <i>Business Insider</i><span class="reference-accessdate">. Retrieved <span class="nowrap">28 September</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Business+Insider&amp;rft.atitle=Why+AI+chatbots+hallucinate%2C+according+to+OpenAI+researchers&amp;rft.aulast=Varanasi&amp;rft.aufirst=Lakshmi&amp;rft&#95;id=https%3A%2F%2Fwww.businessinsider.com%2Fwhy-ai-chatbots-hallucinate-openai-chatgpt-anthropic-claude-2025-9&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.anthropic.com/research/tracing-thoughts-language-model">"Tracing the thoughts of a large language model"</a>. <i>Anthropic</i>. 27 March 2025<span class="reference-accessdate">. Retrieved <span class="nowrap">29 March</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Anthropic&amp;rft.atitle=Tracing+the+thoughts+of+a+large+language+model&amp;rft.date=2025-03-27&amp;rft&#95;id=https%3A%2F%2Fwww.anthropic.com%2Fresearch%2Ftracing-thoughts-language-model&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFAmabilePratt2016" class="citation journal cs1">Amabile, Teresa M.; Pratt, Michael G. (2016). "The dynamic componential model of creativity and innovation in organizations: Making progress, making meaning". <i>Research in Organizational Behavior</i>. <b>36</b>: <span class="nowrap">157–</span>183. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.riob.2016.10.001">10.1016/j.riob.2016.10.001</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Research+in+Organizational+Behavior&amp;rft.atitle=The+dynamic+componential+model+of+creativity+and+innovation+in+organizations%3A+Making+progress%2C+making+meaning&amp;rft.volume=36&amp;rft.pages=157-183&amp;rft.date=2016&amp;rft&#95;id=info%3Adoi%2F10.1016%2Fj.riob.2016.10.001&amp;rft.aulast=Amabile&amp;rft.aufirst=Teresa+M.&amp;rft.au=Pratt%2C+Michael+G.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-39">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFMukherjeeChang2023" class="citation journal cs1">Mukherjee, Anirban; Chang, Hannah H. (2023). <a rel="nofollow" class="external text" href="https://cmr.berkeley.edu/2023/07/managing-the-creative-frontier-of-generative-ai-the-novelty-usefulness-tradeoff">"Managing the Creative Frontier of Generative AI: The Novelty-Usefulness Tradeoff"</a>. <i>California Management Review</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20240105204458/https://cmr.berkeley.edu/2023/07/managing-the-creative-frontier-of-generative-ai-the-novelty-usefulness-tradeoff/">Archived</a> from the original on 5 January 2024<span class="reference-accessdate">. Retrieved <span class="nowrap">5 January</span> 2024</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=California+Management+Review&amp;rft.atitle=Managing+the+Creative+Frontier+of+Generative+AI%3A+The+Novelty-Usefulness+Tradeoff&amp;rft.date=2023&amp;rft.aulast=Mukherjee&amp;rft.aufirst=Anirban&amp;rft.au=Chang%2C+Hannah+H.&amp;rft&#95;id=https%3A%2F%2Fcmr.berkeley.edu%2F2023%2F07%2Fmanaging-the-creative-frontier-of-generative-ai-the-novelty-usefulness-tradeoff&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-40">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFMetz2022" class="citation news cs1">Metz, Cade (10 December 2022). <a rel="nofollow" class="external text" href="https://www.nytimes.com/2022/12/10/technology/ai-chat-bot-chatgpt.html">"The New Chatbots Could Change the World. Can You Trust Them?"</a>. <i>The New York Times</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230117124217/https://www.nytimes.com/2022/12/10/technology/ai-chat-bot-chatgpt.html">Archived</a> from the original on 17 January 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">30 December</span> 2022</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=The+New+Chatbots+Could+Change+the+World.+Can+You+Trust+Them%3F&amp;rft.date=2022-12-10&amp;rft.aulast=Metz&amp;rft.aufirst=Cade&amp;rft&#95;id=https%3A%2F%2Fwww.nytimes.com%2F2022%2F12%2F10%2Ftechnology%2Fai-chat-bot-chatgpt.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-41">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFJiLeeFrieskeYu2024" class="citation web cs1">Ji, Ziwei; Lee, Nayeon; Frieske, Rita; Yu, Tiezheng; Su, Dan; Xu, Yan; Ishii, Etsuko; Bang, Yejin; Chen, Delong; Chan, Ho Shu; Dai, Wenliang; Madotto, Andrea; Fung, Pascale (19 February 2024). <a rel="nofollow" class="external text" href="https://arxiv.org/html/2202.03629v6">"Survey of Hallucination in Natural Language Generation"</a>. <i>arxiv.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">25 November</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=arxiv.org&amp;rft.atitle=Survey+of+Hallucination+in+Natural+Language+Generation&amp;rft.date=2024-02-19&amp;rft.aulast=Ji&amp;rft.aufirst=Ziwei&amp;rft.au=Lee%2C+Nayeon&amp;rft.au=Frieske%2C+Rita&amp;rft.au=Yu%2C+Tiezheng&amp;rft.au=Su%2C+Dan&amp;rft.au=Xu%2C+Yan&amp;rft.au=Ishii%2C+Etsuko&amp;rft.au=Bang%2C+Yejin&amp;rft.au=Chen%2C+Delong&amp;rft.au=Chan%2C+Ho+Shu&amp;rft.au=Dai%2C+Wenliang&amp;rft.au=Madotto%2C+Andrea&amp;rft.au=Fung%2C+Pascale&amp;rft&#95;id=https%3A%2F%2Farxiv.org%2Fhtml%2F2202.03629v6&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-42"><span class="mw-cite-backlink"><b><a href="#cite_ref-42">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFNuñez2025" class="citation web cs1">Nuñez, Michael (27 March 2025). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20250328173957/https://venturebeat.com/ai/anthropic-scientists-expose-how-ai-actually-thinks-and-discover-it-secretly-plans-ahead-and-sometimes-lies/">"Anthropic scientists expose how AI actually 'thinks' — and discover it secretly plans ahead and sometimes lies"</a>. <i>VentureBeat</i>. Archived from <a rel="nofollow" class="external text" href="https://venturebeat.com/ai/anthropic-scientists-expose-how-ai-actually-thinks-and-discover-it-secretly-plans-ahead-and-sometimes-lies/">the original</a> on 28 March 2025<span class="reference-accessdate">. Retrieved <span class="nowrap">30 March</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=VentureBeat&amp;rft.atitle=Anthropic+scientists+expose+how+AI+actually+%27thinks%27+%E2%80%94+and+discover+it+secretly+plans+ahead+and+sometimes+lies&amp;rft.date=2025-03-27&amp;rft.aulast=Nu%C3%B1ez&amp;rft.aufirst=Michael&amp;rft&#95;id=https%3A%2F%2Fventurebeat.com%2Fai%2Fanthropic-scientists-expose-how-ai-actually-thinks-and-discover-it-secretly-plans-ahead-and-sometimes-lies%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFTaylorKardasCucurullScialom2022" class="citation arxiv cs1">Taylor, Ross; Kardas, Marcin; Cucurull, Guillem; Scialom, Thomas; Hartshorn, Anthony; Saravia, Elvis; Poulton, Andrew; Kerkez, Viktor; Stojnic, Robert (16 November 2022). "Galactica: A Large Language Model for Science". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2211.09085">2211.09085</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Galactica%3A+A+Large+Language+Model+for+Science&amp;rft.date=2022-11-16&amp;rft&#95;id=info%3Aarxiv%2F2211.09085&amp;rft.aulast=Taylor&amp;rft.aufirst=Ross&amp;rft.au=Kardas%2C+Marcin&amp;rft.au=Cucurull%2C+Guillem&amp;rft.au=Scialom%2C+Thomas&amp;rft.au=Hartshorn%2C+Anthony&amp;rft.au=Saravia%2C+Elvis&amp;rft.au=Poulton%2C+Andrew&amp;rft.au=Kerkez%2C+Viktor&amp;rft.au=Stojnic%2C+Robert&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-44">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFEdwards2022" class="citation news cs1">Edwards, Benj (18 November 2022). <a rel="nofollow" class="external text" href="https://arstechnica.com/information-technology/2022/11/after-controversy-meta-pulls-demo-of-ai-model-that-writes-scientific-papers/">"New Meta AI demo writes racist and inaccurate scientific literature, gets pulled"</a>. <i><a href="/wiki/Ars_Technica" title="Ars Technica">Ars Technica</a></i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230410190326/https://arstechnica.com/information-technology/2022/11/after-controversy-meta-pulls-demo-of-ai-model-that-writes-scientific-papers/">Archived</a> from the original on 10 April 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">30 December</span> 2022</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Ars+Technica&amp;rft.atitle=New+Meta+AI+demo+writes+racist+and+inaccurate+scientific+literature%2C+gets+pulled&amp;rft.date=2022-11-18&amp;rft.aulast=Edwards&amp;rft.aufirst=Benj&amp;rft&#95;id=https%3A%2F%2Farstechnica.com%2Finformation-technology%2F2022%2F11%2Fafter-controversy-meta-pulls-demo-of-ai-model-that-writes-scientific-papers%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFScialom2024" class="citation interview cs1">Scialom, Thomas (23 July 2024). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20240724180028/https://www.latent.space/p/llama-3">"Llama 2, 3 &amp; 4: Synthetic Data, RLHF, Agents on the path to Open Source AGI"</a>. <i>Latent Space</i> (Interview). Interviewed by swyx &amp; Alessio. Archived from <a rel="nofollow" class="external text" href="https://www.latent.space/p/llama-3">the original</a> on 24 July 2024.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Latent+Space&amp;rft.atitle=Llama+2%2C+3+%26+4%3A+Synthetic+Data%2C+RLHF%2C+Agents+on+the+path+to+Open+Source+AGI&amp;rft.date=2024-07-23&amp;rft.aulast=Scialom&amp;rft.aufirst=Thomas&amp;rft&#95;id=https%3A%2F%2Fwww.latent.space%2Fp%2Fllama-3&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFBowman2022" class="citation news cs1">Bowman, Emma (19 December 2022). <a rel="nofollow" class="external text" href="https://www.npr.org/2022/12/19/1143912956/chatgpt-ai-chatbot-homework-academia">"A new AI chatbot might do your homework for you. But it's still not an A+ student"</a>. <a href="/wiki/NPR" title="NPR">NPR</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230120095239/https://www.npr.org/2022/12/19/1143912956/chatgpt-ai-chatbot-homework-academia/">Archived</a> from the original on 20 January 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">29 December</span> 2022</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=A+new+AI+chatbot+might+do+your+homework+for+you.+But+it%27s+still+not+an+A%2B+student&amp;rft.date=2022-12-19&amp;rft.aulast=Bowman&amp;rft.aufirst=Emma&amp;rft&#95;id=https%3A%2F%2Fwww.npr.org%2F2022%2F12%2F19%2F1143912956%2Fchatgpt-ai-chatbot-homework-academia&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFPitt2022" class="citation news cs1">Pitt, Sofia (15 December 2022). <a rel="nofollow" class="external text" href="https://www.cnbc.com/2022/12/15/google-vs-chatgpt-what-happened-when-i-swapped-services-for-a-day.html">"Google vs. ChatGPT: Here's what happened when I swapped services for a day"</a>. <a href="/wiki/CNBC" title="CNBC">CNBC</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230116171232/https://www.cnbc.com/2022/12/15/google-vs-chatgpt-what-happened-when-i-swapped-services-for-a-day.html">Archived</a> from the original on 16 January 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">30 December</span> 2022</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Google+vs.+ChatGPT%3A+Here%27s+what+happened+when+I+swapped+services+for+a+day&amp;rft.date=2022-12-15&amp;rft.aulast=Pitt&amp;rft.aufirst=Sofia&amp;rft&#95;id=https%3A%2F%2Fwww.cnbc.com%2F2022%2F12%2F15%2Fgoogle-vs-chatgpt-what-happened-when-i-swapped-services-for-a-day.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-48"><span class="mw-cite-backlink"><b><a href="#cite_ref-48">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFHuizinga2022" class="citation news cs1">Huizinga, Raechel (30 December 2022). <a rel="nofollow" class="external text" href="https://www.cbc.ca/news/canada/new-brunswick/ai-question-about-nb-1.6699498">"We asked an AI questions about New Brunswick. Some of the answers may surprise you"</a>. <a href="/wiki/CBC_News" title="CBC News">CBC News</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230106052253/https://www.cbc.ca/news/canada/new-brunswick/ai-question-about-nb-1.6699498">Archived</a> from the original on 6 January 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">30 December</span> 2022</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=We+asked+an+AI+questions+about+New+Brunswick.+Some+of+the+answers+may+surprise+you&amp;rft.date=2022-12-30&amp;rft.aulast=Huizinga&amp;rft.aufirst=Raechel&amp;rft&#95;id=https%3A%2F%2Fwww.cbc.ca%2Fnews%2Fcanada%2Fnew-brunswick%2Fai-question-about-nb-1.6699498&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-49"><span class="mw-cite-backlink"><b><a href="#cite_ref-49">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFZastrow2022" class="citation news cs1">Zastrow, Mark (30 December 2022). <a rel="nofollow" class="external text" href="https://www.discovermagazine.com/technology/we-asked-chatgpt-your-questions-about-astronomy-it-didnt-go-so-well">"We Asked ChatGPT Your Questions About Astronomy. It Didn't Go so Well"</a>. <i><a href="/wiki/Discover_(magazine)" title="Discover (magazine)">Discover</a></i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230326021742/https://www.discovermagazine.com/technology/we-asked-chatgpt-your-questions-about-astronomy-it-didnt-go-so-well">Archived</a> from the original on 26 March 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">31 December</span> 2022</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Discover&amp;rft.atitle=We+Asked+ChatGPT+Your+Questions+About+Astronomy.+It+Didn%27t+Go+so+Well.&amp;rft.date=2022-12-30&amp;rft.aulast=Zastrow&amp;rft.aufirst=Mark&amp;rft&#95;id=https%3A%2F%2Fwww.discovermagazine.com%2Ftechnology%2Fwe-asked-chatgpt-your-questions-about-astronomy-it-didnt-go-so-well&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-fast&#95;company&#95;2022-50"><span class="mw-cite-backlink"><b><a href="#cite_ref-fast_company_2022_50-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFLin2022" class="citation news cs1">Lin, Connie (5 December 2022). <a rel="nofollow" class="external text" href="https://www.fastcompany.com/90819887/how-to-trick-openai-chat-gpt">"How to easily trick OpenAI's genius new ChatGPT"</a>. <i>Fast Company</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230329155859/https://www.fastcompany.com/90819887/how-to-trick-openai-chat-gpt">Archived</a> from the original on 29 March 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">6 January</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Fast+Company&amp;rft.atitle=How+to+easily+trick+OpenAI%27s+genius+new+ChatGPT&amp;rft.date=2022-12-05&amp;rft.aulast=Lin&amp;rft.aufirst=Connie&amp;rft&#95;id=https%3A%2F%2Fwww.fastcompany.com%2F90819887%2Fhow-to-trick-openai-chat-gpt&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-51"><span class="mw-cite-backlink"><b><a href="#cite_ref-51">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFEdwards2022" class="citation news cs1">Edwards, Benj (1 December 2022). <a rel="nofollow" class="external text" href="https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/">"OpenAI invites everyone to test ChatGPT, a new AI-powered chatbot—with amusing results"</a>. <i><a href="/wiki/Ars_Technica" title="Ars Technica">Ars Technica</a></i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230315222006/https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/">Archived</a> from the original on 15 March 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">29 December</span> 2022</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Ars+Technica&amp;rft.atitle=OpenAI+invites+everyone+to+test+ChatGPT%2C+a+new+AI-powered+chatbot%E2%80%94with+amusing+results&amp;rft.date=2022-12-01&amp;rft.aulast=Edwards&amp;rft.aufirst=Benj&amp;rft&#95;id=https%3A%2F%2Farstechnica.com%2Finformation-technology%2F2022%2F12%2Fopenai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-52"><span class="mw-cite-backlink"><b><a href="#cite_ref-52">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFMollick2022" class="citation news cs1">Mollick, Ethan (14 December 2022). <a rel="nofollow" class="external text" href="https://hbr.org/2022/12/chatgpt-is-a-tipping-point-for-ai">"ChatGPT Is a Tipping Point for AI"</a>. <i>Harvard Business Review</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230411151336/https://hbr.org/2022/12/chatgpt-is-a-tipping-point-for-ai">Archived</a> from the original on 11 April 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">29 December</span> 2022</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Harvard+Business+Review&amp;rft.atitle=ChatGPT+Is+a+Tipping+Point+for+AI&amp;rft.date=2022-12-14&amp;rft.aulast=Mollick&amp;rft.aufirst=Ethan&amp;rft&#95;id=https%3A%2F%2Fhbr.org%2F2022%2F12%2Fchatgpt-is-a-tipping-point-for-ai&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-53"><span class="mw-cite-backlink"><b><a href="#cite_ref-53">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFKantrowitz2022" class="citation news cs1">Kantrowitz, Alex (2 December 2022). <a rel="nofollow" class="external text" href="https://slate.com/technology/2022/12/chatgpt-openai-artificial-intelligence-chatbot-whoa.html">"Finally, an A.I. Chatbot That Reliably Passes 'the Nazi Test'<span class="cs1-kern-right"></span>"</a>. <i><a href="/wiki/Slate_(magazine)" title="Slate (magazine)">Slate</a></i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230117012901/https://slate.com/technology/2022/12/chatgpt-openai-artificial-intelligence-chatbot-whoa.html">Archived</a> from the original on 17 January 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">29 December</span> 2022</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Slate&amp;rft.atitle=Finally%2C+an+A.I.+Chatbot+That+Reliably+Passes+%27the+Nazi+Test%27&amp;rft.date=2022-12-02&amp;rft.aulast=Kantrowitz&amp;rft.aufirst=Alex&amp;rft&#95;id=https%3A%2F%2Fslate.com%2Ftechnology%2F2022%2F12%2Fchatgpt-openai-artificial-intelligence-chatbot-whoa.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-54"><span class="mw-cite-backlink"><b><a href="#cite_ref-54">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFMarcus2022" class="citation web cs1">Marcus, Gary (2 December 2022). <a rel="nofollow" class="external text" href="https://garymarcus.substack.com/p/how-come-gpt-can-seem-so-brilliant">"How come GPT can seem so brilliant one minute and so breathtakingly dumb the next?"</a>. <i>The Road to AI We Can Trust</i>. <a href="/wiki/Substack" title="Substack">Substack</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221230003606/https://garymarcus.substack.com/p/how-come-gpt-can-seem-so-brilliant">Archived</a> from the original on 30 December 2022<span class="reference-accessdate">. Retrieved <span class="nowrap">29 December</span> 2022</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Road+to+AI+We+Can+Trust&amp;rft.atitle=How+come+GPT+can+seem+so+brilliant+one+minute+and+so+breathtakingly+dumb+the+next%3F&amp;rft.date=2022-12-02&amp;rft.aulast=Marcus&amp;rft.aufirst=Gary&amp;rft&#95;id=https%3A%2F%2Fgarymarcus.substack.com%2Fp%2Fhow-come-gpt-can-seem-so-brilliant&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-55"><span class="mw-cite-backlink"><b><a href="#cite_ref-55">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation news cs1"><a rel="nofollow" class="external text" href="https://www.reuters.com/technology/google-cautions-against-hallucinating-chatbots-report-2023-02-11/">"Google cautions against 'hallucinating' chatbots, report says"</a>. Reuters. 11 February 2023. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230406154650/https://www.reuters.com/technology/google-cautions-against-hallucinating-chatbots-report-2023-02-11/">Archived</a> from the original on 6 April 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">16 February</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Google+cautions+against+%27hallucinating%27+chatbots%2C+report+says&amp;rft.date=2023-02-11&amp;rft&#95;id=https%3A%2F%2Fwww.reuters.com%2Ftechnology%2Fgoogle-cautions-against-hallucinating-chatbots-report-2023-02-11%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-56"><span class="mw-cite-backlink"><b><a href="#cite_ref-56">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFMaruf2023" class="citation news cs1">Maruf, Ramishah (27 May 2023). <a rel="nofollow" class="external text" href="https://www.cnn.com/2023/05/27/business/chat-gpt-avianca-mata-lawyers/index.html">"Lawyer apologizes for fake court citations from ChatGPT"</a>. CNN Business.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Lawyer+apologizes+for+fake+court+citations+from+ChatGPT&amp;rft.date=2023-05-27&amp;rft.aulast=Maruf&amp;rft.aufirst=Ramishah&amp;rft&#95;id=https%3A%2F%2Fwww.cnn.com%2F2023%2F05%2F27%2Fbusiness%2Fchat-gpt-avianca-mata-lawyers%2Findex.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-57"><span class="mw-cite-backlink"><b><a href="#cite_ref-57">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFBrodkin2023" class="citation news cs1">Brodkin, Jon (31 May 2023). <a rel="nofollow" class="external text" href="https://arstechnica.com/tech-policy/2023/05/federal-judge-no-ai-in-my-courtroom-unless-a-human-verifies-its-accuracy/">"Federal judge: No AI in my courtroom unless a human verifies its accuracy"</a>. <i><a href="/wiki/Ars_Technica" title="Ars Technica">Ars Technica</a></i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230626111111/https://arstechnica.com/tech-policy/2023/05/federal-judge-no-ai-in-my-courtroom-unless-a-human-verifies-its-accuracy/">Archived</a> from the original on 26 June 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">26 June</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Ars+Technica&amp;rft.atitle=Federal+judge%3A+No+AI+in+my+courtroom+unless+a+human+verifies+its+accuracy&amp;rft.date=2023-05-31&amp;rft.aulast=Brodkin&amp;rft.aufirst=Jon&amp;rft&#95;id=https%3A%2F%2Farstechnica.com%2Ftech-policy%2F2023%2F05%2Ffederal-judge-no-ai-in-my-courtroom-unless-a-human-verifies-its-accuracy%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-58"><span class="mw-cite-backlink"><b><a href="#cite_ref-58">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.txnd.uscourts.gov/judge/judge-brantley-starr">"Judge Brantley Starr"</a>. Northern District of Texas | United States District Court. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230626111110/https://www.txnd.uscourts.gov/judge/judge-brantley-starr">Archived</a> from the original on 26 June 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">26 June</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Judge+Brantley+Starr&amp;rft.pub=Northern+District+of+Texas+%7C+United+States+District+Court&amp;rft&#95;id=https%3A%2F%2Fwww.txnd.uscourts.gov%2Fjudge%2Fjudge-brantley-starr&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-59"><span class="mw-cite-backlink"><b><a href="#cite_ref-59">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFBrodkin2023" class="citation news cs1">Brodkin, Jon (23 June 2023). <a rel="nofollow" class="external text" href="https://arstechnica.com/tech-policy/2023/06/lawyers-have-real-bad-day-in-court-after-citing-fake-cases-made-up-by-chatgpt/">"Lawyers have real bad day in court after citing fake cases made up by ChatGPT"</a>. <i><a href="/wiki/Ars_Technica" title="Ars Technica">Ars Technica</a></i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20240126071502/https://arstechnica.com/tech-policy/2023/06/lawyers-have-real-bad-day-in-court-after-citing-fake-cases-made-up-by-chatgpt/">Archived</a> from the original on 26 January 2024<span class="reference-accessdate">. Retrieved <span class="nowrap">26 June</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Ars+Technica&amp;rft.atitle=Lawyers+have+real+bad+day+in+court+after+citing+fake+cases+made+up+by+ChatGPT&amp;rft.date=2023-06-23&amp;rft.aulast=Brodkin&amp;rft.aufirst=Jon&amp;rft&#95;id=https%3A%2F%2Farstechnica.com%2Ftech-policy%2F2023%2F06%2Flawyers-have-real-bad-day-in-court-after-citing-fake-cases-made-up-by-chatgpt%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-60"><span class="mw-cite-backlink"><b><a href="#cite_ref-60">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFBelanger2023" class="citation news cs1">Belanger, Ashley (9 June 2023). <a rel="nofollow" class="external text" href="https://arstechnica.com/tech-policy/2023/06/openai-sued-for-defamation-after-chatgpt-fabricated-yet-another-lawsuit/">"OpenAI faces defamation suit after ChatGPT completely fabricated another lawsuit"</a>. <i><a href="/wiki/Ars_Technica" title="Ars Technica">Ars Technica</a></i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230701023643/https://arstechnica.com/tech-policy/2023/06/openai-sued-for-defamation-after-chatgpt-fabricated-yet-another-lawsuit/">Archived</a> from the original on 1 July 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">1 July</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Ars+Technica&amp;rft.atitle=OpenAI+faces+defamation+suit+after+ChatGPT+completely+fabricated+another+lawsuit&amp;rft.date=2023-06-09&amp;rft.aulast=Belanger&amp;rft.aufirst=Ashley&amp;rft&#95;id=https%3A%2F%2Farstechnica.com%2Ftech-policy%2F2023%2F06%2Fopenai-sued-for-defamation-after-chatgpt-fabricated-yet-another-lawsuit%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-61"><span class="mw-cite-backlink"><b><a href="#cite_ref-61">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFScarcella2025" class="citation news cs1">Scarcella, Mike (19 May 2025). <a rel="nofollow" class="external text" href="https://www.reuters.com/legal/litigation/openai-defeats-radio-hosts-lawsuit-over-allegations-invented-by-chatgpt-2025-05-19/">"OpenAI defeats radio host's lawsuit over allegations invented by ChatGPT"</a>. <i><a href="/wiki/Reuters" title="Reuters">Reuters</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">23 August</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Reuters&amp;rft.atitle=OpenAI+defeats+radio+host%27s+lawsuit+over+allegations+invented+by+ChatGPT&amp;rft.date=2025-05-19&amp;rft.aulast=Scarcella&amp;rft.aufirst=Mike&amp;rft&#95;id=https%3A%2F%2Fwww.reuters.com%2Flegal%2Flitigation%2Fopenai-defeats-radio-hosts-lawsuit-over-allegations-invented-by-chatgpt-2025-05-19%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-62"><span class="mw-cite-backlink"><b><a href="#cite_ref-62">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFBelanger2024" class="citation web cs1">Belanger, Ashley (16 February 2024). <a rel="nofollow" class="external text" href="https://arstechnica.com/tech-policy/2024/02/air-canada-must-honor-refund-policy-invented-by-airlines-chatbot/">"Air Canada must honor refund policy invented by airline's chatbot"</a>. <i>Ars Technica</i><span class="reference-accessdate">. Retrieved <span class="nowrap">22 April</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Ars+Technica&amp;rft.atitle=Air+Canada+must+honor+refund+policy+invented+by+airline%27s+chatbot&amp;rft.date=2024-02-16&amp;rft.aulast=Belanger&amp;rft.aufirst=Ashley&amp;rft&#95;id=https%3A%2F%2Farstechnica.com%2Ftech-policy%2F2024%2F02%2Fair-canada-must-honor-refund-policy-invented-by-airlines-chatbot%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-63"><span class="mw-cite-backlink"><b><a href="#cite_ref-63">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation news cs1"><a rel="nofollow" class="external text" href="https://vancouversun.com/news/local-news/air-canada-told-it-is-responsible-for-errors-by-its-website-chatbot">"Air Canada responsible for errors by website chatbot after B.C. customer denied retroactive discount"</a>. <i>vancouversun</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20250312171918/https://vancouversun.com/news/local-news/air-canada-told-it-is-responsible-for-errors-by-its-website-chatbot">Archived</a> from the original on 12 March 2025<span class="reference-accessdate">. Retrieved <span class="nowrap">22 April</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=vancouversun&amp;rft.atitle=Air+Canada+responsible+for+errors+by+website+chatbot+after+B.C.+customer+denied+retroactive+discount&amp;rft&#95;id=https%3A%2F%2Fvancouversun.com%2Fnews%2Flocal-news%2Fair-canada-told-it-is-responsible-for-errors-by-its-website-chatbot&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-64"><span class="mw-cite-backlink"><b><a href="#cite_ref-64">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFTadros2025" class="citation news cs1">Tadros, Edmund (6 October 2025). <a rel="nofollow" class="external text" href="https://www.afr.com/companies/professional-services/human-intelligence-problem-labor-senator-slams-deloitte-s-ai-bungle-20251006-p5n0ch">"<span class="cs1-kern-left"></span>'Full refund': Senator slams Deloitte's 'human intelligence problem'<span class="cs1-kern-right"></span>"</a>. <i>Australian Financial Review</i><span class="reference-accessdate">. Retrieved <span class="nowrap">12 October</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Australian+Financial+Review&amp;rft.atitle=%27Full+refund%27%3A+Senator+slams+Deloitte%27s+%27human+intelligence+problem%27&amp;rft.date=2025-10-06&amp;rft.aulast=Tadros&amp;rft.aufirst=Edmund&amp;rft&#95;id=https%3A%2F%2Fwww.afr.com%2Fcompanies%2Fprofessional-services%2Fhuman-intelligence-problem-labor-senator-slams-deloitte-s-ai-bungle-20251006-p5n0ch&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-65"><span class="mw-cite-backlink"><b><a href="#cite_ref-65">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFTadrosKarp2025" class="citation news cs1">Tadros, Edmund; Karp, Paul (5 October 2025). <span class="id-lock-subscription" title="Paid subscription required"><a rel="nofollow" class="external text" href="https://www.afr.com/companies/professional-services/deloitte-to-refund-government-after-admitting-ai-errors-in-440k-report-20251005-p5n05p">"Deloitte to refund government, admits using AI in $440k report"</a></span>. <i>Australian Financial Review</i><span class="reference-accessdate">. Retrieved <span class="nowrap">12 October</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Australian+Financial+Review&amp;rft.atitle=Deloitte+to+refund+government%2C+admits+using+AI+in+%24440k+report&amp;rft.date=2025-10-05&amp;rft.aulast=Tadros&amp;rft.aufirst=Edmund&amp;rft.au=Karp%2C+Paul&amp;rft&#95;id=https%3A%2F%2Fwww.afr.com%2Fcompanies%2Fprofessional-services%2Fdeloitte-to-refund-government-after-admitting-ai-errors-in-440k-report-20251005-p5n05p&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-66"><span class="mw-cite-backlink"><b><a href="#cite_ref-66">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFFerrieKaiser2019" class="citation book cs1">Ferrie, C.; Kaiser, S. (2019). <i>Neural Networks for Babies</i>. Naperville, Illinois: Sourcebooks Jabberwocky. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4926-7120-6" title="Special:BookSources/978-1-4926-7120-6"><bdi>978-1-4926-7120-6</bdi></a>. <a href="/wiki/OCLC_(identifier)" class="mw-redirect" title="OCLC (identifier)">OCLC</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/oclc/1086346753">1086346753</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Neural+Networks+for+Babies&amp;rft.place=Naperville%2C+Illinois&amp;rft.pub=Sourcebooks+Jabberwocky&amp;rft.date=2019&amp;rft&#95;id=info%3Aoclcnum%2F1086346753&amp;rft.isbn=978-1-4926-7120-6&amp;rft.aulast=Ferrie&amp;rft.aufirst=C.&amp;rft.au=Kaiser%2C+S.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-67"><span class="mw-cite-backlink"><b><a href="#cite_ref-67">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFMatsakis2019" class="citation magazine cs1">Matsakis, Louise (8 May 2019). <a rel="nofollow" class="external text" href="https://www.wired.com/story/adversarial-examples-ai-may-not-hallucinate/">"Artificial Intelligence May Not 'Hallucinate' After All"</a>. <i>Wired</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230326022415/https://www.wired.com/story/adversarial-examples-ai-may-not-hallucinate/">Archived</a> from the original on 26 March 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">29 December</span> 2022</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft.atitle=Artificial+Intelligence+May+Not+%27Hallucinate%27+After+All&amp;rft.date=2019-05-08&amp;rft.aulast=Matsakis&amp;rft.aufirst=Louise&amp;rft&#95;id=https%3A%2F%2Fwww.wired.com%2Fstory%2Fadversarial-examples-ai-may-not-hallucinate%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-bugs-68"><span class="mw-cite-backlink">^ <a href="#cite_ref-bugs_68-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-bugs_68-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFGilmerHendrycks2019" class="citation journal cs1">Gilmer, Justin; Hendrycks, Dan (6 August 2019). <a rel="nofollow" class="external text" href="https://doi.org/10.23915%2Fdistill.00019.1">"A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Adversarial Example Researchers Need to Expand What is Meant by 'Robustness'<span class="cs1-kern-right"></span>"</a>. <i>Distill</i>. <b>4</b> (8). <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.23915%2Fdistill.00019.1">10.23915/distill.00019.1</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Distill&amp;rft.atitle=A+Discussion+of+%27Adversarial+Examples+Are+Not+Bugs%2C+They+Are+Features%27%3A+Adversarial+Example+Researchers+Need+to+Expand+What+is+Meant+by+%27Robustness%27&amp;rft.volume=4&amp;rft.issue=8&amp;rft.date=2019-08-06&amp;rft&#95;id=info%3Adoi%2F10.23915%2Fdistill.00019.1&amp;rft.aulast=Gilmer&amp;rft.aufirst=Justin&amp;rft.au=Hendrycks%2C+Dan&amp;rft&#95;id=https%3A%2F%2Fdoi.org%2F10.23915%252Fdistill.00019.1&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-69"><span class="mw-cite-backlink"><b><a href="#cite_ref-69">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFZhangZhangZhengZhang2023" class="citation arxiv cs1">Zhang, Chenshuang; Zhang, Chaoning; Zheng, Sheng; Zhang, Mengchun; Qamar, Maryam; Bae, Sung-Ho; Kweon, In So (2 April 2023). "A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2303.13336">2303.13336</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.SD">cs.SD</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=A+Survey+on+Audio+Diffusion+Models%3A+Text+To+Speech+Synthesis+and+Enhancement+in+Generative+AI&amp;rft.date=2023-04-02&amp;rft&#95;id=info%3Aarxiv%2F2303.13336&amp;rft.aulast=Zhang&amp;rft.aufirst=Chenshuang&amp;rft.au=Zhang%2C+Chaoning&amp;rft.au=Zheng%2C+Sheng&amp;rft.au=Zhang%2C+Mengchun&amp;rft.au=Qamar%2C+Maryam&amp;rft.au=Bae%2C+Sung-Ho&amp;rft.au=Kweon%2C+In+So&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-70"><span class="mw-cite-backlink"><b><a href="#cite_ref-70">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFRobertson2024" class="citation news cs1">Robertson, Adi (21 February 2024). <a rel="nofollow" class="external text" href="https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical">"Google apologizes for "missing the mark" after Gemini generated racially diverse Nazis"</a>. <i>The Verge</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20240221232308/https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical">Archived</a> from the original on 21 February 2024<span class="reference-accessdate">. Retrieved <span class="nowrap">14 August</span> 2024</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Verge&amp;rft.atitle=Google+apologizes+for+%22missing+the+mark%22+after+Gemini+generated+racially+diverse+Nazis&amp;rft.date=2024-02-21&amp;rft.aulast=Robertson&amp;rft.aufirst=Adi&amp;rft&#95;id=https%3A%2F%2Fwww.theverge.com%2F2024%2F2%2F21%2F24079371%2Fgoogle-ai-gemini-generative-inaccurate-historical&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-71"><span class="mw-cite-backlink"><b><a href="#cite_ref-71">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://blog.google/products/gemini/gemini-image-generation-issue/">"Gemini image generation got it wrong. We'll do better"</a>. <i>Google</i>. 23 February 2024. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20240421033917/https://blog.google/products/gemini/gemini-image-generation-issue/">Archived</a> from the original on 21 April 2024<span class="reference-accessdate">. Retrieved <span class="nowrap">14 August</span> 2024</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Google&amp;rft.atitle=Gemini+image+generation+got+it+wrong.+We%27ll+do+better.&amp;rft.date=2024-02-23&amp;rft&#95;id=https%3A%2F%2Fblog.google%2Fproducts%2Fgemini%2Fgemini-image-generation-issue%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-72"><span class="mw-cite-backlink"><b><a href="#cite_ref-72">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFLuther2025" class="citation journal cs1">Luther, Kurt (2025). <a rel="nofollow" class="external text" href="https://www.jstor.org/stable/27404321">"A Guide to Exploring Photo Sleuthing and Generative AI"</a>. <i>Military Images</i>. <b>43</b> (4 (234)): <span class="nowrap">8–</span>11. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/1040-4961">1040-4961</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Military+Images&amp;rft.atitle=A+Guide+to+Exploring+Photo+Sleuthing+and+Generative+AI&amp;rft.volume=43&amp;rft.issue=4+%28234%29&amp;rft.pages=8-11&amp;rft.date=2025&amp;rft.issn=1040-4961&amp;rft.aulast=Luther&amp;rft.aufirst=Kurt&amp;rft&#95;id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F27404321&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Athaluri2023-73"><span class="mw-cite-backlink"><b><a href="#cite_ref-Athaluri2023_73-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFAthaluriManthenaKesapragadaYarlagadda2023" class="citation journal cs1">Athaluri, Sai Anirudh; Manthena, Sandeep Varma; Kesapragada, V S R Krishna Manoj; Yarlagadda, Vineel; Dave, Tirth; Duddumpudi, Rama Tulasi Siri (11 April 2023). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10173677">"Exploring the Boundaries of Reality: Investigating the Phenomenon of Artificial Intelligence Hallucination in Scientific Writing Through ChatGPT References"</a>. <i>Cureus</i>. <b>15</b> (4) e37432. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.7759%2Fcureus.37432">10.7759/cureus.37432</a></span>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10173677">10173677</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/37182055">37182055</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cureus&amp;rft.atitle=Exploring+the+Boundaries+of+Reality%3A+Investigating+the+Phenomenon+of+Artificial+Intelligence+Hallucination+in+Scientific+Writing+Through+ChatGPT+References&amp;rft.volume=15&amp;rft.issue=4&amp;rft.artnum=e37432&amp;rft.date=2023-04-11&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC10173677%23id-name%3DPMC&amp;rft&#95;id=info%3Apmid%2F37182055&amp;rft&#95;id=info%3Adoi%2F10.7759%2Fcureus.37432&amp;rft.aulast=Athaluri&amp;rft.aufirst=Sai+Anirudh&amp;rft.au=Manthena%2C+Sandeep+Varma&amp;rft.au=Kesapragada%2C+V+S+R+Krishna+Manoj&amp;rft.au=Yarlagadda%2C+Vineel&amp;rft.au=Dave%2C+Tirth&amp;rft.au=Duddumpudi%2C+Rama+Tulasi+Siri&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC10173677&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-veg-74"><span class="mw-cite-backlink"><b><a href="#cite_ref-veg_74-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFSnoswellWitzenbergerMasri2025" class="citation news cs1">Snoswell, Aaron J.; Witzenberger, Kevin; Masri, Rayane El (15 April 2025). <a rel="nofollow" class="external text" href="https://theconversation.com/a-weird-phrase-is-plaguing-scientific-papers-and-we-traced-it-back-to-a-glitch-in-ai-training-data-254463">"A weird phrase is plaguing scientific papers – and we traced it back to a glitch in AI training data"</a>. <i>The Conversation</i>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Conversation&amp;rft.atitle=A+weird+phrase+is+plaguing+scientific+papers+%E2%80%93+and+we+traced+it+back+to+a+glitch+in+AI+training+data&amp;rft.date=2025-04-15&amp;rft.aulast=Snoswell&amp;rft.aufirst=Aaron+J.&amp;rft.au=Witzenberger%2C+Kevin&amp;rft.au=Masri%2C+Rayane+El&amp;rft&#95;id=https%3A%2F%2Ftheconversation.com%2Fa-weird-phrase-is-plaguing-scientific-papers-and-we-traced-it-back-to-a-glitch-in-ai-training-data-254463&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Goddard2023-75"><span class="mw-cite-backlink">^ <a href="#cite_ref-Goddard2023_75-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Goddard2023_75-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFGoddard2023" class="citation journal cs1">Goddard, Jerome (November 2023). "Hallucinations in ChatGPT: A Cautionary Tale for Biomedical Researchers". <i>The American Journal of Medicine</i>. <b>136</b> (11): <span class="nowrap">1059–</span>1060. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.amjmed.2023.06.012">10.1016/j.amjmed.2023.06.012</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/37369274">37369274</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+American+Journal+of+Medicine&amp;rft.atitle=Hallucinations+in+ChatGPT%3A+A+Cautionary+Tale+for+Biomedical+Researchers&amp;rft.volume=136&amp;rft.issue=11&amp;rft.pages=1059-1060&amp;rft.date=2023-11&amp;rft&#95;id=info%3Adoi%2F10.1016%2Fj.amjmed.2023.06.012&amp;rft&#95;id=info%3Apmid%2F37369274&amp;rft.aulast=Goddard&amp;rft.aufirst=Jerome&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-76"><span class="mw-cite-backlink"><b><a href="#cite_ref-76">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFJiYuXuLee2023" class="citation book cs1">Ji, Ziwei; Yu, Tiezheng; Xu, Yan; Lee, Nayeon; Ishii, Etsuko; Fung, Pascale (2023). "Towards Mitigating LLM Hallucination via Self Reflection". <i>Findings of the Association for Computational Linguistics: EMNLP 2023</i>. pp.&#160;<span class="nowrap">1827–</span>1843. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.18653%2Fv1%2F2023.findings-emnlp.123">10.18653/v1/2023.findings-emnlp.123</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Towards+Mitigating+LLM+Hallucination+via+Self+Reflection&amp;rft.btitle=Findings+of+the+Association+for+Computational+Linguistics%3A+EMNLP+2023&amp;rft.pages=1827-1843&amp;rft.date=2023&amp;rft&#95;id=info%3Adoi%2F10.18653%2Fv1%2F2023.findings-emnlp.123&amp;rft.aulast=Ji&amp;rft.aufirst=Ziwei&amp;rft.au=Yu%2C+Tiezheng&amp;rft.au=Xu%2C+Yan&amp;rft.au=Lee%2C+Nayeon&amp;rft.au=Ishii%2C+Etsuko&amp;rft.au=Fung%2C+Pascale&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-77"><span class="mw-cite-backlink"><b><a href="#cite_ref-77">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFSchick2023" class="citation journal cs1">Schick, Nina (2023). <a rel="nofollow" class="external text" href="https://www.jstor.org/stable/48737215">"FAKING IT: Navigating the new era of generative AI may be the most critical challenge to democracy yet"</a>. <i>RSA Journal</i>. <b>169</b> (2(5593)): <span class="nowrap">40–</span>43. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/0958-0433">0958-0433</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=RSA+Journal&amp;rft.atitle=FAKING+IT%3A+Navigating+the+new+era+of+generative+AI+may+be+the+most+critical+challenge+to+democracy+yet&amp;rft.volume=169&amp;rft.issue=2%285593%29&amp;rft.pages=40-43&amp;rft.date=2023&amp;rft.issn=0958-0433&amp;rft.aulast=Schick&amp;rft.aufirst=Nina&amp;rft&#95;id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F48737215&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-78"><span class="mw-cite-backlink"><b><a href="#cite_ref-78">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFBhattacharyyaMillerBhattacharyyaMiller2023" class="citation journal cs1">Bhattacharyya, Mehul; Miller, Valerie M; Bhattacharyya, Debjani; Miller, Larry E (19 May 2023). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10277170">"High Rates of Fabricated and Inaccurate References in ChatGPT-Generated Medical Content"</a>. <i>Cureus</i>. <b>15</b> (5) e39238. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.7759%2Fcureus.39238">10.7759/cureus.39238</a></span>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10277170">10277170</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/37337480">37337480</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cureus&amp;rft.atitle=High+Rates+of+Fabricated+and+Inaccurate+References+in+ChatGPT-Generated+Medical+Content&amp;rft.volume=15&amp;rft.issue=5&amp;rft.artnum=e39238&amp;rft.date=2023-05-19&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC10277170%23id-name%3DPMC&amp;rft&#95;id=info%3Apmid%2F37337480&amp;rft&#95;id=info%3Adoi%2F10.7759%2Fcureus.39238&amp;rft.aulast=Bhattacharyya&amp;rft.aufirst=Mehul&amp;rft.au=Miller%2C+Valerie+M&amp;rft.au=Bhattacharyya%2C+Debjani&amp;rft.au=Miller%2C+Larry+E&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC10277170&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-79"><span class="mw-cite-backlink"><b><a href="#cite_ref-79">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFElse2023" class="citation journal cs1">Else, Holly (19 January 2023). "Abstracts written by ChatGPT fool scientists". <i>Nature</i>. <b>613</b> (7944): 423. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2023Natur.613..423E">2023Natur.613..423E</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fd41586-023-00056-7">10.1038/d41586-023-00056-7</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/36635510">36635510</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Abstracts+written+by+ChatGPT+fool+scientists&amp;rft.volume=613&amp;rft.issue=7944&amp;rft.pages=423&amp;rft.date=2023-01-19&amp;rft&#95;id=info%3Apmid%2F36635510&amp;rft&#95;id=info%3Adoi%2F10.1038%2Fd41586-023-00056-7&amp;rft&#95;id=info%3Abibcode%2F2023Natur.613..423E&amp;rft.aulast=Else&amp;rft.aufirst=Holly&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-80"><span class="mw-cite-backlink"><b><a href="#cite_ref-80">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFGaoHowardMarkovDyer2023" class="citation journal cs1">Gao, Catherine A.; Howard, Frederick M.; Markov, Nikolay S.; Dyer, Emma C.; Ramesh, Siddhi; Luo, Yuan; Pearson, Alexander T. (26 April 2023). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10133283">"Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers"</a>. <i>npj Digital Medicine</i>. <b>6</b> (1): 75. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fs41746-023-00819-6">10.1038/s41746-023-00819-6</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10133283">10133283</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/37100871">37100871</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=npj+Digital+Medicine&amp;rft.atitle=Comparing+scientific+abstracts+generated+by+ChatGPT+to+real+abstracts+with+detectors+and+blinded+human+reviewers&amp;rft.volume=6&amp;rft.issue=1&amp;rft.pages=75&amp;rft.date=2023-04-26&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC10133283%23id-name%3DPMC&amp;rft&#95;id=info%3Apmid%2F37100871&amp;rft&#95;id=info%3Adoi%2F10.1038%2Fs41746-023-00819-6&amp;rft.aulast=Gao&amp;rft.aufirst=Catherine+A.&amp;rft.au=Howard%2C+Frederick+M.&amp;rft.au=Markov%2C+Nikolay+S.&amp;rft.au=Dyer%2C+Emma+C.&amp;rft.au=Ramesh%2C+Siddhi&amp;rft.au=Luo%2C+Yuan&amp;rft.au=Pearson%2C+Alexander+T.&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC10133283&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-81"><span class="mw-cite-backlink"><b><a href="#cite_ref-81">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFEmsley2023" class="citation journal cs1">Emsley, Robin (19 August 2023). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10439949">"ChatGPT: these are not hallucinations – they're fabrications and falsifications"</a>. <i>Schizophrenia</i>. <b>9</b> (1) 52. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fs41537-023-00379-4">10.1038/s41537-023-00379-4</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10439949">10439949</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/37598184">37598184</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Schizophrenia&amp;rft.atitle=ChatGPT%3A+these+are+not+hallucinations+%E2%80%93+they%27re+fabrications+and+falsifications&amp;rft.volume=9&amp;rft.issue=1&amp;rft.artnum=52&amp;rft.date=2023-08-19&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC10439949%23id-name%3DPMC&amp;rft&#95;id=info%3Apmid%2F37598184&amp;rft&#95;id=info%3Adoi%2F10.1038%2Fs41537-023-00379-4&amp;rft.aulast=Emsley&amp;rft.aufirst=Robin&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC10439949&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-82"><span class="mw-cite-backlink"><b><a href="#cite_ref-82">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFGiray2024" class="citation journal cs1">Giray, Louie (2 January 2024). <span class="id-lock-subscription" title="Paid subscription required"><a rel="nofollow" class="external text" href="https://www.tandfonline.com/doi/full/10.1080/10875301.2023.2265369">"ChatGPT References Unveiled: Distinguishing the Reliable from the Fake"</a></span>. <i>Internet Reference Services Quarterly</i>. <b>28</b> (1): <span class="nowrap">9–</span>18. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F10875301.2023.2265369">10.1080/10875301.2023.2265369</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/1087-5301">1087-5301</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Internet+Reference+Services+Quarterly&amp;rft.atitle=ChatGPT+References+Unveiled%3A+Distinguishing+the+Reliable+from+the+Fake&amp;rft.volume=28&amp;rft.issue=1&amp;rft.pages=9-18&amp;rft.date=2024-01-02&amp;rft&#95;id=info%3Adoi%2F10.1080%2F10875301.2023.2265369&amp;rft.issn=1087-5301&amp;rft.aulast=Giray&amp;rft.aufirst=Louie&amp;rft&#95;id=https%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Ffull%2F10.1080%2F10875301.2023.2265369&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-83"><span class="mw-cite-backlink"><b><a href="#cite_ref-83">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFTeelWangLund2023" class="citation journal cs1">Teel, Zoë (Abbie); Wang, Ting; Lund, Brady (2023). <a rel="nofollow" class="external text" href="https://crln.acrl.org/index.php/crlnews/article/view/25931">"ChatGPT conundrums: Probing plagiarism and parroting problems in higher education practices"</a>. <i>College &amp; Research Libraries News</i>. <b>84</b> (6). <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.5860%2Fcrln.84.6.205">10.5860/crln.84.6.205</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=College+%26+Research+Libraries+News&amp;rft.atitle=ChatGPT+conundrums%3A+Probing+plagiarism+and+parroting+problems+in+higher+education+practices&amp;rft.volume=84&amp;rft.issue=6&amp;rft.date=2023&amp;rft&#95;id=info%3Adoi%2F10.5860%2Fcrln.84.6.205&amp;rft.aulast=Teel&amp;rft.aufirst=Zo%C3%AB+%28Abbie%29&amp;rft.au=Wang%2C+Ting&amp;rft.au=Lund%2C+Brady&amp;rft&#95;id=https%3A%2F%2Fcrln.acrl.org%2Findex.php%2Fcrlnews%2Farticle%2Fview%2F25931&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-84"><span class="mw-cite-backlink"><b><a href="#cite_ref-84">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.edsurge.com/news/2020-02-12-are-algorithmically-generated-term-papers-the-next-big-challenge-to-academic-integrity">"Are Algorithmically-Generated Term Papers the Next Big Challenge to Academic Integrity? - EdSurge News"</a>. <i>EdSurge</i>. 12 February 2020<span class="reference-accessdate">. Retrieved <span class="nowrap">5 November</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=EdSurge&amp;rft.atitle=Are+Algorithmically-Generated+Term+Papers+the+Next+Big+Challenge+to+Academic+Integrity%3F+-+EdSurge+News&amp;rft.date=2020-02-12&amp;rft&#95;id=https%3A%2F%2Fwww.edsurge.com%2Fnews%2F2020-02-12-are-algorithmically-generated-term-papers-the-next-big-challenge-to-academic-integrity&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-85"><span class="mw-cite-backlink"><b><a href="#cite_ref-85">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFWatson2024" class="citation journal cs1">Watson, Alex P. (3 July 2024). <a rel="nofollow" class="external text" href="https://www.tandfonline.com/doi/full/10.1080/0361526X.2024.2433640">"Hallucinated Citation Analysis: Delving into Student-Submitted AI-Generated Sources at the University of Mississippi"</a>. <i>The Serials Librarian</i>. <b>85</b> (<span class="nowrap">5–</span>6): <span class="nowrap">172–</span>180. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F0361526X.2024.2433640">10.1080/0361526X.2024.2433640</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/0361-526X">0361-526X</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Serials+Librarian&amp;rft.atitle=Hallucinated+Citation+Analysis%3A+Delving+into+Student-Submitted+AI-Generated+Sources+at+the+University+of+Mississippi&amp;rft.volume=85&amp;rft.issue=%3Cspan+class%3D%22nowrap%22%3E5%E2%80%93%3C%2Fspan%3E6&amp;rft.pages=172-180&amp;rft.date=2024-07-03&amp;rft&#95;id=info%3Adoi%2F10.1080%2F0361526X.2024.2433640&amp;rft.issn=0361-526X&amp;rft.aulast=Watson&amp;rft.aufirst=Alex+P.&amp;rft&#95;id=https%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Ffull%2F10.1080%2F0361526X.2024.2433640&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-86"><span class="mw-cite-backlink"><b><a href="#cite_ref-86">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFJainNimonkarJadhav2025" class="citation journal cs1">Jain, Anuj; Nimonkar, Pranali; Jadhav, Pratap (October 2025). <a rel="nofollow" class="external text" href="https://linkinghub.elsevier.com/retrieve/pii/S101051822500263X">"Citation integrity in the age of AI: evaluating the risks of reference hallucination in maxillofacial literature"</a>. <i>Journal of Cranio-Maxillofacial Surgery</i>. <b>53</b> (10): <span class="nowrap">1871–</span>1872. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.jcms.2025.08.004">10.1016/j.jcms.2025.08.004</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Cranio-Maxillofacial+Surgery&amp;rft.atitle=Citation+integrity+in+the+age+of+AI%3A+evaluating+the+risks+of+reference+hallucination+in+maxillofacial+literature&amp;rft.volume=53&amp;rft.issue=10&amp;rft.pages=1871-1872&amp;rft.date=2025-10&amp;rft&#95;id=info%3Adoi%2F10.1016%2Fj.jcms.2025.08.004&amp;rft.aulast=Jain&amp;rft.aufirst=Anuj&amp;rft.au=Nimonkar%2C+Pranali&amp;rft.au=Jadhav%2C+Pratap&amp;rft&#95;id=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS101051822500263X&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-87"><span class="mw-cite-backlink"><b><a href="#cite_ref-87">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://mitsloanedtech.mit.edu/ai/teach/ai-detectors-dont-work/">"AI Detectors Don't Work. Here's What to Do Instead"</a>. <i>MIT Sloan Teaching &amp; Learning Technologies</i><span class="reference-accessdate">. Retrieved <span class="nowrap">5 November</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=MIT+Sloan+Teaching+%26+Learning+Technologies&amp;rft.atitle=AI+Detectors+Don%27t+Work.+Here%27s+What+to+Do+Instead.&amp;rft&#95;id=https%3A%2F%2Fmitsloanedtech.mit.edu%2Fai%2Fteach%2Fai-detectors-dont-work%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-88"><span class="mw-cite-backlink"><b><a href="#cite_ref-88">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://openai.com/index/new-ai-classifier-for-indicating-ai-written-text/">"New AI classifier for indicating AI-written text"</a>. <i>openai.com</i>. 13 March 2024<span class="reference-accessdate">. Retrieved <span class="nowrap">5 November</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=openai.com&amp;rft.atitle=New+AI+classifier+for+indicating+AI-written+text&amp;rft.date=2024-03-13&amp;rft&#95;id=https%3A%2F%2Fopenai.com%2Findex%2Fnew-ai-classifier-for-indicating-ai-written-text%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-89"><span class="mw-cite-backlink"><b><a href="#cite_ref-89">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFXuJainKankanhalli2024" class="citation arxiv cs1">Xu, Ziwei; Jain, Sanjay; Kankanhalli, Mohan (2024). "Hallucination is Inevitable: An Innate Limitation of Large Language Models". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2401.11817">2401.11817</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Hallucination+is+Inevitable%3A+An+Innate+Limitation+of+Large+Language+Models&amp;rft.date=2024&amp;rft&#95;id=info%3Aarxiv%2F2401.11817&amp;rft.aulast=Xu&amp;rft.aufirst=Ziwei&amp;rft.au=Jain%2C+Sanjay&amp;rft.au=Kankanhalli%2C+Mohan&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-90"><span class="mw-cite-backlink"><b><a href="#cite_ref-90">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFNieYaoWangPan2019" class="citation book cs1">Nie, Feng; Yao, Jin-Ge; Wang, Jinpeng; Pan, Rong; Lin, Chin-Yew (2019). "A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation". <i>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</i>. pp.&#160;<span class="nowrap">2673–</span>2679. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.18653%2Fv1%2FP19-1256">10.18653/v1/P19-1256</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=A+Simple+Recipe+towards+Reducing+Hallucination+in+Neural+Surface+Realisation&amp;rft.btitle=Proceedings+of+the+57th+Annual+Meeting+of+the+Association+for+Computational+Linguistics&amp;rft.pages=2673-2679&amp;rft.date=2019&amp;rft&#95;id=info%3Adoi%2F10.18653%2Fv1%2FP19-1256&amp;rft.aulast=Nie&amp;rft.aufirst=Feng&amp;rft.au=Yao%2C+Jin-Ge&amp;rft.au=Wang%2C+Jinpeng&amp;rft.au=Pan%2C+Rong&amp;rft.au=Lin%2C+Chin-Yew&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-91"><span class="mw-cite-backlink"><b><a href="#cite_ref-91">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFDziriMiltonYuZaiane2022" class="citation book cs1">Dziri, Nouha; Milton, Sivan; Yu, Mo; Zaiane, Osmar; Reddy, Siva (2022). "On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?". <i>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</i>. pp.&#160;<span class="nowrap">5271–</span>5285. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.18653%2Fv1%2F2022.naacl-main.387">10.18653/v1/2022.naacl-main.387</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=On+the+Origin+of+Hallucinations+in+Conversational+Models%3A+Is+it+the+Datasets+or+the+Models%3F&amp;rft.btitle=Proceedings+of+the+2022+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics%3A+Human+Language+Technologies&amp;rft.pages=5271-5285&amp;rft.date=2022&amp;rft&#95;id=info%3Adoi%2F10.18653%2Fv1%2F2022.naacl-main.387&amp;rft.aulast=Dziri&amp;rft.aufirst=Nouha&amp;rft.au=Milton%2C+Sivan&amp;rft.au=Yu%2C+Mo&amp;rft.au=Zaiane%2C+Osmar&amp;rft.au=Reddy%2C+Siva&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-92"><span class="mw-cite-backlink"><b><a href="#cite_ref-92">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFVynck2023" class="citation news cs1">Vynck, Gerrit De (30 May 2023). <a rel="nofollow" class="external text" href="https://www.washingtonpost.com/technology/2023/05/30/ai-chatbots-chatgpt-bard-trustworthy/">"ChatGPT 'hallucinates.' Some researchers worry it isn't fixable"</a>. <i>Washington Post</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230617021157/https://www.washingtonpost.com/technology/2023/05/30/ai-chatbots-chatgpt-bard-trustworthy/">Archived</a> from the original on 17 June 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">31 May</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Washington+Post&amp;rft.atitle=ChatGPT+%27hallucinates.%27+Some+researchers+worry+it+isn%27t+fixable.&amp;rft.date=2023-05-30&amp;rft.aulast=Vynck&amp;rft.aufirst=Gerrit+De&amp;rft&#95;id=https%3A%2F%2Fwww.washingtonpost.com%2Ftechnology%2F2023%2F05%2F30%2Fai-chatbots-chatgpt-bard-trustworthy%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-93"><span class="mw-cite-backlink"><b><a href="#cite_ref-93">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFVarshneyYaoZhangChen2023" class="citation arxiv cs1">Varshney, Neeraj; Yao, Wenling; Zhang, Hongming; Chen, Jianshu; Yu, Dong (2023). "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2307.03987">2307.03987</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=A+Stitch+in+Time+Saves+Nine%3A+Detecting+and+Mitigating+Hallucinations+of+LLMs+by+Validating+Low-Confidence+Generation&amp;rft.date=2023&amp;rft&#95;id=info%3Aarxiv%2F2307.03987&amp;rft.aulast=Varshney&amp;rft.aufirst=Neeraj&amp;rft.au=Yao%2C+Wenling&amp;rft.au=Zhang%2C+Hongming&amp;rft.au=Chen%2C+Jianshu&amp;rft.au=Yu%2C+Dong&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-94"><span class="mw-cite-backlink"><b><a href="#cite_ref-94">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFŠekrst" class="citation encyclopaedia cs1">Šekrst, Kristina. <a rel="nofollow" class="external text" href="https://philarchive.org/archive/EKRUUQ">"Unjustified untrue "beliefs": AI hallucinations and justification logics"</a>. In Grgić, Filip; Świętorzecka, Kordula; Brożek, Anna (eds.). <i>Logic, Knowledge, and Tradition: Essays in Honor of Srecko Kovač</i><span class="reference-accessdate">. Retrieved <span class="nowrap">4 June</span> 2024</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Unjustified+untrue+%22beliefs%22%3A+AI+hallucinations+and+justification+logics&amp;rft.btitle=Logic%2C+Knowledge%2C+and+Tradition%3A+Essays+in+Honor+of+Srecko+Kova%C4%8D&amp;rft.aulast=%C5%A0ekrst&amp;rft.aufirst=Kristina&amp;rft&#95;id=https%3A%2F%2Fphilarchive.org%2Farchive%2FEKRUUQ&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Chen-2024-95"><span class="mw-cite-backlink"><b><a href="#cite_ref-Chen-2024_95-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFChenMueller2024" class="citation arxiv cs1">Chen, Jiuhai; Mueller, Jonas (2024). "Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2308.16175">2308.16175</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Quantifying+Uncertainty+in+Answers+from+any+Language+Model+and+Enhancing+their+Trustworthiness&amp;rft.date=2024&amp;rft&#95;id=info%3Aarxiv%2F2308.16175&amp;rft.aulast=Chen&amp;rft.aufirst=Jiuhai&amp;rft.au=Mueller%2C+Jonas&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-96"><span class="mw-cite-backlink"><b><a href="#cite_ref-96">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFJiangJiangChuGulati2024" class="citation conference cs1">Jiang, Ling; Jiang, Keer; Chu, Xiaoyu; Gulati, Saaransh; Garg, Pulkit (2024). <a rel="nofollow" class="external text" href="https://aclanthology.org/2024.ecnlp-1.4/">"Hallucination Detection in LLM-enriched Product Listings"</a>. <i>Proceedings of the Seventh Workshop on e-Commerce and NLP (ECNLP 2024)</i>. Association for Computational Linguistics. pp.&#160;<span class="nowrap">38–</span>48<span class="reference-accessdate">. Retrieved <span class="nowrap">5 October</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Hallucination+Detection+in+LLM-enriched+Product+Listings&amp;rft.btitle=Proceedings+of+the+Seventh+Workshop+on+e-Commerce+and+NLP+%28ECNLP+2024%29&amp;rft.pages=38-48&amp;rft.pub=Association+for+Computational+Linguistics&amp;rft.date=2024&amp;rft.aulast=Jiang&amp;rft.aufirst=Ling&amp;rft.au=Jiang%2C+Keer&amp;rft.au=Chu%2C+Xiaoyu&amp;rft.au=Gulati%2C+Saaransh&amp;rft.au=Garg%2C+Pulkit&amp;rft&#95;id=https%3A%2F%2Faclanthology.org%2F2024.ecnlp-1.4%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Luo-2024-97"><span class="mw-cite-backlink">^ <a href="#cite_ref-Luo-2024_97-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Luo-2024_97-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFLuoLiWuJenkin2024" class="citation arxiv cs1">Luo, Junliang; Li, Tianyu; Wu, Di; Jenkin, Michael; Liu, Steve; Dudek, Gregory (2024). "Hallucination Detection and Hallucination Mitigation: An Investigation". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2401.08358">2401.08358</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Hallucination+Detection+and+Hallucination+Mitigation%3A+An+Investigation&amp;rft.date=2024&amp;rft&#95;id=info%3Aarxiv%2F2401.08358&amp;rft.aulast=Luo&amp;rft.aufirst=Junliang&amp;rft.au=Li%2C+Tianyu&amp;rft.au=Wu%2C+Di&amp;rft.au=Jenkin%2C+Michael&amp;rft.au=Liu%2C+Steve&amp;rft.au=Dudek%2C+Gregory&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-98"><span class="mw-cite-backlink"><b><a href="#cite_ref-98">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFDziriMadottoZaianeBose2021" class="citation arxiv cs1">Dziri, Nouha; Madotto, Andrea; Zaiane, Osmar; Bose, Avishek Joey (2021). "Neural path hunter: Reducing hallucination in dialogue systems via path grounding". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2104.08455">2104.08455</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Neural+path+hunter%3A+Reducing+hallucination+in+dialogue+systems+via+path+grounding&amp;rft.date=2021&amp;rft&#95;id=info%3Aarxiv%2F2104.08455&amp;rft.aulast=Dziri&amp;rft.aufirst=Nouha&amp;rft.au=Madotto%2C+Andrea&amp;rft.au=Zaiane%2C+Osmar&amp;rft.au=Bose%2C+Avishek+Joey&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-99"><span class="mw-cite-backlink"><b><a href="#cite_ref-99">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFRashkinReitterTomarDas2021" class="citation book cs1">Rashkin, Hannah; Reitter, David; Tomar, Gaurav Singh; Das, Dipanjan (2021). "Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features". <i>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</i>. pp.&#160;<span class="nowrap">704–</span>718. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.18653%2Fv1%2F2021.acl-long.58">10.18653/v1/2021.acl-long.58</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Increasing+Faithfulness+in+Knowledge-Grounded+Dialogue+with+Controllable+Features&amp;rft.btitle=Proceedings+of+the+59th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+and+the+11th+International+Joint+Conference+on+Natural+Language+Processing+%28Volume+1%3A+Long+Papers%29&amp;rft.pages=704-718&amp;rft.date=2021&amp;rft&#95;id=info%3Adoi%2F10.18653%2Fv1%2F2021.acl-long.58&amp;rft.aulast=Rashkin&amp;rft.aufirst=Hannah&amp;rft.au=Reitter%2C+David&amp;rft.au=Tomar%2C+Gaurav+Singh&amp;rft.au=Das%2C+Dipanjan&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-100"><span class="mw-cite-backlink"><b><a href="#cite_ref-100">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFSunShiGaoRen2022" class="citation arxiv cs1">Sun, Weiwei; Shi, Zhengliang; Gao, Shen; Ren, Pengjie; de Rijke, Maarten; Ren, Zhaochun (2022). "Contrastive Learning Reduces Hallucination in Conversations". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2212.10400">2212.10400</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Contrastive+Learning+Reduces+Hallucination+in+Conversations&amp;rft.date=2022&amp;rft&#95;id=info%3Aarxiv%2F2212.10400&amp;rft.aulast=Sun&amp;rft.aufirst=Weiwei&amp;rft.au=Shi%2C+Zhengliang&amp;rft.au=Gao%2C+Shen&amp;rft.au=Ren%2C+Pengjie&amp;rft.au=de+Rijke%2C+Maarten&amp;rft.au=Ren%2C+Zhaochun&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-101"><span class="mw-cite-backlink"><b><a href="#cite_ref-101">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFZhaoCohenWebber2020" class="citation book cs1">Zhao, Zheng; Cohen, Shay B.; Webber, Bonnie (2020). "Reducing Quantity Hallucinations in Abstractive Summarization". <i>Findings of the Association for Computational Linguistics: EMNLP 2020</i>. pp.&#160;<span class="nowrap">2237–</span>2249. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2009.13312">2009.13312</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.18653%2Fv1%2F2020.findings-emnlp.203">10.18653/v1/2020.findings-emnlp.203</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Reducing+Quantity+Hallucinations+in+Abstractive+Summarization&amp;rft.btitle=Findings+of+the+Association+for+Computational+Linguistics%3A+EMNLP+2020&amp;rft.pages=2237-2249&amp;rft.date=2020&amp;rft&#95;id=info%3Aarxiv%2F2009.13312&amp;rft&#95;id=info%3Adoi%2F10.18653%2Fv1%2F2020.findings-emnlp.203&amp;rft.aulast=Zhao&amp;rft.aufirst=Zheng&amp;rft.au=Cohen%2C+Shay+B.&amp;rft.au=Webber%2C+Bonnie&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-102"><span class="mw-cite-backlink"><b><a href="#cite_ref-102">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFMündlerHeJenkoVechev2023" class="citation arxiv cs1">Mündler, Niels; He, Jingxuan; Jenko, Slobodan; Vechev, Martin (2023). "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2305.15852">2305.15852</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Self-contradictory+Hallucinations+of+Large+Language+Models%3A+Evaluation%2C+Detection+and+Mitigation&amp;rft.date=2023&amp;rft&#95;id=info%3Aarxiv%2F2305.15852&amp;rft.aulast=M%C3%BCndler&amp;rft.aufirst=Niels&amp;rft.au=He%2C+Jingxuan&amp;rft.au=Jenko%2C+Slobodan&amp;rft.au=Vechev%2C+Martin&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-103"><span class="mw-cite-backlink"><b><a href="#cite_ref-103">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFLeswing2023" class="citation news cs1">Leswing, Kif (25 April 2023). <a rel="nofollow" class="external text" href="https://www.cnbc.com/2023/04/25/nvidia-nemo-guardrails-software-stops-ai-chatbots-from-hallucinating.html">"Nvidia has a new way to prevent A.I. chatbots from 'hallucinating' wrong facts"</a>. <i>CNBC</i><span class="reference-accessdate">. Retrieved <span class="nowrap">15 June</span> 2023</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=CNBC&amp;rft.atitle=Nvidia+has+a+new+way+to+prevent+A.I.+chatbots+from+%27hallucinating%27+wrong+facts&amp;rft.date=2023-04-25&amp;rft.aulast=Leswing&amp;rft.aufirst=Kif&amp;rft&#95;id=https%3A%2F%2Fwww.cnbc.com%2F2023%2F04%2F25%2Fnvidia-nemo-guardrails-software-stops-ai-chatbots-from-hallucinating.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-104"><span class="mw-cite-backlink"><b><a href="#cite_ref-104">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFPotsawee2024" class="citation web cs1">Potsawee (9 May 2024). <a rel="nofollow" class="external text" href="https://github.com/potsawee/selfcheckgpt">"potsawee/selfcheckgpt"</a>. <i><a href="/wiki/GitHub" title="GitHub">GitHub</a></i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20240509231835/https://github.com/potsawee/selfcheckgpt">Archived</a> from the original on 9 May 2024<span class="reference-accessdate">. Retrieved <span class="nowrap">9 May</span> 2024</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=GitHub&amp;rft.atitle=potsawee%2Fselfcheckgpt&amp;rft.date=2024-05-09&amp;rft.au=Potsawee&amp;rft&#95;id=https%3A%2F%2Fgithub.com%2Fpotsawee%2Fselfcheckgpt&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-105"><span class="mw-cite-backlink"><b><a href="#cite_ref-105">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.technologyreview.com/2024/04/25/1091835/chatbot-hallucination-new-tool-trustworthy-language-model/">"Chatbot answers are all made up. This new tool helps you figure out which ones to trust"</a>. MIT Technology Review. 25 April 2024. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20240426140708/https://www.technologyreview.com/2024/04/25/1091835/chatbot-hallucination-new-tool-trustworthy-language-model/">Archived</a> from the original on 26 April 2024<span class="reference-accessdate">. Retrieved <span class="nowrap">31 December</span> 2024</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Chatbot+answers+are+all+made+up.+This+new+tool+helps+you+figure+out+which+ones+to+trust.&amp;rft.pub=MIT+Technology+Review&amp;rft.date=2024-04-25&amp;rft&#95;id=https%3A%2F%2Fwww.technologyreview.com%2F2024%2F04%2F25%2F1091835%2Fchatbot-hallucination-new-tool-trustworthy-language-model%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-106"><span class="mw-cite-backlink"><b><a href="#cite_ref-106">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://aimon.ai/">"Aimon"</a>. aimonlabs. 8 May 2024. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20240508193601/https://aimon.ai/">Archived</a> from the original on 8 May 2024<span class="reference-accessdate">. Retrieved <span class="nowrap">9 May</span> 2024</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Aimon&amp;rft.pub=aimonlabs&amp;rft.date=2024-05-08&amp;rft&#95;id=https%3A%2F%2Faimon.ai%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-:0-107"><span class="mw-cite-backlink"><b><a href="#cite_ref-:0_107-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222" /><cite id="CITEREFXing2025" class="citation web cs1">Xing, Wei (12 September 2025). <a rel="nofollow" class="external text" href="https://theconversation.com/why-openais-solution-to-ai-hallucinations-would-kill-chatgpt-tomorrow-265107">"Why OpenAI's solution to AI hallucinations would kill ChatGPT tomorrow"</a>. <i>The Conversation</i><span class="reference-accessdate">. Retrieved <span class="nowrap">16 September</span> 2025</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Conversation&amp;rft.atitle=Why+OpenAI%27s+solution+to+AI+hallucinations+would+kill+ChatGPT+tomorrow&amp;rft.date=2025-09-12&amp;rft.aulast=Xing&amp;rft.aufirst=Wei&amp;rft&#95;id=http%3A%2F%2Ftheconversation.com%2Fwhy-openais-solution-to-ai-hallucinations-would-kill-chatgpt-tomorrow-265107&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AHallucination+%28artificial+intelligence%29" class="Z3988"></span></span>
</li>
</ol></div></div>
<div class="navbox-styles"><style data-mw-deduplicate="TemplateStyles:r1129693374">.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:": "}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:" · ";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:" (";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:")";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:" "counter(listitem)"\a0 "}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:" ("counter(listitem)"\a0 "}</style><style data-mw-deduplicate="TemplateStyles:r1314944253">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd;color:inherit}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf;color:inherit}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf;color:inherit}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff;color:inherit}.mw-parser-output .navbox-even{background-color:#f7f7f7;color:inherit}.mw-parser-output .navbox-odd{background-color:transparent;color:inherit}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}@media print{body.ns-0 .mw-parser-output .navbox{display:none!important}}</style></div><div role="navigation" class="navbox" aria-labelledby="Artificial&#95;intelligence&#95;(AI)7556" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374" /><style data-mw-deduplicate="TemplateStyles:r1239400231">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}</style><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Artificial_intelligence_navbox" title="Template:Artificial intelligence navbox"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Artificial_intelligence_navbox" title="Template talk:Artificial intelligence navbox"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Artificial_intelligence_navbox" title="Special:EditPage/Template:Artificial intelligence navbox"><abbr title="Edit this template">e</abbr></a></li></ul></div><div id="Artificial&#95;intelligence&#95;(AI)7556" style="font-size:114%;margin:0 4em"><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial intelligence</a> (AI)</div></th></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><a href="/wiki/History_of_artificial_intelligence" title="History of artificial intelligence">History</a>
<ul><li><a href="/wiki/Timeline_of_artificial_intelligence" title="Timeline of artificial intelligence">timeline</a></li></ul></li>
<li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary</a></li>
<li><a href="/wiki/List_of_artificial_intelligence_companies" title="List of artificial intelligence companies">Companies</a></li>
<li><a href="/wiki/List_of_artificial_intelligence_projects" title="List of artificial intelligence projects">Projects</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Parameter" title="Parameter">Parameter</a>
<ul><li><a href="/wiki/Hyperparameter_(machine_learning)" title="Hyperparameter (machine learning)">Hyperparameter</a></li></ul></li>
<li><a href="/wiki/Loss_functions_for_classification" title="Loss functions for classification">Loss functions</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a>
<ul><li><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Bias–variance tradeoff</a></li>
<li><a href="/wiki/Double_descent" title="Double descent">Double descent</a></li>
<li><a href="/wiki/Overfitting" title="Overfitting">Overfitting</a></li></ul></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Gradient_descent" title="Gradient descent">Gradient descent</a>
<ul><li><a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">SGD</a></li>
<li><a href="/wiki/Quasi-Newton_method" title="Quasi-Newton method">Quasi-Newton method</a></li>
<li><a href="/wiki/Conjugate_gradient_method" title="Conjugate gradient method">Conjugate gradient method</a></li></ul></li>
<li><a href="/wiki/Backpropagation" title="Backpropagation">Backpropagation</a></li>
<li><a href="/wiki/Attention_(machine_learning)" title="Attention (machine learning)">Attention</a></li>
<li><a href="/wiki/Convolution" title="Convolution">Convolution</a></li>
<li><a href="/wiki/Normalization_(machine_learning)" title="Normalization (machine learning)">Normalization</a>
<ul><li><a href="/wiki/Batch_normalization" title="Batch normalization">Batchnorm</a></li></ul></li>
<li><a href="/wiki/Activation_function" title="Activation function">Activation</a>
<ul><li><a href="/wiki/Softmax_function" title="Softmax function">Softmax</a></li>
<li><a href="/wiki/Sigmoid_function" title="Sigmoid function">Sigmoid</a></li>
<li><a href="/wiki/Rectifier_(neural_networks)" class="mw-redirect" title="Rectifier (neural networks)">Rectifier</a></li></ul></li>
<li><a href="/wiki/Gating_mechanism" title="Gating mechanism">Gating</a></li>
<li><a href="/wiki/Weight_initialization" title="Weight initialization">Weight initialization</a></li>
<li><a href="/wiki/Regularization_(mathematics)" title="Regularization (mathematics)">Regularization</a></li>
<li><a href="/wiki/Training,_validation,_and_test_data_sets" title="Training, validation, and test data sets">Datasets</a>
<ul><li><a href="/wiki/Data_augmentation" title="Data augmentation">Augmentation</a></li></ul></li>
<li><a href="/wiki/Prompt_engineering" title="Prompt engineering">Prompt engineering</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a>
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Imitation_learning" title="Imitation learning">Imitation</a></li>
<li><a href="/wiki/Policy_gradient_method" title="Policy gradient method">Policy gradient</a></li></ul></li>
<li><a href="/wiki/Diffusion_process" title="Diffusion process">Diffusion</a></li>
<li><a href="/wiki/Latent_diffusion_model" title="Latent diffusion model">Latent diffusion model</a></li>
<li><a href="/wiki/Autoregressive_model" title="Autoregressive model">Autoregression</a></li>
<li><a href="/wiki/Adversarial_machine_learning" title="Adversarial machine learning">Adversary</a></li>
<li><a href="/wiki/Retrieval-augmented_generation" title="Retrieval-augmented generation">RAG</a></li>
<li><a href="/wiki/Uncanny_valley" title="Uncanny valley">Uncanny valley</a></li>
<li><a href="/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">RLHF</a></li>
<li><a href="/wiki/Self-supervised_learning" title="Self-supervised learning">Self-supervised learning</a></li>
<li><a href="/wiki/Reflection_(artificial_intelligence)" class="mw-redirect" title="Reflection (artificial intelligence)">Reflection</a></li>
<li><a href="/wiki/Recursive_self-improvement" title="Recursive self-improvement">Recursive self-improvement</a></li>
<li><a class="mw-selflink selflink">Hallucination</a></li>
<li><a href="/wiki/Word_embedding" title="Word embedding">Word embedding</a></li>
<li><a href="/wiki/Vibe_coding" title="Vibe coding">Vibe coding</a></li>
<li><a href="/wiki/AI_safety" title="AI safety">Safety</a> (<a href="/wiki/AI_alignment" title="AI alignment">Alignment</a>)</li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Applications</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a>
<ul><li><a href="/wiki/Prompt_engineering#In-context_learning" title="Prompt engineering">In-context learning</a></li></ul></li>
<li><a href="/wiki/Neural_network_(machine_learning)" title="Neural network (machine learning)">Artificial neural network</a>
<ul><li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li></ul></li>
<li><a href="/wiki/Language_model" title="Language model">Language model</a>
<ul><li><a href="/wiki/Large_language_model" title="Large language model">Large</a></li>
<li><a href="/wiki/Neural_machine_translation" title="Neural machine translation">NMT</a></li>
<li><a href="/wiki/Reasoning_model" title="Reasoning model">Reasoning</a></li></ul></li>
<li><a href="/wiki/Model_Context_Protocol" title="Model Context Protocol">Model Context Protocol</a></li>
<li><a href="/wiki/Intelligent_agent" title="Intelligent agent">Intelligent agent</a></li>
<li><a href="/wiki/Artificial_human_companion" title="Artificial human companion">Artificial human companion</a></li>
<li><a href="/wiki/Humanity%27s_Last_Exam" title="Humanity&#39;s Last Exam">Humanity's Last Exam</a></li>
<li><a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">Artificial general intelligence (AGI)</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Implementations</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%">Audio–visual</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>
<li><a href="/wiki/WaveNet" title="WaveNet">WaveNet</a></li>
<li><a href="/wiki/Human_image_synthesis" title="Human image synthesis">Human image synthesis</a></li>
<li><a href="/wiki/Handwriting_recognition" title="Handwriting recognition">HWR</a></li>
<li><a href="/wiki/Optical_character_recognition" title="Optical character recognition">OCR</a></li>
<li><a href="/wiki/Computer_vision" title="Computer vision">Computer vision</a></li>
<li><a href="/wiki/Deep_learning_speech_synthesis" title="Deep learning speech synthesis">Speech synthesis</a>
<ul><li><a href="/wiki/15.ai" title="15.ai">15.ai</a></li>
<li><a href="/wiki/ElevenLabs" title="ElevenLabs">ElevenLabs</a></li></ul></li>
<li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a>
<ul><li><a href="/wiki/Whisper_(speech_recognition_system)" title="Whisper (speech recognition system)">Whisper</a></li></ul></li>
<li><a href="/wiki/Facial_recognition_system" title="Facial recognition system">Facial recognition</a></li>
<li><a href="/wiki/AlphaFold" title="AlphaFold">AlphaFold</a></li>
<li><a href="/wiki/Text-to-image_model" title="Text-to-image model">Text-to-image models</a>
<ul><li><a href="/wiki/Aurora_(text-to-image_model)" class="mw-redirect" title="Aurora (text-to-image model)">Aurora</a></li>
<li><a href="/wiki/DALL-E" title="DALL-E">DALL-E</a></li>
<li><a href="/wiki/Adobe_Firefly" title="Adobe Firefly">Firefly</a></li>
<li><a href="/wiki/Flux_(text-to-image_model)" title="Flux (text-to-image model)">Flux</a></li>
<li><a href="/wiki/Ideogram_(text-to-image_model)" title="Ideogram (text-to-image model)">Ideogram</a></li>
<li><a href="/wiki/Imagen_(text-to-image_model)" title="Imagen (text-to-image model)">Imagen</a></li>
<li><a href="/wiki/Midjourney" title="Midjourney">Midjourney</a></li>
<li><a href="/wiki/Recraft" title="Recraft">Recraft</a></li>
<li><a href="/wiki/Stable_Diffusion" title="Stable Diffusion">Stable Diffusion</a></li></ul></li>
<li><a href="/wiki/Text-to-video_model" title="Text-to-video model">Text-to-video models</a>
<ul><li><a href="/wiki/Dream_Machine_(text-to-video_model)" title="Dream Machine (text-to-video model)">Dream Machine</a></li>
<li><a href="/wiki/Runway_(company)#Services_and_technologies" title="Runway (company)">Runway Gen</a></li>
<li><a href="/wiki/MiniMax_(company)#Hailuo_AI" title="MiniMax (company)">Hailuo AI</a></li>
<li><a href="/wiki/Kling_AI_(company)" class="mw-redirect" title="Kling AI (company)">Kling</a></li>
<li><a href="/wiki/Sora_(text-to-video_model)" title="Sora (text-to-video model)">Sora</a></li>
<li><a href="/wiki/Veo_(text-to-video_model)" title="Veo (text-to-video model)">Veo</a></li></ul></li>
<li><a href="/wiki/Music_and_artificial_intelligence" title="Music and artificial intelligence">Music generation</a>
<ul><li><a href="/wiki/Riffusion" title="Riffusion">Riffusion</a></li>
<li><a href="/wiki/Suno_AI" class="mw-redirect" title="Suno AI">Suno AI</a></li>
<li><a href="/wiki/Udio" title="Udio">Udio</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Text</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Word2vec" title="Word2vec">Word2vec</a></li>
<li><a href="/wiki/Seq2seq" title="Seq2seq">Seq2seq</a></li>
<li><a href="/wiki/GloVe" title="GloVe">GloVe</a></li>
<li><a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a></li>
<li><a href="/wiki/T5_(language_model)" title="T5 (language model)">T5</a></li>
<li><a href="/wiki/Llama_(language_model)" title="Llama (language model)">Llama</a></li>
<li><a href="/wiki/Chinchilla_(language_model)" title="Chinchilla (language model)">Chinchilla AI</a></li>
<li><a href="/wiki/PaLM" title="PaLM">PaLM</a></li>
<li><a href="/wiki/Generative_pre-trained_transformer" title="Generative pre-trained transformer">GPT</a>
<ul><li><a href="/wiki/GPT-1" title="GPT-1">1</a></li>
<li><a href="/wiki/GPT-2" title="GPT-2">2</a></li>
<li><a href="/wiki/GPT-3" title="GPT-3">3</a></li>
<li><a href="/wiki/GPT-J" title="GPT-J">J</a></li>
<li><a href="/wiki/ChatGPT" title="ChatGPT">ChatGPT</a></li>
<li><a href="/wiki/GPT-4" title="GPT-4">4</a></li>
<li><a href="/wiki/GPT-4o" title="GPT-4o">4o</a></li>
<li><a href="/wiki/OpenAI_o1" title="OpenAI o1">o1</a></li>
<li><a href="/wiki/OpenAI_o3" title="OpenAI o3">o3</a></li>
<li><a href="/wiki/GPT-4.5" title="GPT-4.5">4.5</a></li>
<li><a href="/wiki/GPT-4.1" title="GPT-4.1">4.1</a></li>
<li><a href="/wiki/OpenAI_o4-mini" title="OpenAI o4-mini">o4-mini</a></li>
<li><a href="/wiki/GPT-5" title="GPT-5">5</a></li>
<li><a href="/wiki/GPT-5.1" title="GPT-5.1">5.1</a></li></ul></li>
<li><a href="/wiki/Claude_(language_model)" title="Claude (language model)">Claude</a></li>
<li><a href="/wiki/Gemini_(chatbot)" class="mw-redirect" title="Gemini (chatbot)">Gemini</a>
<ul><li><a href="/wiki/Gemini_(language_model)" title="Gemini (language model)">Gemini (language model)</a></li>
<li><a href="/wiki/Gemma_(language_model)" title="Gemma (language model)">Gemma</a></li></ul></li>
<li><a href="/wiki/Grok_(chatbot)" title="Grok (chatbot)">Grok</a></li>
<li><a href="/wiki/LaMDA" title="LaMDA">LaMDA</a></li>
<li><a href="/wiki/BLOOM_(language_model)" title="BLOOM (language model)">BLOOM</a></li>
<li><a href="/wiki/DBRX" title="DBRX">DBRX</a></li>
<li><a href="/wiki/Project_Debater" title="Project Debater">Project Debater</a></li>
<li><a href="/wiki/IBM_Watson" title="IBM Watson">IBM Watson</a></li>
<li><a href="/wiki/IBM_Watsonx" title="IBM Watsonx">IBM Watsonx</a></li>
<li><a href="/wiki/IBM_Granite" title="IBM Granite">Granite</a></li>
<li><a href="/wiki/Huawei_PanGu" title="Huawei PanGu">PanGu-Σ</a></li>
<li><a href="/wiki/DeepSeek_(chatbot)" title="DeepSeek (chatbot)">DeepSeek</a></li>
<li><a href="/wiki/Qwen" title="Qwen">Qwen</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Decisional</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlphaGo" title="AlphaGo">AlphaGo</a></li>
<li><a href="/wiki/AlphaZero" title="AlphaZero">AlphaZero</a></li>
<li><a href="/wiki/OpenAI_Five" title="OpenAI Five">OpenAI Five</a></li>
<li><a href="/wiki/Self-driving_car" title="Self-driving car">Self-driving car</a></li>
<li><a href="/wiki/MuZero" title="MuZero">MuZero</a></li>
<li><a href="/wiki/Action_selection" title="Action selection">Action selection</a>
<ul><li><a href="/wiki/AutoGPT" title="AutoGPT">AutoGPT</a></li></ul></li>
<li><a href="/wiki/Robot_control" title="Robot control">Robot control</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Alan_Turing" title="Alan Turing">Alan Turing</a></li>
<li><a href="/wiki/Warren_Sturgis_McCulloch" title="Warren Sturgis McCulloch">Warren Sturgis McCulloch</a></li>
<li><a href="/wiki/Walter_Pitts" title="Walter Pitts">Walter Pitts</a></li>
<li><a href="/wiki/John_von_Neumann" title="John von Neumann">John von Neumann</a></li>
<li><a href="/wiki/Christopher_D._Manning" title="Christopher D. Manning">Christopher D. Manning</a></li>
<li><a href="/wiki/Claude_Shannon" title="Claude Shannon">Claude Shannon</a></li>
<li><a href="/wiki/Shun%27ichi_Amari" title="Shun&#39;ichi Amari">Shun'ichi Amari</a></li>
<li><a href="/wiki/Kunihiko_Fukushima" title="Kunihiko Fukushima">Kunihiko Fukushima</a></li>
<li><a href="/wiki/Takeo_Kanade" title="Takeo Kanade">Takeo Kanade</a></li>
<li><a href="/wiki/Marvin_Minsky" title="Marvin Minsky">Marvin Minsky</a></li>
<li><a href="/wiki/John_McCarthy_(computer_scientist)" title="John McCarthy (computer scientist)">John McCarthy</a></li>
<li><a href="/wiki/Nathaniel_Rochester_(computer_scientist)" title="Nathaniel Rochester (computer scientist)">Nathaniel Rochester</a></li>
<li><a href="/wiki/Allen_Newell" title="Allen Newell">Allen Newell</a></li>
<li><a href="/wiki/Cliff_Shaw" title="Cliff Shaw">Cliff Shaw</a></li>
<li><a href="/wiki/Herbert_A._Simon" title="Herbert A. Simon">Herbert A. Simon</a></li>
<li><a href="/wiki/Oliver_Selfridge" title="Oliver Selfridge">Oliver Selfridge</a></li>
<li><a href="/wiki/Frank_Rosenblatt" title="Frank Rosenblatt">Frank Rosenblatt</a></li>
<li><a href="/wiki/Bernard_Widrow" title="Bernard Widrow">Bernard Widrow</a></li>
<li><a href="/wiki/Joseph_Weizenbaum" title="Joseph Weizenbaum">Joseph Weizenbaum</a></li>
<li><a href="/wiki/Seymour_Papert" title="Seymour Papert">Seymour Papert</a></li>
<li><a href="/wiki/Seppo_Linnainmaa" title="Seppo Linnainmaa">Seppo Linnainmaa</a></li>
<li><a href="/wiki/Paul_Werbos" title="Paul Werbos">Paul Werbos</a></li>
<li><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a></li>
<li><a href="/wiki/John_Hopfield" title="John Hopfield">John Hopfield</a></li>
<li><a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a></li>
<li><a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a></li>
<li><a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Yoshua Bengio</a></li>
<li><a href="/wiki/Lotfi_A._Zadeh" title="Lotfi A. Zadeh">Lotfi A. Zadeh</a></li>
<li><a href="/wiki/Stephen_Grossberg" title="Stephen Grossberg">Stephen Grossberg</a></li>
<li><a href="/wiki/Alex_Graves_(computer_scientist)" title="Alex Graves (computer scientist)">Alex Graves</a></li>
<li><a href="/wiki/James_Goodnight" title="James Goodnight">James Goodnight</a></li>
<li><a href="/wiki/Andrew_Ng" title="Andrew Ng">Andrew Ng</a></li>
<li><a href="/wiki/Fei-Fei_Li" title="Fei-Fei Li">Fei-Fei Li</a></li>
<li><a href="/wiki/Alex_Krizhevsky" title="Alex Krizhevsky">Alex Krizhevsky</a></li>
<li><a href="/wiki/Ilya_Sutskever" title="Ilya Sutskever">Ilya Sutskever</a></li>
<li><a href="/wiki/Oriol_Vinyals" title="Oriol Vinyals">Oriol Vinyals</a></li>
<li><a href="/wiki/Quoc_V._Le" title="Quoc V. Le">Quoc V. Le</a></li>
<li><a href="/wiki/Ian_Goodfellow" title="Ian Goodfellow">Ian Goodfellow</a></li>
<li><a href="/wiki/Demis_Hassabis" title="Demis Hassabis">Demis Hassabis</a></li>
<li><a href="/wiki/David_Silver_(computer_scientist)" title="David Silver (computer scientist)">David Silver</a></li>
<li><a href="/wiki/Andrej_Karpathy" title="Andrej Karpathy">Andrej Karpathy</a></li>
<li><a href="/wiki/Ashish_Vaswani" title="Ashish Vaswani">Ashish Vaswani</a></li>
<li><a href="/wiki/Noam_Shazeer" title="Noam Shazeer">Noam Shazeer</a></li>
<li><a href="/wiki/Aidan_Gomez" title="Aidan Gomez">Aidan Gomez</a></li>
<li><a href="/wiki/John_Schulman" title="John Schulman">John Schulman</a></li>
<li><a href="/wiki/Mustafa_Suleyman" title="Mustafa Suleyman">Mustafa Suleyman</a></li>
<li><a href="/wiki/Jan_Leike" title="Jan Leike">Jan Leike</a></li>
<li><a href="/wiki/Daniel_Kokotajlo_(researcher)" title="Daniel Kokotajlo (researcher)">Daniel Kokotajlo</a></li>
<li><a href="/wiki/Fran%C3%A7ois_Chollet" title="François Chollet">François Chollet</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Architectures</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Neural_Turing_machine" title="Neural Turing machine">Neural Turing machine</a></li>
<li><a href="/wiki/Differentiable_neural_computer" title="Differentiable neural computer">Differentiable neural computer</a></li>
<li><a href="/wiki/Transformer_(deep_learning_architecture)" class="mw-redirect" title="Transformer (deep learning architecture)">Transformer</a>
<ul><li><a href="/wiki/Vision_transformer" title="Vision transformer">Vision transformer (ViT)</a></li></ul></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network (RNN)</a></li>
<li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">Long short-term memory (LSTM)</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">Gated recurrent unit (GRU)</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">Echo state network</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron (MLP)</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network (CNN)</a></li>
<li><a href="/wiki/Residual_neural_network" title="Residual neural network">Residual neural network (RNN)</a></li>
<li><a href="/wiki/Highway_network" title="Highway network">Highway network</a></li>
<li><a href="/wiki/Mamba_(deep_learning_architecture)" title="Mamba (deep learning architecture)">Mamba</a></li>
<li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Variational_autoencoder" title="Variational autoencoder">Variational autoencoder (VAE)</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">Generative adversarial network (GAN)</a></li>
<li><a href="/wiki/Graph_neural_network" title="Graph neural network">Graph neural network (GNN)</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><span class="noviewer" typeof="mw:File"><span title="Category"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/20px-Symbol_category_class.svg.png" decoding="async" width="16" height="16" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/40px-Symbol_category_class.svg.png 1.5x" data-file-width="180" data-file-height="185" /></span></span> <a href="/wiki/Category:Artificial_intelligence" title="Category:Artificial intelligence">Category</a></li></ul>
</div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw‐api‐ext.eqiad.main‐5974dcddcf‐kg9lz
Cached time: 20251204165443
Cache expiry: 25520
Reduced expiry: true
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 1.157 seconds
Real time usage: 1.315 seconds
Preprocessor visited node count: 7097/1000000
Revision size: 83152/2097152 bytes
Post‐expand include size: 290852/2097152 bytes
Template argument size: 3212/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 8/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 435455/5000000 bytes
Lua time usage: 0.765/10.000 seconds
Lua memory usage: 6582401/52428800 bytes
Number of Wikibase entities loaded: 1/500
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00% 1118.852      1 -total
 66.93%  748.859      1 Template:Reflist
 21.37%  239.131     27 Template:Cite_journal
 12.04%  134.735     32 Template:Cite_news
  9.85%  110.174      1 Template:Artificial_intelligence_navbox
  9.84%  110.051      2 Template:Navbox
  8.22%   91.982     13 Template:Cite_arXiv
  8.17%   91.367     22 Template:Cite_web
  7.22%   80.783      1 Template:Short_description
  5.13%   57.450      9 Template:Cite_book
-->

<!-- Saved in parser cache with key enwiki:pcache:72607666:|#|:idhash:canonical and timestamp 20251204165443 and revision id 1325695960. Rendering was triggered because: api-parse
 -->
</div><noscript><img src="https://en.wikipedia.org/wiki/Special:CentralAutoLogin/start?useformat=desktop&amp;type=1x1&amp;usesul3=1" alt="" width="1" height="1" style="border: none; position: absolute;"></noscript>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Hallucination_(artificial_intelligence)&amp;oldid=1325695960">https://en.wikipedia.org/w/index.php?title=Hallucination_(artificial_intelligence)&amp;oldid=1325695960</a>"</div></div>
					<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Anthropomorphism" title="Category:Anthropomorphism">Anthropomorphism</a></li><li><a href="/wiki/Category:Communication_of_falsehoods" title="Category:Communication of falsehoods">Communication of falsehoods</a></li><li><a href="/wiki/Category:Computational_linguistics" title="Category:Computational linguistics">Computational linguistics</a></li><li><a href="/wiki/Category:Computational_neuroscience" title="Category:Computational neuroscience">Computational neuroscience</a></li><li><a href="/wiki/Category:Deep_learning" title="Category:Deep learning">Deep learning</a></li><li><a href="/wiki/Category:Disinformation" title="Category:Disinformation">Disinformation</a></li><li><a href="/wiki/Category:Language_modeling" title="Category:Language modeling">Language modeling</a></li><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li><li><a href="/wiki/Category:Misinformation" title="Category:Misinformation">Misinformation</a></li><li><a href="/wiki/Category:Software_bugs" title="Category:Software bugs">Software bugs</a></li><li><a href="/wiki/Category:Generative_artificial_intelligence" title="Category:Generative artificial intelligence">Generative artificial intelligence</a></li><li><a href="/wiki/Category:Philosophy_of_artificial_intelligence" title="Category:Philosophy of artificial intelligence">Philosophy of artificial intelligence</a></li><li><a href="/wiki/Category:Unsupervised_learning" title="Category:Unsupervised learning">Unsupervised learning</a></li><li><a href="/wiki/Category:Pejorative_terms_related_to_technology" title="Category:Pejorative terms related to technology">Pejorative terms related to technology</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_is_different_from_Wikidata" title="Category:Short description is different from Wikidata">Short description is different from Wikidata</a></li><li><a href="/wiki/Category:Use_dmy_dates_from_December_2022" title="Category:Use dmy dates from December 2022">Use dmy dates from December 2022</a></li><li><a href="/wiki/Category:Pages_using_multiple_image_with_auto_scaled_images" title="Category:Pages using multiple image with auto scaled images">Pages using multiple image with auto scaled images</a></li><li><a href="/wiki/Category:Articles_containing_video_clips" title="Category:Articles containing video clips">Articles containing video clips</a></li></ul></div></div>
				</div>
			</main>
			
		</div>
		<div class="mw-footer-container">
			
<footer id="footer" class="mw-footer" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 4 December 2025, at 16:54<span class="anonymous-show">&#160;(UTC)</span>.</li>
	<li id="footer-info-copyright">Text is available under the <a href="/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License" title="Wikipedia:Text of the Creative Commons Attribution-ShareAlike 4.0 International License">Creative Commons Attribution-ShareAlike 4.0 License</a>;
additional terms may apply. By using this site, you agree to the <a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use" class="extiw" title="foundation:Special:MyLanguage/Policy:Terms of Use">Terms of Use</a> and <a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy" class="extiw" title="foundation:Special:MyLanguage/Policy:Privacy policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a rel="nofollow" class="external text" href="https://wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Wikipedia:About">About Wikipedia</a></li>
	<li id="footer-places-disclaimers"><a href="/wiki/Wikipedia:General_disclaimer">Disclaimers</a></li>
	<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
	<li id="footer-places-wm-codeofconduct"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct">Code of Conduct</a></li>
	<li id="footer-places-developers"><a href="https://developer.wikimedia.org">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement">Cookie statement</a></li>
	<li id="footer-places-mobileview"><a href="//en.wikipedia.org/w/index.php?title=Hallucination_(artificial_intelligence)&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://www.wikimedia.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><picture><source media="(min-width: 500px)" srcset="/static/images/footer/wikimedia-button.svg" width="84" height="29"><img src="/static/images/footer/wikimedia.svg" width="25" height="25" alt="Wikimedia Foundation" lang="en" loading="lazy"></picture></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><picture><source media="(min-width: 500px)" srcset="/w/resources/assets/poweredby_mediawiki.svg" width="88" height="31"><img src="/w/resources/assets/mediawiki_compact.svg" alt="Powered by MediaWiki" lang="en" width="25" height="25" loading="lazy"></picture></a></li>
</ul>

</footer>

		</div>
	</div> 
</div> 
<div class="vector-header-container vector-sticky-header-container no-font-mode-scale">
	<div id="vector-sticky-header" class="vector-sticky-header">
		<div class="vector-sticky-header-start">
			<div class="vector-sticky-header-icon-start vector-button-flush-left vector-button-flush-right" aria-hidden="true">
				<button class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-sticky-header-search-toggle" tabindex="-1" data-event-name="ui.vector-sticky-search-form.icon"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
			</button>
		</div>
			
		<div role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box">
			<div class="vector-typeahead-search-container">
				<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail">
					<form action="/w/index.php" id="vector-sticky-search-form" class="cdx-search-input cdx-search-input--has-end-button">
						<div  class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
							<div class="cdx-text-input cdx-text-input--has-start-icon">
								<input
									class="cdx-text-input__input mw-searchInput" autocomplete="off"
									
									type="search" name="search" placeholder="Search Wikipedia">
								<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
							</div>
							<input type="hidden" name="title" value="Special:Search">
						</div>
						<button class="cdx-button cdx-search-input__end-button">Search</button>
					</form>
				</div>
			</div>
		</div>
		<div class="vector-sticky-header-context-bar">
				<nav aria-label="Contents" class="vector-toc-landmark">
						
					<div id="vector-sticky-header-toc" class="vector-dropdown mw-portlet mw-portlet-sticky-header-toc vector-sticky-header-toc vector-button-flush-left"  >
						<input type="checkbox" id="vector-sticky-header-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-sticky-header-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
						<label id="vector-sticky-header-toc-label" for="vector-sticky-header-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
						</label>
						<div class="vector-dropdown-content">
					
						<div id="vector-sticky-header-toc-unpinned-container" class="vector-unpinned-container">
						</div>
					
						</div>
					</div>
			</nav>
				<div class="vector-sticky-header-context-bar-primary" aria-hidden="true" ><span class="mw-page-title-main">Hallucination (artificial intelligence)</span></div>
			</div>
		</div>
		<div class="vector-sticky-header-end" aria-hidden="true">
			<div class="vector-sticky-header-icons">
				<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-talk-sticky-header" tabindex="-1" data-event-name="talk-sticky-header"><span class="vector-icon mw-ui-icon-speechBubbles mw-ui-icon-wikimedia-speechBubbles"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-subject-sticky-header" tabindex="-1" data-event-name="subject-sticky-header"><span class="vector-icon mw-ui-icon-article mw-ui-icon-wikimedia-article"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-history-sticky-header" tabindex="-1" data-event-name="history-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-history mw-ui-icon-wikimedia-wikimedia-history"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only mw-watchlink" id="ca-watchstar-sticky-header" tabindex="-1" data-event-name="watch-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-star mw-ui-icon-wikimedia-wikimedia-star"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-edit-sticky-header" tabindex="-1" data-event-name="wikitext-edit-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-wikiText mw-ui-icon-wikimedia-wikimedia-wikiText"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-ve-edit-sticky-header" tabindex="-1" data-event-name="ve-edit-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-edit mw-ui-icon-wikimedia-wikimedia-edit"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-viewsource-sticky-header" tabindex="-1" data-event-name="ve-edit-protected-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-editLock mw-ui-icon-wikimedia-wikimedia-editLock"></span>

<span></span>
			</a>
		</div>
			<div class="vector-sticky-header-buttons">
				<button class="cdx-button cdx-button--weight-quiet mw-interlanguage-selector" id="p-lang-btn-sticky-header" tabindex="-1" data-event-name="ui.dropdown-p-lang-btn-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-language mw-ui-icon-wikimedia-wikimedia-language"></span>

<span>31 languages</span>
			</button>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive" id="ca-addsection-sticky-header" tabindex="-1" data-event-name="addsection-sticky-header"><span class="vector-icon mw-ui-icon-speechBubbleAdd-progressive mw-ui-icon-wikimedia-speechBubbleAdd-progressive"></span>

<span>Add topic</span>
			</a>
		</div>
			<div class="vector-sticky-header-icon-end">
				<div class="vector-user-links">
				</div>
			</div>
		</div>
	</div>
</div>
<div class="mw-portlet mw-portlet-dock-bottom emptyPortlet" id="p-dock-bottom">
	<ul>
		
	</ul>
</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgHostname":"mw-web.eqiad.main-68fb48f4b9-h2psj","wgBackendResponseTime":171,"wgPageParseReport":{"limitreport":{"cputime":"1.157","walltime":"1.315","ppvisitednodes":{"value":7097,"limit":1000000},"revisionsize":{"value":83152,"limit":2097152},"postexpandincludesize":{"value":290852,"limit":2097152},"templateargumentsize":{"value":3212,"limit":2097152},"expansiondepth":{"value":15,"limit":100},"expensivefunctioncount":{"value":8,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":435455,"limit":5000000},"entityaccesscount":{"value":1,"limit":500},"timingprofile":["100.00% 1118.852      1 -total"," 66.93%  748.859      1 Template:Reflist"," 21.37%  239.131     27 Template:Cite_journal"," 12.04%  134.735     32 Template:Cite_news","  9.85%  110.174      1 Template:Artificial_intelligence_navbox","  9.84%  110.051      2 Template:Navbox","  8.22%   91.982     13 Template:Cite_arXiv","  8.17%   91.367     22 Template:Cite_web","  7.22%   80.783      1 Template:Short_description","  5.13%   57.450      9 Template:Cite_book"]},"scribunto":{"limitreport-timeusage":{"value":"0.765","limit":"10.000"},"limitreport-memusage":{"value":6582401,"limit":52428800}},"cachereport":{"origin":"mw-api-ext.eqiad.main-5974dcddcf-kg9lz","timestamp":"20251204165443","ttl":25520,"transientcontent":true}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Hallucination (artificial intelligence)","url":"https:\/\/en.wikipedia.org\/wiki\/Hallucination_(artificial_intelligence)","sameAs":"http:\/\/www.wikidata.org\/entity\/Q116197048","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q116197048","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2022-12-30T00:36:02Z","dateModified":"2025-12-04T16:54:38Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/9\/9c\/Photoreal-train.webm","headline":"confident unjustified claim by an AI"}</script>
</body>
</html> title 51 Hallucination (artificial intelligence) - Wikipedia contentType 9 text/html url 73 https://en.wikipedia.org:443/wiki/Hallucination_(artificial_intelligence) responseCode 3 200 